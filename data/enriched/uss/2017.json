[
    {
        "@score": "1",
        "@id": "3511125",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "17/5702-1",
                        "text": "Sangho Lee 0001"
                    },
                    {
                        "@pid": "117/5466",
                        "text": "Ming-Wei Shih"
                    },
                    {
                        "@pid": "190/7262",
                        "text": "Prasun Gera"
                    },
                    {
                        "@pid": "38/8882",
                        "text": "Taesoo Kim"
                    },
                    {
                        "@pid": "87/5743",
                        "text": "Hyesoon Kim"
                    },
                    {
                        "@pid": "56/6925",
                        "text": "Marcus Peinado"
                    }
                ]
            },
            "title": "Inferring Fine-grained Control Flow Inside SGX Enclaves with Branch Shadowing.",
            "venue": "USENIX Security Symposium",
            "pages": "557-574",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/0001SGKKP17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/lee-sangho",
            "url": "https://dblp.org/rec/conf/uss/0001SGKKP17",
            "abstract": "Intel has introduced a hardware-based trusted execution environment, Intel Software Guard Extensions (SGX), that provides a secure, isolated execution environment, or enclave, for a user program without trusting any underlying software (e.g., an operating system) or firmware. Researchers have demonstrated that SGX is vulnerable to a page-fault-based attack. However, the attack only reveals page-level memory accesses within an enclave.\nIn this paper, we explore a new, yet critical, sidechannel attack, branch shadowing, that reveals fine-grained control flows (branch granularity) in an enclave. The root cause of this attack is that SGX does not clear branch history when switching from enclave to non-enclave mode, leaving fine-grained traces for the outside world to observe, which gives rise to a branch-prediction side channel. However, exploiting this channel in practice is challenging because 1) measuring branch execution time is too noisy for distinguishing fine-grained control-flow changes and 2) pausing an enclave right after it has executed the code block we target requires sophisticated control. To overcome these challenges, we develop two novel exploitation techniques: 1) a last branch record (LBR)-based history-inferring technique and 2) an advanced programmable interrupt controller (APIC)-based technique to control the execution of an enclave in a fine-grained manner. An evaluation against RSA shows that our attack infers each private key bit with 99.8% accuracy. Finally, we thoroughly study the feasibility of hardware-based solutions (i.e., branch history flushing) and propose a software-based approach that mitigates the attack.",
            "pdf_url": "",
            "keywords": [
                "Trusted Execution Environment",
                "Intel SGX",
                "Side-channel Attack",
                "Branch Shadowing",
                "Control Flow Inference"
            ]
        },
        "url": "URL#3511125"
    },
    {
        "@score": "1",
        "@id": "3511126",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "200/5775",
                        "text": "Nikolaos Alexopoulos"
                    },
                    {
                        "@pid": "47/3682",
                        "text": "Aggelos Kiayias"
                    },
                    {
                        "@pid": "52/10733",
                        "text": "Riivo Talviste"
                    },
                    {
                        "@pid": "76/1308-1",
                        "text": "Thomas Zacharias 0001"
                    }
                ]
            },
            "title": "MCMix: Anonymous Messaging via Secure Multiparty Computation.",
            "venue": "USENIX Security Symposium",
            "pages": "1217-1234",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/AlexopoulosKT017",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/alexopoulos",
            "url": "https://dblp.org/rec/conf/uss/AlexopoulosKT017",
            "abstract": "We present MCMix, an anonymous messaging system that completely hides communication metadata and can scale in the order of hundreds of thousands of users. Our approach is to isolate two suitable functionalities, called dialing and conversation, that when used in succession, realize anonymous messaging. With this as a starting point, we apply secure multiparty computation (\u201cMC\u201d or MPC) and proceed to realize them. We then present an implementation using Sharemind, a prevalent MPC system. Our implementation is competitive in terms of latency with previous messaging systems that only offer weaker privacy guarantees. Our solution can be instantiated in a variety of different ways with different MPC implementations, overall illustrating how MPC is a viable and competitive alternative to mix-nets and DC-nets for anonymous communication.",
            "keywords": [
                "Anonymous Messaging",
                "Secure Multiparty Computation",
                "Communication Metadata",
                "Privacy Guarantees",
                "Scalability"
            ]
        },
        "url": "URL#3511126",
        "sema_paperId": "995742e26a8b8a5e77e236bb1bc885be45de2b5a"
    },
    {
        "@score": "1",
        "@id": "3511127",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "26/216",
                        "text": "Manos Antonakakis"
                    },
                    {
                        "@pid": "205/2253",
                        "text": "Tim April"
                    },
                    {
                        "@pid": "359/0958",
                        "text": "Michael D. Bailey"
                    },
                    {
                        "@pid": "205/2112",
                        "text": "Matt Bernhard"
                    },
                    {
                        "@pid": "20/7004",
                        "text": "Elie Bursztein"
                    },
                    {
                        "@pid": "205/2236",
                        "text": "Jaime Cochran"
                    },
                    {
                        "@pid": "143/5673",
                        "text": "Zakir Durumeric"
                    },
                    {
                        "@pid": "h/JAlexHalderman",
                        "text": "J. Alex Halderman"
                    },
                    {
                        "@pid": "57/9185",
                        "text": "Luca Invernizzi"
                    },
                    {
                        "@pid": "68/4656",
                        "text": "Michalis Kallitsis 0001"
                    },
                    {
                        "@pid": "66/3984-6",
                        "text": "Deepak Kumar 0006"
                    },
                    {
                        "@pid": "134/8710",
                        "text": "Chaz Lever"
                    },
                    {
                        "@pid": "198/3346",
                        "text": "Zane Ma"
                    },
                    {
                        "@pid": "34/3008",
                        "text": "Joshua Mason"
                    },
                    {
                        "@pid": "205/2016",
                        "text": "Damian Menscher"
                    },
                    {
                        "@pid": "205/2254",
                        "text": "Chad Seaman"
                    },
                    {
                        "@pid": "172/5512",
                        "text": "Nick Sullivan"
                    },
                    {
                        "@pid": "68/8283",
                        "text": "Kurt Thomas"
                    },
                    {
                        "@pid": "01/1901",
                        "text": "Yi Zhou"
                    }
                ]
            },
            "title": "Understanding the Mirai Botnet.",
            "venue": "USENIX Security Symposium",
            "pages": "1093-1110",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/AntonakakisABBB17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/antonakakis",
            "url": "https://dblp.org/rec/conf/uss/AntonakakisABBB17",
            "abstract": "The Mirai botnet, composed primarily of embedded and IoT devices, took the Internet by storm in late 2016 when it overwhelmed several high-profile targets with massive distributed denial-of-service (DDoS) attacks. In this paper, we provide a seven-month retrospective analysis of Mirai's growth to a peak of 600k infections and a history of its DDoS victims. By combining a variety of measurement perspectives, we analyze how the botnet emerged, what classes of devices were affected, and how Mirai variants evolved and competed for vulnerable hosts. Our measurements serve as a lens into the fragile ecosystem of IoT devices. We argue that Mirai may represent a sea change in the evolutionary development of botnets--the simplicity through which devices were infected and its precipitous growth, demonstrate that novice malicious techniques can compromise enough low-end devices to threaten even some of the best-defended targets. To address this risk, we recommend technical and nontechnical interventions, as well as propose future research directions.",
            "keywords": [
                "IoT Security",
                "Botnet Analysis",
                "DDoS Attacks",
                "Mirai Botnet",
                "Device Vulnerability"
            ]
        },
        "url": "URL#3511127",
        "sema_paperId": "220a7eed5c859f596a0d9dbc194034d170a6af51"
    },
    {
        "@score": "1",
        "@id": "3511129",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "200/7947",
                        "text": "Brendan Avent"
                    },
                    {
                        "@pid": "08/1446",
                        "text": "Aleksandra Korolova"
                    },
                    {
                        "@pid": "75/5658",
                        "text": "David Zeber"
                    },
                    {
                        "@pid": "131/4838",
                        "text": "Torgeir Hovden"
                    },
                    {
                        "@pid": "46/2924",
                        "text": "Benjamin Livshits"
                    }
                ]
            },
            "title": "BLENDER: Enabling Local Search with a Hybrid Differential Privacy Model.",
            "venue": "USENIX Security Symposium",
            "pages": "747-764",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/AventKZHL17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/avent",
            "url": "https://dblp.org/rec/conf/uss/AventKZHL17",
            "abstract": "We propose a hybrid model of differential privacy that considers a combination of regular and opt-in users who desire the differential privacy guarantees of the local privacy model and the trusted curator model, respectively. We demonstrate that within this model, it is possible to design a new type of blended algorithm that improves the utility of obtained data, while providing users with their desired privacy guarantees. \nWe apply this algorithm to the task of privately computing the head of the search log and show that the blended approach provides significant improvements in the utility of the data compared to related work. \nSpecifically, on two large search click data sets, comprising 1.75 and 16 GB, respectively, our approach attains NDCG values exceeding 95% across a range of privacy budget values.",
            "keywords": [
                "Differential Privacy",
                "Local Privacy Model",
                "Trusted Curator Model",
                "Search Log Analysis",
                "Utility Improvement"
            ]
        },
        "url": "URL#3511129",
        "sema_paperId": "68cf05b4efb856411549ad9802b66ec7d1cd239b"
    },
    {
        "@score": "1",
        "@id": "3511130",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "20/10240",
                        "text": "Xiaolong Bai"
                    },
                    {
                        "@pid": "34/6503-1",
                        "text": "Zhe Zhou 0001"
                    },
                    {
                        "@pid": "06/6268",
                        "text": "XiaoFeng Wang 0001"
                    },
                    {
                        "@pid": "62/4119-1",
                        "text": "Zhou Li 0001"
                    },
                    {
                        "@pid": "192/2270",
                        "text": "Xianghang Mi"
                    },
                    {
                        "@pid": "28/6297-18",
                        "text": "Nan Zhang 0018"
                    },
                    {
                        "@pid": "140/7353",
                        "text": "Tongxin Li"
                    },
                    {
                        "@pid": "h/ShiMinHu",
                        "text": "Shi-Min Hu 0001"
                    },
                    {
                        "@pid": "66/6560",
                        "text": "Kehuan Zhang"
                    }
                ]
            },
            "title": "Picking Up My Tab: Understanding and Mitigating Synchronized Token Lifting and Spending in Mobile Payment.",
            "venue": "USENIX Security Symposium",
            "pages": "593-608",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/BaiZWLMZLHZ17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/bai",
            "url": "https://dblp.org/rec/conf/uss/BaiZWLMZLHZ17",
            "abstract": "Mobile off-line payment enables purchase over the counter even in the absence of reliable network connections. Popular solutions proposed by leading payment service providers (e.g., Google, Amazon, Samsung, Apple) rely on direct communication between the payer's device and the POS system, through Near-Field Communication (NFC), Magnetic Secure Transaction (MST), audio and QR code. Although pre-cautions have been taken to protect the payment transactions through these channels, their security implications are less understood, particularly in the presence of unique threats to this new e-commerce service. \n \nIn the paper, we report a new type of over-the-counter payment frauds on mobile off-line payment, which exploit the designs of existing schemes that apparently fail to consider the adversary capable of actively affecting the payment process. Our attack, called Synchronized Token Lifting and Spending (STLS), demonstrates that an active attacker can sniff the payment token, halt the ongoing transaction through various means and transmit the token quickly to a colluder to spend it in a different transaction while the token is still valid. Our research shows that such STLS attacks pose a realistic threat to popular offline payment schemes, particularly those meant to be backwardly compatible, like Samsung Pay and AliPay. \n \nTo mitigate the newly discovered threats, we propose a new solution called POSAUTH. One fundamental cause of the STLS risk is the nature of the communication channels used by the vulnerable mobile off-line payment schemes, which are easy to sniff and jam, and more importantly, unable to support a secure mutual challenge-response protocols since information can only be transmitted in one-way. POSAUTH addresses this issue by incorporating one unique ID of the current POS terminal into the generation of payment tokens by requiring a quick scanning of QR code printed on the POS terminal. When combined with a short valid period, POSAUTH can ensure that tokens generated for one transaction can only be used in that transaction.",
            "keywords": [
                "Mobile Payment Security",
                "Offline Payment Fraud",
                "Synchronized Token Lifting and Spending",
                "Payment Token Vulnerabilities",
                "POS Authentication"
            ]
        },
        "url": "URL#3511130",
        "sema_paperId": "dede48198a33606457ae583c52e774159d24c74d"
    },
    {
        "@score": "1",
        "@id": "3511131",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "33/7672",
                        "text": "Sebastian Banescu"
                    },
                    {
                        "@pid": "c/ChristianSCollberg",
                        "text": "Christian S. Collberg"
                    },
                    {
                        "@pid": "98/5319",
                        "text": "Alexander Pretschner"
                    }
                ]
            },
            "title": "Predicting the Resilience of Obfuscated Code Against Symbolic Execution Attacks via Machine Learning.",
            "venue": "USENIX Security Symposium",
            "pages": "661-678",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/BanescuCP17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/banescu",
            "url": "https://dblp.org/rec/conf/uss/BanescuCP17",
            "abstract": "Software obfuscation transforms code such that it is more \ndifficult to reverse engineer. However, it is known that \ngiven enough resources, an attacker will successfully reverse \nengineer an obfuscated program. Therefore, an \nopen challenge for software obfuscation is estimating the \ntime an obfuscated program is able to withstand a given \nreverse engineering attack. This paper proposes a general \nframework for choosing the most relevant software \nfeatures to estimate the effort of automated attacks. Our \nframework uses these software features to build regression \nmodels that can predict the resilience of different \nsoftware protection transformations against automated \nattacks. To evaluate the effectiveness of our approach, \nwe instantiate it in a case-study about predicting the time \nneeded to deobfuscate a set of C programs, using an attack \nbased on symbolic execution. To train regression \nmodels our system requires a large set of programs as \ninput. We have therefore implemented a code generator \nthat can generate large numbers of arbitrarily complex \nrandom C functions. Our results show that features \nsuch as the number of community structures in the graphrepresentation \nof symbolic path-constraints, are far more \nrelevant for predicting deobfuscation time than other features \ngenerally used to measure the potency of controlflow \nobfuscation (e.g. cyclomatic complexity). Our best \nmodel is able to predict the number of seconds of symbolic \nexecution-based deobfuscation attacks with over \n90% accuracy for 80% of the programs in our dataset, \nwhich also includes several realistic hash functions.",
            "keywords": [
                "Software Obfuscation",
                "Symbolic Execution",
                "Deobfuscation Time Prediction",
                "Automated Reverse Engineering Attacks",
                "Regression Models for Code Resilience"
            ]
        },
        "url": "URL#3511131",
        "sema_paperId": "5e0a6e47feba6fcba4cc64334e56e5107aa5eea5"
    },
    {
        "@score": "1",
        "@id": "3511132",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "205/2026",
                        "text": "Christian Bayens"
                    },
                    {
                        "@pid": "147/9791",
                        "text": "Tuan Le"
                    },
                    {
                        "@pid": "67/136-1",
                        "text": "Luis Garcia 0001"
                    },
                    {
                        "@pid": "12/6354",
                        "text": "Raheem A. Beyah"
                    },
                    {
                        "@pid": "172/2705",
                        "text": "Mehdi Javanmard"
                    },
                    {
                        "@pid": "89/4316",
                        "text": "Saman A. Zonouz"
                    }
                ]
            },
            "title": "See No Evil, Hear No Evil, Feel No Evil, Print No Evil? Malicious Fill Patterns Detection in Additive Manufacturing.",
            "venue": "USENIX Security Symposium",
            "pages": "1181-1198",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/BayensLGBJZ17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/bayens",
            "url": "https://dblp.org/rec/conf/uss/BayensLGBJZ17",
            "abstract": "Additive Manufacturing is an increasingly integral part of industrial manufacturing. Safety-critical products, such as medical prostheses and parts for aerospace and automotive industries are being printed by additive manufacturing methods with no standard means of verification. In this paper, we develop a scheme of verification and intrusion detection that is independent of the printer firmware and controller PC. The scheme incorporates analyses of the acoustic signature of a manufacturing process, real-time tracking of machine components, and post production materials analysis. Not only will these methods allow the end user to verify the accuracy of printed models, but they will also save material costs by verifying the prints in real time and stopping the process in the event of a discrepancy. We evaluate our methods using three different types of 3D printers and one CNC machine and find them to be 100% accurate when detecting erroneous prints in real time. We also present a use case in which an erroneous print of a tibial knee prosthesis is identified.",
            "keywords": [
                "Additive Manufacturing",
                "Intrusion Detection",
                "Verification Scheme",
                "Acoustic Signature Analysis",
                "Erroneous Prints Detection"
            ]
        },
        "url": "URL#3511132",
        "sema_paperId": "29aa4e03d46dd46d278e62f0e6d1af9c56368c31"
    },
    {
        "@score": "1",
        "@id": "3511133",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "145/1200",
                        "text": "Priyam Biswas"
                    },
                    {
                        "@pid": "123/3267",
                        "text": "Alessandro Di Federico"
                    },
                    {
                        "@pid": "176/5333",
                        "text": "Scott A. Carr"
                    },
                    {
                        "@pid": "205/2020",
                        "text": "Prabhu Rajasekaran"
                    },
                    {
                        "@pid": "127/6103",
                        "text": "Stijn Volckaert"
                    },
                    {
                        "@pid": "37/9431",
                        "text": "Yeoul Na"
                    },
                    {
                        "@pid": "f/MichaelFranz",
                        "text": "Michael Franz"
                    },
                    {
                        "@pid": "31/1273",
                        "text": "Mathias Payer"
                    }
                ]
            },
            "title": "Venerable Variadic Vulnerabilities Vanquished.",
            "venue": "USENIX Security Symposium",
            "pages": "186-198",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/BiswasFCRVNFP17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/biswas",
            "url": "https://dblp.org/rec/conf/uss/BiswasFCRVNFP17",
            "abstract": "Programming languages such as C and C ++ support vari-adic functions, i.e., functions that accept a variable number of arguments (e.g., printf ). While variadic functions are \ufb02exible, they are inherently not type-safe. In fact, the semantics and parameters of variadic functions are de\ufb01ned implicitly by their implementation. It is left to the programmer to ensure that the caller and callee follow this implicit speci\ufb01cation, without the help of a static type checker. An adversary can take advantage of a mismatch between the argument types used by the caller of a variadic function and the types expected by the callee to violate the language semantics and to tamper with memory. Format string attacks are the most popular example of such a mismatch.",
            "keywords": [
                "Variadic Functions",
                "Type Safety",
                "Memory Tampering",
                "Format String Attacks",
                "Argument Type Mismatch"
            ]
        },
        "url": "URL#3511133",
        "sema_paperId": "0009a4b50d1ad0353f1cabfc50a769801d90cc3c"
    },
    {
        "@score": "1",
        "@id": "3511134",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "205/2079",
                        "text": "Tim Blazytko"
                    },
                    {
                        "@pid": "150/7970",
                        "text": "Moritz Contag"
                    },
                    {
                        "@pid": "160/7844",
                        "text": "Cornelius Aschermann"
                    },
                    {
                        "@pid": "h/ThorstenHolz",
                        "text": "Thorsten Holz"
                    }
                ]
            },
            "title": "Syntia: Synthesizing the Semantics of Obfuscated Code.",
            "venue": "USENIX Security Symposium",
            "pages": "643-659",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/BlazytkoCAH17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/blazytko",
            "url": "https://dblp.org/rec/conf/uss/BlazytkoCAH17",
            "abstract": "Current state-of-the-art deobfuscation approaches operate on instruction traces and use a mixed approach of symbolic execution and taint analysis; two techniques that require precise analysis of the underlying code. However, recent research has shown that both techniques can easily be thwarted by specific transformations. As program synthesis can synthesize code of arbitrary code complexity, it is only limited by the complexity of the underlying code\u2019s semantic. In our work, we propose a generic approach for automated code deobfuscation using program synthesis guided by Monte Carlo Tree Search (MCTS). Specifically, our prototype implementation, Syntia, simplifies execution traces by dividing them into distinct trace windows whose semantics are then \u201clearned\u201d by the synthesis. To demonstrate the practical feasibility of our approach, we automatically learn the semantics of 489 out of 500 random expressions obfuscated via Mixed Boolean-Arithmetic. Furthermore, we synthesize the semantics of arithmetic instruction handlers in two state-of-the art commercial virtualization-based obfuscators (VMProtect and Themida) with a success rate of more than 94%. Finally, to substantiate our claim that the approach is generic and applicable to different use cases, we show that Syntia can also automatically learn the semantics of ROP gadgets.",
            "keywords": [
                "Code Deobfuscation",
                "Program Synthesis",
                "Monte Carlo Tree Search",
                "Obfuscated Code Semantics",
                "Mixed Boolean-Arithmetic"
            ]
        },
        "url": "URL#3511134",
        "sema_paperId": "79a6ee9140b95a35ce1e6a9a143a988e51ce1565"
    },
    {
        "@score": "1",
        "@id": "3511135",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "74/9407",
                        "text": "Barry Bond"
                    },
                    {
                        "@pid": "22/3053",
                        "text": "Chris Hawblitzel"
                    },
                    {
                        "@pid": "23/6695",
                        "text": "Manos Kapritsos"
                    },
                    {
                        "@pid": "l/KRMLeino",
                        "text": "K. Rustan M. Leino"
                    },
                    {
                        "@pid": "25/3228",
                        "text": "Jacob R. Lorch"
                    },
                    {
                        "@pid": "49/6324",
                        "text": "Bryan Parno"
                    },
                    {
                        "@pid": "63/10367",
                        "text": "Ashay Rane"
                    },
                    {
                        "@pid": "68/8463",
                        "text": "Srinath T. V. Setty"
                    },
                    {
                        "@pid": "155/8149",
                        "text": "Laure Thompson"
                    }
                ]
            },
            "title": "Vale: Verifying High-Performance Cryptographic Assembly Code.",
            "venue": "USENIX Security Symposium",
            "pages": "917-934",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/BondHKLLPRST17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/bond",
            "url": "https://dblp.org/rec/conf/uss/BondHKLLPRST17",
            "abstract": "High-performance cryptographic code often relies on complex hand-tuned assembly language that is customized for individual hardware platforms. Such code is difficult to understand or analyze. We introduce a new programming language and tool called Vale that supports flexible, automated verification of high-performance assembly code. The Vale tool transforms annotated assembly language into an abstract syntax tree (AST), while also generating proofs about the AST that are verified via an SMT solver. Since the AST is a first-class proof term, it can be further analyzed and manipulated by proven-correct code before being extracted into standard assembly. For example, we have developed a novel, proven-correct taint-analysis engine that verifies the code\u2019s freedom from digital side channels. Using these tools, we verify the correctness, safety, and security of implementations of SHA-256 on x86 and ARM, Poly1305 on x64, and hardware-accelerated AES-CBC on x86. Several implementations meet or beat the performance of unverified, state-of-the-art cryptographic libraries.",
            "keywords": [
                "Cryptographic Code Verification",
                "Assembly Language",
                "Automated Verification",
                "Taint Analysis",
                "Digital Side Channels"
            ]
        },
        "url": "URL#3511135",
        "sema_paperId": "8a3fa56ae7563fd04c935f705fe0410f2ca4ab04"
    },
    {
        "@score": "1",
        "@id": "3511136",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "117/5967",
                        "text": "Ferdinand Brasser"
                    },
                    {
                        "@pid": "73/7564",
                        "text": "Lucas Davi"
                    },
                    {
                        "@pid": "164/2771",
                        "text": "David Gens"
                    },
                    {
                        "@pid": "131/5096",
                        "text": "Christopher Liebchen"
                    },
                    {
                        "@pid": "s/AhmadRezaSadeghi",
                        "text": "Ahmad-Reza Sadeghi"
                    }
                ]
            },
            "title": "CAn&apos;t Touch This: Software-only Mitigation against Rowhammer Attacks targeting Kernel Memory.",
            "venue": "USENIX Security Symposium",
            "pages": "117-130",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/BrasserDGLS17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/brasser",
            "url": "https://dblp.org/rec/conf/uss/BrasserDGLS17",
            "abstract": "Rowhammer is a hardware bug that can be exploited to implement privilege escalation and remote code execution attacks. Previous proposals on rowhammer mitigations either require hardware changes or follow heuristicbased approaches (based on CPU performance counters). To date, there exists no instant protection against rowhammer attacks on legacy systems. In this paper, we present the design and implementation of a practical and efficient software-only defense against rowhammer attacks. Our defense, called CATT, prevents the attacker from leveraging rowhammer to corrupt kernel memory from user mode. To do so, we extend the physical memory allocator of the OS to physically isolate the memory of the kernel and user space. We implemented CATT on x86 and ARM to mitigate rowhammer-based kernel exploits. Our extensive evaluation shows that our mitigation (i) can stop available realworld rowhammer attacks, (ii) imposes virtually no runtime overhead for common user and kernel benchmarks as well as commonly used applications, and (iii) does not affect the stability of the overall system.",
            "keywords": [
                "Rowhammer Mitigation",
                "Kernel Memory Protection",
                "Software Defense",
                "Memory Isolation",
                "Privilege Escalation"
            ]
        },
        "url": "URL#3511136",
        "sema_paperId": "ba0935cb5d8424b8662e1869eb71fb19579073d2"
    },
    {
        "@score": "1",
        "@id": "3511137",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "167/1573",
                        "text": "Jo Van Bulck"
                    },
                    {
                        "@pid": "185/6233",
                        "text": "Nico Weichbrodt"
                    },
                    {
                        "@pid": "27/2697",
                        "text": "R\u00fcdiger Kapitza"
                    },
                    {
                        "@pid": "85/6647",
                        "text": "Frank Piessens"
                    },
                    {
                        "@pid": "97/7967",
                        "text": "Raoul Strackx"
                    }
                ]
            },
            "title": "Telling Your Secrets without Page Faults: Stealthy Page Table-Based Attacks on Enclaved Execution.",
            "venue": "USENIX Security Symposium",
            "pages": "1041-1056",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/BulckWKPS17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/van-bulck",
            "url": "https://dblp.org/rec/conf/uss/BulckWKPS17",
            "abstract": "Protected module architectures, such as Intel SGX, enable strong trusted computing guarantees for hardware-enforced enclaves on top a potentially malicious operating system. However, such enclaved execution environments are known to be vulnerable to a powerful class of controlled-channel attacks. Recent research convincingly demonstrated that adversarial system software can extract sensitive data from enclaved applications by carefully revoking access rights on enclave pages, and recording the associated page faults. As a response, a number of state-of-the-art defense techniques has been proposed that suppress page faults during enclave execution. \n \nThis paper shows, however, that page table-based threats go beyond page faults. We demonstrate that an untrusted operating system can observe enclave page accesses without resorting to page faults, by exploiting other side-effects of the address translation process. We contribute two novel attack vectors that infer enclaved memory accesses from page table attributes, as well as from the caching behavior of unprotected page table memory. We demonstrate the effectiveness of our attacks by recovering EdDSA session keys with little to no noise from the popular Libgcrypt cryptographic software suite.",
            "keywords": [
                "Enclaved Execution",
                "Intel SGX",
                "Controlled-Channel Attacks",
                "Page Table-Based Attacks",
                "Memory Access Inference"
            ]
        },
        "url": "URL#3511137",
        "sema_paperId": "19c07d7e5bbcb6a3894ad0f1c775994d1b323f0a"
    },
    {
        "@score": "1",
        "@id": "3511139",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "89/9526",
                        "text": "Stefano Calzavara"
                    },
                    {
                        "@pid": "159/0703",
                        "text": "Alvise Rabitti"
                    },
                    {
                        "@pid": "b/MicheleBugliesi",
                        "text": "Michele Bugliesi"
                    }
                ]
            },
            "title": "CCSP: Controlled Relaxation of Content Security Policies by Runtime Policy Composition.",
            "venue": "USENIX Security Symposium",
            "pages": "695-712",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/CalzavaraRB17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/calzavara",
            "url": "https://dblp.org/rec/conf/uss/CalzavaraRB17",
            "abstract": "Content Security Policy (CSP) is a W3C standard designed to prevent and mitigate the impact of content injection vulnerabilities on websites by means of browserenforced security policies. Though CSP is gaining a lot of popularity in the wild, previous research questioned one of its key design choices, namely the use of static white-lists to define legitimate content inclusions. In this paper we present Compositional CSP (CCSP), an extension of CSP based on runtime policy composition. CCSP is designed to overcome the limitations arising from the use of static white-lists, while avoiding a major overhaul of CSP and the logic underlying policy writing. We perform an extensive evaluation of the design of CCSP by focusing on the general security guarantees it provides, its backward compatibility and its deployment cost. We then assess the potential impact of CCSP on the web and we implement a prototype of our proposal, which we test on major websites. In the end, we conclude that the deployment of CCSP can be done with limited efforts and would lead to significant benefits for the large majority of the websites.",
            "keywords": [
                "Content Security Policy",
                "Runtime Policy Composition",
                "Web Security",
                "Static White-lists",
                "Compositional CSP (CCSP)"
            ]
        },
        "url": "URL#3511139",
        "sema_paperId": "0632b69aca6500bf65f3d9c82aad62369b2bc682"
    },
    {
        "@score": "1",
        "@id": "3511140",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "79/5815",
                        "text": "Yue Chen"
                    },
                    {
                        "@pid": "32/9374",
                        "text": "Yulong Zhang"
                    },
                    {
                        "@pid": "95/6543-4",
                        "text": "Zhi Wang 0004"
                    },
                    {
                        "@pid": "205/2057",
                        "text": "Liangzhao Xia"
                    },
                    {
                        "@pid": "205/2109",
                        "text": "Chenfu Bao"
                    },
                    {
                        "@pid": "64/5099",
                        "text": "Tao Wei"
                    }
                ]
            },
            "title": "Adaptive Android Kernel Live Patching.",
            "venue": "USENIX Security Symposium",
            "pages": "1253-1270",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/ChenZWXBW17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/chen",
            "url": "https://dblp.org/rec/conf/uss/ChenZWXBW17",
            "abstract": "Android kernel vulnerabilities pose a serious threat to user security and privacy. They allow attackers to take full control over victim devices, install malicious and unwanted apps, and maintain persistent control. Unfortunately, most Android devices are never timely updated to protect their users from kernel exploits. Recent Android malware even has built-in kernel exploits to take advantage of this large window of vulnerability. An effective solution to this problem must be adaptable to lots of (out-of-date) devices, quickly deployable, and secure from misuse. However, the fragmented Android ecosystem makes this a complex and challenging task. To address that, we systematically studied 1,139 Android kernels and all the recent critical Android kernel vulnerabilities. We accordingly propose KARMA, an adaptive live patching system for Android kernels. KARMA features a multi-level adaptive patching model to protect kernel vulnerabilities from exploits. Specifically, patches in KARMA can be placed at multiple levels in the kernel to filter malicious inputs, and they can be automatically adapted to thousands of Android devices. In addition, KARMA\u2019s patches are written in a high-level memory-safe language, making them secure and easy to vet, and their run-time behaviors are strictly confined to prevent them from being misused. Our evaluation demonstrates that KARMA can protect most critical kernel vulnerabilities on many Android devices (520 devices in our evaluation) with only minor performance overhead (< 1%).",
            "keywords": [
                "Android Kernel Security",
                "Live Patching",
                "Kernel Vulnerabilities",
                "Adaptive Patching",
                "KARMA"
            ]
        },
        "url": "URL#3511140",
        "sema_paperId": "73753f69c1dc16fe19c8e07661fc55898427ca6a"
    },
    {
        "@score": "1",
        "@id": "3511142",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "150/5333",
                        "text": "Zheng Leong Chua"
                    },
                    {
                        "@pid": "169/3386",
                        "text": "Shiqi Shen"
                    },
                    {
                        "@pid": "90/105",
                        "text": "Prateek Saxena"
                    },
                    {
                        "@pid": "99/4951",
                        "text": "Zhenkai Liang"
                    }
                ]
            },
            "title": "Neural Nets Can Learn Function Type Signatures From Binaries.",
            "venue": "USENIX Security Symposium",
            "pages": "99-116",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/ChuaSSL17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/chua",
            "url": "https://dblp.org/rec/conf/uss/ChuaSSL17",
            "abstract": "Function type signatures are important for binary analysis, but they are not available in COTS binaries. In this paper, we present a new system called E KLAVYA which trains a recurrent neural network to recover function type signatures from disassembled binary code. E KLAVYA assumes no knowledge of the target instruction set semantics to make such inference. More importantly, E KLAVYA results are \u201cexplicable\u201d: we \ufb01nd by analyzing its model that it auto-learns relationships between instructions, compiler conventions, stack frame setup instructions, use-before-write patterns, and operations relevant to identifying types directly from binaries. In our evaluation on Linux binaries compiled with clang and gcc , for two different architectures (x86 and x64), E KLAVYA exhibits accuracy of around 84% and 81% for function argument count and type recovery tasks respectively. E KLAVYA generalizes well across the compilers tested on two different instruction sets with various optimization levels, without any specialized prior knowledge of the instruction set, compiler or optimization level.",
            "keywords": [
                "Binary Analysis",
                "Function Type Signatures",
                "Disassembled Code",
                "Neural Network Training",
                "Type Recovery"
            ]
        },
        "url": "URL#3511142",
        "sema_paperId": "520176be03fae892b0265c820c7512d16cecb6d8"
    },
    {
        "@score": "1",
        "@id": "3511144",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "90/8396",
                        "text": "Taejoong Chung"
                    },
                    {
                        "@pid": "155/5773",
                        "text": "Roland van Rijswijk-Deij"
                    },
                    {
                        "@pid": "c/BChandrasekaran2",
                        "text": "Balakrishnan Chandrasekaran 0002"
                    },
                    {
                        "@pid": "48/6854",
                        "text": "David R. Choffnes"
                    },
                    {
                        "@pid": "03/6428",
                        "text": "Dave Levin"
                    },
                    {
                        "@pid": "m/BruceMMaggs",
                        "text": "Bruce M. Maggs"
                    },
                    {
                        "@pid": "31/3833",
                        "text": "Alan Mislove"
                    },
                    {
                        "@pid": "79/5135",
                        "text": "Christo Wilson"
                    }
                ]
            },
            "title": "A Longitudinal, End-to-End View of the DNSSEC Ecosystem.",
            "venue": "USENIX Security Symposium",
            "pages": "1307-1322",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/ChungR0CLMMW17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/chung",
            "url": "https://dblp.org/rec/conf/uss/ChungR0CLMMW17",
            "abstract": "The Domain Name System's Security Extensions (DNSSEC) allow clients and resolvers to verify that DNS responses have not been forged or modified inflight. DNSSEC uses a public key infrastructure (PKI) to achieve this integrity, without which users can be subject to a wide range of attacks. However, DNSSEC can operate only if each of the principals in its PKI properly performs its management tasks: authoritative name servers must generate and publish their keys and signatures correctly, child zones that support DNSSEC must be correctly signed with their parent's keys, and resolvers must actually validate the chain of signatures. \n \nThis paper performs the first large-scale, longitudinal measurement study into how well DNSSEC's PKI is managed. We use data from all DNSSEC-enabled subdomains under the .com, .org, and .net TLDs over a period of 21 months to analyze DNSSEC deployment and management by domains; we supplement this with active measurements of more than 59K DNS resolvers worldwide to evaluate resolver-side validation. \n \nOur investigation reveals pervasive mismanagement of the DNSSEC infrastructure. For example, we found that 31% of domains that support DNSSEC fail to publish all relevant records required for validation; 39% of the domains use insufficiently strong key-signing keys; and although 82% of resolvers in our study request DNSSEC records, only 12% of them actually attempt to validate them. These results highlight systemic problems, which motivate improved automation and auditing of DNSSEC management.",
            "keywords": [
                "DNSSEC",
                "Public Key Infrastructure",
                "DNS Management",
                "Domain Mismanagement",
                "Resolver Validation"
            ]
        },
        "url": "URL#3511144",
        "sema_paperId": "86a4a83a5d404c6116e76573e4fe3ec08054fc63"
    },
    {
        "@score": "1",
        "@id": "3511145",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "139/6635",
                        "text": "Thurston H. Y. Dang"
                    },
                    {
                        "@pid": "m/PetrosManiatis",
                        "text": "Petros Maniatis"
                    },
                    {
                        "@pid": "42/5626",
                        "text": "David A. Wagner 0001"
                    }
                ]
            },
            "title": "Oscar: A Practical Page-Permissions-Based Scheme for Thwarting Dangling Pointers.",
            "venue": "USENIX Security Symposium",
            "pages": "815-832",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/DangM017",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/dang",
            "url": "https://dblp.org/rec/conf/uss/DangM017",
            "abstract": "Using memory after it has been freed opens programs up to both data and control-flow exploits. Recent work on temporal memory safety has focused on using explicit lock-and-key mechanisms (objects are assigned a new lock upon allocation, and pointers must have the correct key to be dereferenced) or corrupting the pointer values upon free(). Placing objects on separate pages and using page permissions to enforce safety is an older, wellknown technique that has been maligned as too slow, without comprehensive analysis. We show that both old and new techniques are conceptually instances of lockand-key, and argue that, in principle, page permissions should be the most desirable approach. We then validate this insight experimentally by designing, implementing, and evaluating Oscar, a new protection scheme based on page permissions. Unlike prior attempts, Oscar does not require source code, is compatible with standard and custom memory allocators, and works correctly with programs that fork. Also, Oscar performs favorably \u2013 often by more than an order of magnitude \u2013 compared to recent proposals: overall, it has similar or lower runtime overhead, and lower memory overhead than competing systems.",
            "keywords": [
                "Memory Safety",
                "Dangling Pointers",
                "Page Permissions",
                "Temporal Memory Safety",
                "Runtime Overhead"
            ]
        },
        "url": "URL#3511145",
        "sema_paperId": "dba12b6296f9b81bdbc7696a228b0ec0b25ef23c"
    },
    {
        "@score": "1",
        "@id": "3511147",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "19/2685-1",
                        "text": "Ren Ding 0001"
                    },
                    {
                        "@pid": "147/2276",
                        "text": "Chenxiong Qian"
                    },
                    {
                        "@pid": "69/6818",
                        "text": "Chengyu Song"
                    },
                    {
                        "@pid": "09/4583",
                        "text": "William Harris"
                    },
                    {
                        "@pid": "38/8882",
                        "text": "Taesoo Kim"
                    },
                    {
                        "@pid": "29/5976",
                        "text": "Wenke Lee"
                    }
                ]
            },
            "title": "Efficient Protection of Path-Sensitive Control Security.",
            "venue": "USENIX Security Symposium",
            "pages": "131-148",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/DingQSHKL17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/ding",
            "url": "https://dblp.org/rec/conf/uss/DingQSHKL17",
            "abstract": "Control-Flow Integrity (CFI), as a means to prevent control-flow hijacking attacks, enforces that each instruction transfers control to an address in a set of valid targets. The security guarantee of CFI thus depends on the definition of valid targets, which conventionally are defined as the result of a static analysis. Unfortunately, previous research has demonstrated that such a definition, and thus any implementation that enforces it, still allows practical control-flow attacks. In this work, we present a path-sensitive variation of CFI that utilizes runtime path-sensitive point-to analysis to compute the legitimate control transfer targets. We have designed and implemented a runtime environment, PITTYPAT, that enforces path-sensitive CFI efficiently by combining commodity, low-overhead hardware monitoring and a novel runtime points-to analysis. Our formal analysis and empirical evaluation demonstrate that, compared to CFI based on static analysis, PITTYPAT ensures that applications satisfy stronger security guarantees, with acceptable overhead for security-critical contexts.",
            "keywords": [
                "Control-Flow Integrity",
                "Path-Sensitive Analysis",
                "Runtime Security",
                "Control-Flow Hijacking",
                "PITTYPAT"
            ]
        },
        "url": "URL#3511147",
        "sema_paperId": "65ea39f3cb19e446d708b639060460c580a328e4"
    },
    {
        "@score": "1",
        "@id": "3511148",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "199/4885",
                        "text": "Craig Disselkoen"
                    },
                    {
                        "@pid": "131/5093",
                        "text": "David Kohlbrenner"
                    },
                    {
                        "@pid": "60/5029",
                        "text": "Leo Porter 0001"
                    },
                    {
                        "@pid": "t/DeanMTullsen",
                        "text": "Dean M. Tullsen"
                    }
                ]
            },
            "title": "Prime+Abort: A Timer-Free High-Precision L3 Cache Attack using Intel TSX.",
            "venue": "USENIX Security Symposium",
            "pages": "51-67",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/DisselkoenKPT17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/disselkoen",
            "url": "https://dblp.org/rec/conf/uss/DisselkoenKPT17",
            "abstract": "Last-Level Cache (LLC) attacks typically exploit timing side channels in hardware, and thus rely heavily on timers for their operation. Many proposed defenses against such side-channel attacks capitalize on this reliance. This paper presents PRIME+ABORT, a new cache attack which bypasses these defenses by not depending on timers for its function. Instead of a timing side channel, PRIME+ABORT leverages the Intel TSX hardware widely available in both server- and consumer-grade processors. This work shows that PRIME+ABORT is not only invulnerable to important classes of defenses, it also outperforms state-of-the-art LLC PRIME+PROBE attacks in both accuracy and efficiency, having a maximum detection speed (in events per second) 3\u00d7 higher than LLC PRIME+PROBE on Intel's Skylake architecture while producing fewer false positives.",
            "keywords": [
                "Cache Attacks",
                "Intel TSX",
                "Timing Side Channels",
                "Last-Level Cache (LLC)",
                "PRIME+ABORT"
            ]
        },
        "url": "URL#3511148",
        "sema_paperId": "ae7bc5ad8dd0a861fe0df7bab07d5b4c75376bb4"
    },
    {
        "@score": "1",
        "@id": "3511150",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "157/9161",
                        "text": "Kevin Eykholt"
                    },
                    {
                        "@pid": "p/AtulPrakash",
                        "text": "Atul Prakash 0001"
                    },
                    {
                        "@pid": "45/2495",
                        "text": "Barzan Mozafari"
                    }
                ]
            },
            "title": "Ensuring Authorized Updates in Multi-user Database-Backed Applications.",
            "venue": "USENIX Security Symposium",
            "pages": "1445-1462",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/EykholtPM17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/eykholt",
            "url": "https://dblp.org/rec/conf/uss/EykholtPM17",
            "abstract": "Database-backed applications rely on access control policies based on views to protect sensitive data from unauthorized parties. Current techniques assume that the application\u2019s database tables contain a column that enables mapping a user to rows in the table. This assumption allows database views or similar mechanisms to enforce per-user access controls. However, not all database tables contain sufficient information to map a user to rows in the table, as a result of database normalization, and thus, require the joining of multiple tables. In a survey of 10 popular open-source web applications, on average, 21% of the database tables require a join. This means that current techniques cannot enforce security policies on all update queries for these applications, due to a well-known view update problem. In this paper, we propose phantom extraction, a technique, which enforces per user access control policies on all database update queries. Phantom extraction does not make the same assumptions as previous work, and, more importantly, does not use database views as a core enforcement mechanism. Therefore, it does not fall victim to the view update problem. We have created SafeD as a practical access control solution, which uses our phantom extraction technique. SafeD uses a declarative language for defining security policies, while retaining the simplicity of database views. We evaluated our system on two popular databases for open source web applications, MySQL and Postgres. On MySQL, which has no built-in access control, we observe a 6% increase in transaction latency. On Postgres, SafeD outperforms the built-in access control by an order of magnitude when security policies involved joins.",
            "keywords": [
                "Database Access Control",
                "Multi-user Applications",
                "Phantom Extraction",
                "View Update Problem",
                "Row-level Security"
            ]
        },
        "url": "URL#3511150",
        "sema_paperId": "6606bb872ec146da70b28e46ded4d5fdb9fa6d54"
    },
    {
        "@score": "1",
        "@id": "3511151",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "52/8024",
                        "text": "Adrienne Porter Felt"
                    },
                    {
                        "@pid": "153/3948",
                        "text": "Richard Barnes 0001"
                    },
                    {
                        "@pid": "205/2258",
                        "text": "April King"
                    },
                    {
                        "@pid": "46/1779",
                        "text": "Chris Palmer"
                    },
                    {
                        "@pid": "205/2045",
                        "text": "Chris Bentzel"
                    },
                    {
                        "@pid": "93/5018",
                        "text": "Parisa Tabriz"
                    }
                ]
            },
            "title": "Measuring HTTPS Adoption on the Web.",
            "venue": "USENIX Security Symposium",
            "pages": "1323-1338",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/FeltBKPBT17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/felt",
            "url": "https://dblp.org/rec/conf/uss/FeltBKPBT17",
            "abstract": "HTTPS ensures that the Web has a base level of privacy and integrity. Security engineers, researchers, and browser vendors have long worked to spread HTTPS to as much of the Web as possible via outreach efforts, developer tools, and browser changes. How much progress have we made toward this goal of widespread HTTPS adoption? We gather metrics to benchmark the status and progress of HTTPS adoption on the Web in 2017. To evaluate HTTPS adoption from a user perspective, we collect large-scale, aggregate user metrics from two major browsers (Google Chrome and Mozilla Firefox). To measure HTTPS adoption from a Web developer perspective, we survey server support for HTTPS among top and long-tail websites. We draw on these metrics to gain insight into the current state of the HTTPS ecosystem.",
            "keywords": [
                "HTTPS Adoption",
                "Web Security",
                "User Metrics",
                "Developer Support",
                "Web Ecosystem"
            ]
        },
        "url": "URL#3511151",
        "sema_paperId": "2c0476d0825964abcc1ccd710bb176932847a5fc"
    },
    {
        "@score": "1",
        "@id": "3511155",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "181/1564",
                        "text": "Cesar Pereida Garc\u00eda"
                    },
                    {
                        "@pid": "25/1876",
                        "text": "Billy Bob Brumley"
                    }
                ]
            },
            "title": "Constant-Time Callees with Variable-Time Callers.",
            "venue": "USENIX Security Symposium",
            "pages": "83-98",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/GarciaB17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/garcia",
            "url": "https://dblp.org/rec/conf/uss/GarciaB17",
            "abstract": "Side-channel attacks are a serious threat to securitycritical software. To mitigate remote timing and cachetiming attacks, many ubiquitous cryptography software libraries feature constant-time implementations of cryptographic primitives. In this work, we disclose a vulnerability in OpenSSL 1.0.1u that recovers ECDSA private keys for the standardized elliptic curve P-256 despite the library featuring both constant-time curve operations and modular inversion with microarchitecture attack mitigations. Exploiting this defect, we target the errant modular inversion code path with a cache-timing and improved performance degradation attack, recovering the inversion state sequence. We propose a new approach of extracting a variable number of nonce bits from these sequences, and improve upon the best theoretical result to recover private keys in a lattice attack with as few as 50 signatures and corresponding traces. As far as we are aware, this is the first timing attack against OpenSSL ECDSA that does not target scalar multiplication, the first side-channel attack on cryptosystems leveraging P-256 constant-time scalar multiplication and furthermore, we extend our attack to TLS and SSH protocols, both linked to OpenSSL for P-256 ECDSA signing.",
            "keywords": [
                "Side-Channel Attacks",
                "Cryptographic Primitives",
                "OpenSSL Vulnerability",
                "ECDSA Private Key Recovery",
                "Cache-Timing Attack"
            ]
        },
        "url": "URL#3511155",
        "sema_paperId": "0e9b9e095fed2df3762ee5bf00ec4f15001a3a7c"
    },
    {
        "@score": "1",
        "@id": "3511156",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "161/2424",
                        "text": "Ioannis Gasparis"
                    },
                    {
                        "@pid": "31/8302",
                        "text": "Zhiyun Qian"
                    },
                    {
                        "@pid": "69/6818",
                        "text": "Chengyu Song"
                    },
                    {
                        "@pid": "k/SrikanthVKrishnamurthy",
                        "text": "Srikanth V. Krishnamurthy"
                    }
                ]
            },
            "title": "Detecting Android Root Exploits by Learning from Root Providers.",
            "venue": "USENIX Security Symposium",
            "pages": "1129-1144",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/GasparisQSK17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/gasparis",
            "url": "https://dblp.org/rec/conf/uss/GasparisQSK17",
            "abstract": "Malware that are capable of rooting Android phones are arguably, the most dangerous ones. Unfortunately, detecting the presence of root exploits in malware is a very challenging problem. This is because such malware typically target specific Android devices and/or OS versions and simply abort upon detecting that an expected runtime environment (e.g., specific vulnerable device driver or preconditions) is not present; thus, emulators such as Google Bouncer fail in triggering and revealing such root exploits. In this paper, we build a system RootExplorer, to tackle this problem. The key observation that drives the design of RootExplorer is that, in addition to malware, there are legitimate commercial grade Android apps backed by large companies that facilitate the rooting of phones, referred to as root providers or one-click root apps. By conducting extensive analysis on oneclick root apps, RootExplorer learns the precise preconditions and environmental requirements of root exploits. It then uses this information to construct proper analysis environments either in an emulator or on a smartphone testbed to effectively detect embedded root exploits in malware. Our extensive experimental evaluations with RootExplorer show that it is able to detect all malware samples known to perform root exploits and incurs no false positives. We have also found an app that is currently available on the markets, that has an embedded",
            "keywords": [
                "Android Malware Detection",
                "Root Exploits",
                "Root Providers",
                "Emulator Analysis",
                "Malware Analysis"
            ]
        },
        "url": "URL#3511156",
        "sema_paperId": "4754cf3c0e779e76579a117d2cf9e86c926cda2e"
    },
    {
        "@score": "1",
        "@id": "3511157",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "205/2229",
                        "text": "Nirnimesh Ghose"
                    },
                    {
                        "@pid": "15/3615",
                        "text": "Loukas Lazos"
                    },
                    {
                        "@pid": "l/MingLi3",
                        "text": "Ming Li 0003"
                    }
                ]
            },
            "title": "HELP: Helper-Enabled In-Band Device Pairing Resistant Against Signal Cancellation.",
            "venue": "USENIX Security Symposium",
            "pages": "433-450",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/GhoseLL17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/ghose",
            "url": "https://dblp.org/rec/conf/uss/GhoseLL17",
            "abstract": "Bootstrapping trust between wireless devices without entering or preloading secrets is a fundamental security problem in many applications, including home networking, mobile device tethering, and the Internet-of-Things. This is because many new wireless devices lack the necessary interfaces (keyboard, screen, etc.) to manually enter passwords, or are often preloaded with default keys that are easily leaked. Alternatively, two devices can establish a common secret by executing key agreement protocols. However, the latter are vulnerable to Man-in-theMiddle (MitM) attacks. In the wireless domain, MitM attacks can be launched by manipulating the over-the-air transmissions. The strongest form of manipulation is signal cancellation, which completely annihilates the signal at a targeted receiver. Recently, cancellation attacks were shown to be practical under predictable channel conditions, without an effective defense mechanism. In this paper, we propose HELP, a helper-assisted message integrity verification primitive that detects message manipulation and signal cancellation over the wireless channel (rather than prevent it). By leveraging transmissions from a helper device which has already established trust with one of the devices (e.g., the hub), we enable signal tampering detection with high probability. We then use HELP to build a device pairing protocol, which securely introduces new devices to the network without requiring them to share any secret keys with the existing devices beforehand. We carry out extensive analysis and real-world experiments to validate the security and performance of our proposed protocol.",
            "keywords": [
                "Wireless Device Pairing",
                "Signal Cancellation",
                "Message Integrity Verification",
                "Man-in-the-Middle Attacks",
                "Helper-Assisted Protocol"
            ]
        },
        "url": "URL#3511157",
        "sema_paperId": "0a0195eaf4cb94ce55e73bae65e4fd165ce114cc"
    },
    {
        "@score": "1",
        "@id": "3511160",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "170/1844-1",
                        "text": "Marc Green 0001"
                    },
                    {
                        "@pid": "140/6560",
                        "text": "Leandro Rodrigues Lima"
                    },
                    {
                        "@pid": "60/10279",
                        "text": "Andreas Zankl"
                    },
                    {
                        "@pid": "145/1722",
                        "text": "Gorka Irazoqui"
                    },
                    {
                        "@pid": "82/9413",
                        "text": "Johann Heyszl"
                    },
                    {
                        "@pid": "72/817",
                        "text": "Thomas Eisenbarth 0001"
                    }
                ]
            },
            "title": "AutoLock: Why Cache Attacks on ARM Are Harder Than You Think.",
            "venue": "USENIX Security Symposium",
            "pages": "1075-1091",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/GreenLZIHE17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/green",
            "url": "https://dblp.org/rec/conf/uss/GreenLZIHE17",
            "abstract": "Attacks on the microarchitecture of modern processors have become a practical threat to security and privacy in desktop and cloud computing. Recently, cache attacks have successfully been demonstrated on ARM based mobile devices, suggesting they are as vulnerable as their desktop or server counterparts. In this work, we show that previous literature might have left an overly pessimistic conclusion of ARM\u2019s security as we unveil AutoLock: an internal performance enhancement found in inclusive cache levels of ARM processors that adversely affects Evict+Time, Prime+Probe, and Evict+Reload attacks. AutoLock\u2019s presence on system-on-chips (SoCs) is not publicly documented, yet knowing that it is implemented is vital to correctly assess the risk of cache attacks. We therefore provide a detailed description of the feature and propose three ways to detect its presence on actual SoCs. We illustrate how AutoLock impedes cross-core cache evictions, but show that its effect can also be compensated in a practical attack. Our findings highlight the intricacies of cache attacks on ARM and suggest that a fair and comprehensive vulnerability assessment requires an in-depth understanding of ARM\u2019s cache architectures and rigorous testing across a broad range of ARM based devices.",
            "pdf_url": "",
            "keywords": [
                "Cache Attacks",
                "ARM Architecture",
                "Microarchitecture Security",
                "AutoLock Feature",
                "Cross-Core Cache Evictions"
            ]
        },
        "url": "URL#3511160"
    },
    {
        "@score": "1",
        "@id": "3511161",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "166/1494",
                        "text": "Daniel Gruss"
                    },
                    {
                        "@pid": "182/6599",
                        "text": "Julian Lettner"
                    },
                    {
                        "@pid": "119/7702",
                        "text": "Felix Schuster"
                    },
                    {
                        "@pid": "70/4765",
                        "text": "Olga Ohrimenko"
                    },
                    {
                        "@pid": "13/8190",
                        "text": "Istv\u00e1n Haller"
                    },
                    {
                        "@pid": "35/6770",
                        "text": "Manuel Costa"
                    }
                ]
            },
            "title": "Strong and Efficient Cache Side-Channel Protection using Hardware Transactional Memory.",
            "venue": "USENIX Security Symposium",
            "pages": "217-233",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/GrussLSOHC17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/gruss",
            "url": "https://dblp.org/rec/conf/uss/GrussLSOHC17",
            "abstract": "Cache-based side-channel attacks are a serious problem in multi-tenant environments, for example, modern cloud data centers. We address this problem with Cloak, a new technique that uses hardware transactional memory to prevent adversarial observation of cache misses on sensitive code and data. We show that Cloak provides strong protection against all known cache-based side-channel attacks with low performance overhead. We demonstrate the efficacy of our approach by retrofitting vulnerable code with Cloak and experimentally confirming immunity against state-of-the-art attacks. We also show that by applying Cloak to code running inside Intel SGX enclaves we can effectively block information leakage through cache side channels from enclaves, thus addressing one of the main weaknesses of SGX.",
            "keywords": [
                "Cache Side-Channel Attacks",
                "Hardware Transactional Memory",
                "Information Leakage",
                "Multi-Tenant Environments",
                "Intel SGX"
            ]
        },
        "url": "URL#3511161",
        "sema_paperId": "cd2f62b6218613b7e3f13b70b1410db610ca3c6a"
    },
    {
        "@score": "1",
        "@id": "3511162",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "150/5927",
                        "text": "Grant Ho"
                    },
                    {
                        "@pid": "70/5324",
                        "text": "Aashish Sharma"
                    },
                    {
                        "@pid": "48/7429",
                        "text": "Mobin Javed"
                    },
                    {
                        "@pid": "p/VernPaxson",
                        "text": "Vern Paxson"
                    },
                    {
                        "@pid": "42/5626",
                        "text": "David A. Wagner 0001"
                    }
                ]
            },
            "title": "Detecting Credential Spearphishing in Enterprise Settings.",
            "venue": "USENIX Security Symposium",
            "pages": "469-485",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/HoSJP017",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/ho",
            "url": "https://dblp.org/rec/conf/uss/HoSJP017",
            "abstract": "We present a new approach for detecting credential spearphishing attacks in enterprise settings. Our method uses features derived from an analysis of fundamental characteristics of spearphishing attacks, combined with a new non-parametric anomaly scoring technique for ranking alerts. We evaluate our technique on a multi-year dataset of over 370 million emails from a large enterprise with thousands of employees. Our system successfully detects 6 known spearphishing campaigns that succeeded (missing one instance); an additional 9 that failed; plus 2 successful spearphishing attacks that were previously unknown, thus demonstrating the value of our approach. We also establish that our detector\u2019s false positive rate is low enough to be practical: on average, a single analyst can investigate an entire month\u2019s worth of alerts in under 15 minutes. Comparing our anomaly scoring method against standard anomaly detection techniques, we find that standard techniques using the same features would need to generate at least 9 times as many alerts as our method to detect the same number of attacks.",
            "pdf_url": "",
            "keywords": [
                "Spearphishing Detection",
                "Anomaly Scoring",
                "Enterprise Email Security",
                "Credential Theft",
                "Phishing Campaigns"
            ]
        },
        "url": "URL#3511162",
        "sema_paperId": "b505d6036f7c12fbf9dc958c07f78df538e7b83a"
    },
    {
        "@score": "1",
        "@id": "3511163",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "188/6160",
                        "text": "Md Nahid Hossain"
                    },
                    {
                        "@pid": "161/7154",
                        "text": "Sadegh M. Milajerdi"
                    },
                    {
                        "@pid": "188/5931",
                        "text": "Junao Wang"
                    },
                    {
                        "@pid": "37/7954",
                        "text": "Birhanu Eshete"
                    },
                    {
                        "@pid": "88/4984",
                        "text": "Rigel Gjomemo"
                    },
                    {
                        "@pid": "90/1136-1",
                        "text": "R. Sekar 0001"
                    },
                    {
                        "@pid": "09/6787",
                        "text": "Scott D. Stoller"
                    },
                    {
                        "@pid": "90/5014",
                        "text": "V. N. Venkatakrishnan"
                    }
                ]
            },
            "title": "SLEUTH: Real-time Attack Scenario Reconstruction from COTS Audit Data.",
            "venue": "USENIX Security Symposium",
            "pages": "487-504",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/HossainMWEGSSV17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/hossain",
            "url": "https://dblp.org/rec/conf/uss/HossainMWEGSSV17",
            "abstract": "We present an approach and system for real-time reconstruction of attack scenarios on an enterprise host. To meet the scalability and real-time needs of the problem, we develop a platform-neutral, main-memory based, dependency graph abstraction of audit-log data. We then present efficient, tag-based techniques for attack detection and reconstruction, including source identification and impact analysis. We also develop methods to reveal the big picture of attacks by construction of compact, visual graphs of attack steps. Our system participated in a red team evaluation organized by DARPA and was able to successfully detect and reconstruct the details of the red team\u2019s attacks on hosts running Windows, FreeBSD and Linux.",
            "pdf_url": "",
            "keywords": [
                "Attack Reconstruction",
                "Audit Log Analysis",
                "Dependency Graph",
                "Real-time Detection",
                "Impact Analysis"
            ]
        },
        "url": "URL#3511163"
    },
    {
        "@score": "1",
        "@id": "3511164",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "190/4771-1",
                        "text": "Zhichao Hua 0001"
                    },
                    {
                        "@pid": "197/6723-1",
                        "text": "Jinyu Gu 0001"
                    },
                    {
                        "@pid": "01/615",
                        "text": "Yubin Xia"
                    },
                    {
                        "@pid": "31/6601-1",
                        "text": "Haibo Chen 0001"
                    },
                    {
                        "@pid": "86/680",
                        "text": "Binyu Zang"
                    },
                    {
                        "@pid": "96/5680",
                        "text": "Haibing Guan"
                    }
                ]
            },
            "title": "vTZ: Virtualizing ARM TrustZone.",
            "venue": "USENIX Security Symposium",
            "pages": "541-556",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/HuaGXCZG17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/hua",
            "url": "https://dblp.org/rec/conf/uss/HuaGXCZG17",
            "abstract": "ARM TrustZone, a security extension that provides a secure world, a trusted execution environment (TEE), to run security-sensitive code, has been widely adopted in mobile platforms. With the increasing momentum of ARM64 being adopted in server markets like cloud, it is likely to see TrustZone being adopted as a key pillar for cloud security. Unfortunately, TrustZone is not designed to be virtualizable as there is only one TEE provided by the hardware, which prevents it from being securely shared by multiple virtual machines (VMs). This paper conducts a study on variable approaches to virtualizing TrustZone in virtualized environments and then presents vTZ, a solution that securely provides each guest VM with a virtualized guest TEE using existing hardware. vTZ leverages the idea of separating functionality from protection by maintaining a secure co-running VM to serve as a guest TEE, while using the hardware TrustZone to enforce strong isolation among guest TEEs and the untrusted hypervisor. Specifically, vTZ uses a tiny monitor running within the physical TrustZone that securely interposes and virtualizes memory mapping and world switching. vTZ further leverages a few pieces of protected, self-contained code running in a Constrained Isolated Execution Environment (CIEE) to provide secure virtualization and isolation among multiple guest TEEs. We have implemented vTZ on Xen 4.8 on both ARMv7 and ARMv8 development boards. Evaluation using two common TEE-kernels (secure kernel running in TEE) such as seL4 1 and OP-TEE shows that vTZ provides strong security with small performance overhead.",
            "keywords": [
                "ARM TrustZone",
                "Virtualization",
                "Trusted Execution Environment (TEE)",
                "Secure Isolation",
                "vTZ"
            ]
        },
        "url": "URL#3511164",
        "sema_paperId": "bad6552f0b3e533194c9c1d8a26a5fd470b1b24d"
    },
    {
        "@score": "1",
        "@id": "3511167",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "164/3347",
                        "text": "Samuel Jero"
                    },
                    {
                        "@pid": "139/5808",
                        "text": "William Koch"
                    },
                    {
                        "@pid": "137/3754",
                        "text": "Richard Skowyra"
                    },
                    {
                        "@pid": "14/5114",
                        "text": "Hamed Okhravi"
                    },
                    {
                        "@pid": "n/CristinaNitaRotaru",
                        "text": "Cristina Nita-Rotaru"
                    },
                    {
                        "@pid": "144/6532",
                        "text": "David Bigelow"
                    }
                ]
            },
            "title": "Identifier Binding Attacks and Defenses in Software-Defined Networks.",
            "venue": "USENIX Security Symposium",
            "pages": "415-432",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/JeroKSONB17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/jero",
            "url": "https://dblp.org/rec/conf/uss/JeroKSONB17",
            "abstract": "In this work, we demonstrate a novel attack in SDN networks, Persona Hijacking, that breaks the bindings of all layers of the networking stack and fools the network infrastructure into believing that the attacker is the legitimate owner of the victim\u2019s identifiers, which significantly increases persistence. We then present a defense, SECUREBINDER, that prevents identifier binding attacks at all layers of the network by leveraging SDN\u2019s data and control plane separation, global network view, and programmatic control of the network, while building upon IEEE 802.1x as a root of trust. To evaluate its effectiveness we both implement it in a testbed and use model checking to verify the guarantees it provides.",
            "keywords": [
                "Software-Defined Networks",
                "Identifier Binding Attacks",
                "Persona Hijacking",
                "Network Security",
                "SECUREBINDER"
            ]
        },
        "url": "URL#3511167",
        "sema_paperId": "213e02d8ca1207155fc3e4a3ae931fba360426cb"
    },
    {
        "@score": "1",
        "@id": "3511168",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "205/2209",
                        "text": "Xiangkun Jia"
                    },
                    {
                        "@pid": "94/3019-8",
                        "text": "Chao Zhang 0008"
                    },
                    {
                        "@pid": "46/3714",
                        "text": "Purui Su"
                    },
                    {
                        "@pid": "33/4854-40",
                        "text": "Yi Yang 0040"
                    },
                    {
                        "@pid": "137/5262",
                        "text": "Huafeng Huang"
                    },
                    {
                        "@pid": "79/2089",
                        "text": "Dengguo Feng"
                    }
                ]
            },
            "title": "Towards Efficient Heap Overflow Discovery.",
            "venue": "USENIX Security Symposium",
            "pages": "989-1006",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/JiaZSYHF17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/jia",
            "url": "https://dblp.org/rec/conf/uss/JiaZSYHF17",
            "abstract": "Heap overflow is a prevalent memory corruption vulnerability, playing an important role in recent attacks. Finding such vulnerabilities in applications is thus critical for security. Many state-of-art solutions focus on runtime detection, requiring abundant inputs to explore program paths in order to reach a high code coverage and luckily trigger security violations. It is likely that the inputs being tested could exercise vulnerable program paths, but fail to trigger (and thus miss) vulnerabilities in these paths. Moreover, these solutions may also miss heap vulnerabilities due to incomplete vulnerability models. In this paper, we propose a new solution HOTracer to discover potential heap vulnerabilities. We model heap overflows as spatial inconsistencies between heap allocation and heap access operations, and perform an indepth offline analysis on representative program execution traces to identify heap overflows. Combining with several optimizations, it could efficiently find heap overflows that are hard to trigger in binary programs. We implemented a prototype of HOTracer, evaluated it on 17 real world applications, and found 47 previously unknown heap vulnerabilities, showing its effectiveness.",
            "keywords": [
                "Heap Overflow",
                "Memory Corruption",
                "Vulnerability Discovery",
                "Program Analysis",
                "HOTracer"
            ]
        },
        "url": "URL#3511168",
        "sema_paperId": "72e1e8172b1fdd24c17127a9bac0e7577875f613"
    },
    {
        "@score": "1",
        "@id": "3511170",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "160/9532",
                        "text": "Roberto Jordaney"
                    },
                    {
                        "@pid": "150/6191",
                        "text": "Kumar Sharad"
                    },
                    {
                        "@pid": "94/2235",
                        "text": "Santanu Kumar Dash 0001"
                    },
                    {
                        "@pid": "95/6543",
                        "text": "Zhi Wang"
                    },
                    {
                        "@pid": "15/10028",
                        "text": "Davide Papini"
                    },
                    {
                        "@pid": "17/210",
                        "text": "Ilia Nouretdinov"
                    },
                    {
                        "@pid": "95/5162",
                        "text": "Lorenzo Cavallaro"
                    }
                ]
            },
            "title": "Transcend: Detecting Concept Drift in Malware Classification Models.",
            "venue": "USENIX Security Symposium",
            "pages": "625-642",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/JordaneySDWPNC17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/jordaney",
            "url": "https://dblp.org/rec/conf/uss/JordaneySDWPNC17",
            "abstract": "Building machine learning models of malware behavior is widely accepted as a panacea towards effective malware classification. A crucial requirement for building sustainable learning models, though, is to train on a wide variety of malware samples. Unfortunately, malware evolves rapidly and it thus becomes hard\u2014if not impossible\u2014to generalize learning models to reflect future, previously-unseen behaviors. Consequently, most malware classifiers become unsustainable in the long run, becoming rapidly antiquated as malware continues to evolve. In this work, we propose Transcend, a framework to identify aging classification models in vivo during deployment, much before the machine learning model\u2019s performance starts to degrade. This is a significant departure from conventional approaches that retrain aging models retrospectively when poor performance is observed. Our approach uses a statistical comparison of samples seen during deployment with those used to train the model, thereby building metrics for prediction quality. We show how Transcend can be used to identify concept drift based on two separate case studies on Android andWindows malware, raising a red flag before the model starts making consistently poor decisions due to out-of-date training.",
            "keywords": [
                "Malware Classification",
                "Concept Drift",
                "Model Aging",
                "Statistical Comparison",
                "Prediction Quality"
            ]
        },
        "url": "URL#3511170",
        "sema_paperId": "12f0a5268975d535761d99819856992f7021780b"
    },
    {
        "@score": "1",
        "@id": "3511172",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "131/5093",
                        "text": "David Kohlbrenner"
                    },
                    {
                        "@pid": "35/1061",
                        "text": "Hovav Shacham"
                    }
                ]
            },
            "title": "On the effectiveness of mitigations against floating-point timing channels.",
            "venue": "USENIX Security Symposium",
            "pages": "69-81",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/KohlbrennerS17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/kohlbrenner",
            "url": "https://dblp.org/rec/conf/uss/KohlbrennerS17",
            "abstract": "The duration of floating-point instructions is a known timing side channel that has been used to break Same-Origin Policy (SOP) privacy on Mozilla Firefox and the Fuzz differentially private database. Several defenses have been proposed to mitigate these attacks. We present detailed benchmarking of floating point performance for various operations based on operand values. We identify families of values that induce slow and fast paths beyond the classes (normal, subnormal, etc.) considered in previous work, and note that different processors exhibit different timing behavior. We evaluate the efficacy of the defenses deployed (or not) in Web browsers to floating point side channel attacks on SVG filters. We find that Google Chrome, Mozilla Firefox, and Apple\u2019s Safari have insufficiently addressed the floating-point side channel, and we present attacks for each that extract pixel data cross-origin on most platforms. We evaluate the vector-operation based defensive mechanism proposed at USENIX Security 2016 by Rane, Lin and Tiwari and find that it only reduces, not eliminates, the floating-point side channel signal. Together, these measurements and attacks cause us to conclude that floating point is simply too variable to use in a timing security sensitive context.",
            "keywords": [
                "Floating-Point Timing Channels",
                "Web Browser Security",
                "Cross-Origin Data Leakage",
                "Timing Side Channels",
                "Mitigation Efficacy"
            ]
        },
        "url": "URL#3511172",
        "sema_paperId": "1aea7174b6e709d09570738cdfe4720b841e0398"
    },
    {
        "@score": "1",
        "@id": "3511173",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "150/9456",
                        "text": "Philipp Koppe"
                    },
                    {
                        "@pid": "153/5832",
                        "text": "Benjamin Kollenda"
                    },
                    {
                        "@pid": "135/0314",
                        "text": "Marc Fyrbiak"
                    },
                    {
                        "@pid": "135/0525",
                        "text": "Christian Kison"
                    },
                    {
                        "@pid": "150/5154",
                        "text": "Robert Gawlik"
                    },
                    {
                        "@pid": "p/ChristofPaar",
                        "text": "Christof Paar"
                    },
                    {
                        "@pid": "h/ThorstenHolz",
                        "text": "Thorsten Holz"
                    }
                ]
            },
            "title": "Reverse Engineering x86 Processor Microcode.",
            "venue": "USENIX Security Symposium",
            "pages": "1163-1180",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/KoppeKFKGPH17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/koppe",
            "url": "https://dblp.org/rec/conf/uss/KoppeKFKGPH17",
            "abstract": "Microcode is an abstraction layer on top of the physical components of a CPU and present in most general-purpose CPUs today. In addition to facilitate complex and vast instruction sets, it also provides an update mechanism that allows CPUs to be patched in-place without requiring any special hardware. While it is well-known that CPUs are regularly updated with this mechanism, very little is known about its inner workings given that microcode and the update mechanism are proprietary and have not been throughly analyzed yet.\nIn this paper, we reverse engineer the microcode semantics and inner workings of its update mechanism of conventional COTS CPUs on the example of AMD\u2019s K8 and K10 microarchitectures. Furthermore, we demonstrate how to develop custom microcode updates. We describe the microcode semantics and additionally present a set of microprograms that demonstrate the possibilities offered by this technology. To this end, our microprograms range from CPU-assisted instrumentation to microcoded Trojans that can even be reached from within a web browser and enable remote code execution and cryptographic implementation attacks.",
            "pdf_url": "",
            "keywords": [
                "Microcode",
                "CPU Architecture",
                "Reverse Engineering",
                "Microcode Updates",
                "Microcoded Trojans"
            ]
        },
        "url": "URL#3511173"
    },
    {
        "@score": "1",
        "@id": "3511174",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "128/4803",
                        "text": "Katharina Krombholz"
                    },
                    {
                        "@pid": "170/0100",
                        "text": "Wilfried Mayer"
                    },
                    {
                        "@pid": "169/9708",
                        "text": "Martin Schmiedecker"
                    },
                    {
                        "@pid": "w/EdgarRWeippl",
                        "text": "Edgar R. Weippl"
                    }
                ]
            },
            "title": "&quot;I Have No Idea What I&apos;m Doing&quot; - On the Usability of Deploying HTTPS.",
            "venue": "USENIX Security Symposium",
            "pages": "1339-1356",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/KrombholzMSW17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/krombholz",
            "url": "https://dblp.org/rec/conf/uss/KrombholzMSW17",
            "abstract": "Protecting communication content at scale is a difficult task, and TLS is the protocol most commonly used to do so. However, it has been shown that deploying it in a truly secure fashion is challenging for a large fraction of online service operators. While Let\u2019s Encrypt was specifically built and launched to promote the adoption of HTTPS, this paper aims to understand the reasons for why it has been so hard to deploy TLS correctly and studies the usability of the deployment process for HTTPS. We performed a series of experiments with 28 knowledgable participants and revealed significant usability challenges that result in weak TLS configurations. Additionally, we conducted expert interviews with 7 experienced security auditors. Our results suggest that the deployment process is far too complex even for people with proficient knowledge in the field, and that server configurations should have stronger security by default. While the results from our expert interviews confirm the ecological validity of the lab study results, they additionally highlight that even educated users prefer solutions that are easy to use. An improved and less vulnerable workflow would be very beneficial to finding stronger configurations in the wild.",
            "keywords": [
                "HTTPS Deployment",
                "TLS Usability",
                "Security Configuration",
                "User Experience",
                "Weak TLS Configurations"
            ]
        },
        "url": "URL#3511174",
        "sema_paperId": "b8ce674c135e1379ec42149289699ad28a0a64fe"
    },
    {
        "@score": "1",
        "@id": "3511175",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "160/8004",
                        "text": "Russell W. F. Lai"
                    },
                    {
                        "@pid": "33/6847-1",
                        "text": "Christoph Egger 0001"
                    },
                    {
                        "@pid": "52/6199",
                        "text": "Dominique Schr\u00f6der"
                    },
                    {
                        "@pid": "c/ShermanSMChow",
                        "text": "Sherman S. M. Chow"
                    }
                ]
            },
            "title": "Phoenix: Rebirth of a Cryptographic Password-Hardening Service.",
            "venue": "USENIX Security Symposium",
            "pages": "899-916",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/Lai0SC17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/lai",
            "url": "https://dblp.org/rec/conf/uss/Lai0SC17",
            "abstract": "Password remains the most widespread means of authentication, especially on the Internet. As such, it is the Achilles heel of many modern systems. Facebook pioneered using external cryptographic services to harden password-based authentication in a large scale. Everspaugh et al. (Usenix Security \u201915) provided the first comprehensive treatment of such a service and proposed the Pythia PRF-Service as a cryptographically secure solution. Recently, Schneider et al. (ACM CCS \u201916) proposed a more efficient solution which is secure in a weaker security model. In this work, we show that the scheme of Schneider et al. is vulnerable to offline attacks just after a single validation query. Therefore, it defeats the purpose of using an external crypto service in the first place and it should not be used in practice. Our attacks do not contradict their security claims, but instead show that their definitions are simply too weak. We thus suggest stronger security definitions that cover these kinds of real-world attacks, and an even more efficient construction, Phoenix, to achieve them. Our comprehensive evaluation confirms the practicability of Phoenix: It can handle up to 50% more requests than the scheme of Schneider et al. and up to three times more than Pythia.",
            "keywords": [
                "Cryptographic Password Hardening",
                "External Cryptographic Services",
                "Offline Attacks",
                "Security Definitions",
                "Phoenix Construction"
            ]
        },
        "url": "URL#3511175",
        "sema_paperId": "24eaa6529636044126ee866b60e5e29a983003b6"
    },
    {
        "@score": "1",
        "@id": "3511176",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "00/3175",
                        "text": "Tobias Lauinger"
                    },
                    {
                        "@pid": "02/8037",
                        "text": "Abdelberi Chaabane"
                    },
                    {
                        "@pid": "159/9706",
                        "text": "Ahmet Salih Buyukkayhan"
                    },
                    {
                        "@pid": "77/8031",
                        "text": "Kaan Onarlioglu"
                    },
                    {
                        "@pid": "r/WilliamKRobertson",
                        "text": "William Robertson 0002"
                    }
                ]
            },
            "title": "Game of Registrars: An Empirical Analysis of Post-Expiration Domain Name Takeovers.",
            "venue": "USENIX Security Symposium",
            "pages": "865-880",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/LauingerCBO017",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/lauinger",
            "url": "https://dblp.org/rec/conf/uss/LauingerCBO017",
            "abstract": "Every day, hundreds of thousands of Internet domain names are abandoned by their owners and become available for re-registration. Yet, there appears to be enough residual value and demand from domain speculators to give rise to a highly competitive ecosystem of drop-catch services that race to be the first to re-register potentially desirable domain names in the very instant the old registration is deleted. To pre-empt the competitive (and uncertain) race to re-registration, some registrars sell their own customers\u2019 expired domains pre-release, that is, even before the names are returned to general availability. These practices are not without controversy, and can have serious security consequences. In this paper, we present an empirical analysis of these two kinds of postexpiration domain ownership changes.We find that 10 % of all com domains are re-registered on the same day as their old registration is deleted. In the case of org, over 50 % of re-registrations on the deletion day occur during only 30 s. Furthermore, drop-catch services control over 75 % of accredited domain registrars and cause more than 80 % of domain creation attempts, but represent at most 9.5 % of successful domain creations. These findings highlight a significant demand for expired domains, and hint at highly competitive re-registrations. Our work sheds light on various questionable practices in an opaque ecosystem. The implications go beyond the annoyance of websites turned into \u201cInternet graffiti\u201d [26], as domain ownership changes have the potential to circumvent established security mechanisms.",
            "keywords": [
                "Domain Name System",
                "Post-Expiration Domain Takeovers",
                "Drop-Catch Services",
                "Domain Re-Registration",
                "Security Implications of Domain Ownership Changes"
            ]
        },
        "url": "URL#3511176",
        "sema_paperId": "2e570b7afc09acb2f32ae388db8b1c983e191ca0"
    },
    {
        "@score": "1",
        "@id": "3511177",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "65/1125",
                        "text": "Jae-Hyuk Lee"
                    },
                    {
                        "@pid": "43/6896",
                        "text": "Jin Soo Jang"
                    },
                    {
                        "@pid": "150/5222",
                        "text": "Yeongjin Jang"
                    },
                    {
                        "@pid": "205/2101",
                        "text": "Nohyun Kwak"
                    },
                    {
                        "@pid": "205/2238",
                        "text": "Yeseul Choi"
                    },
                    {
                        "@pid": "73/6184",
                        "text": "Changho Choi"
                    },
                    {
                        "@pid": "38/8882",
                        "text": "Taesoo Kim"
                    },
                    {
                        "@pid": "56/6925",
                        "text": "Marcus Peinado"
                    },
                    {
                        "@pid": "17/6702",
                        "text": "Brent ByungHoon Kang"
                    }
                ]
            },
            "title": "Hacking in Darkness: Return-oriented Programming against Secure Enclaves.",
            "venue": "USENIX Security Symposium",
            "pages": "523-539",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/LeeJJKCCKPK17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/lee-jaehyuk",
            "url": "https://dblp.org/rec/conf/uss/LeeJJKCCKPK17",
            "abstract": "Intel Software Guard Extensions (SGX) is a hardware-based Trusted Execution Environment (TEE) that is widely seen as a promising solution to traditional security threats. While SGX promises strong protections to bug-free software, decades of experience show that we have to expect vulnerabilities in any non-trivial application. In a traditional environment, such vulnerabilities often allow attackers to take complete control over the vulnerable systems. Efforts to evaluate the security of SGX have been focusing on side-channels. So far, neither a practical attack against a vulnerability in enclave code nor a proof-of-concept attack scenario has been demonstrated. Thus, a fundamental question remains: What are the consequences and dangers of having a memory corruption vulnerability in enclave code? To answer this question, we comprehensively analyze the exploitation technique against vulnerabilities inside enclaves. We demonstrate practical exploitation techniques, called Dark-ROP, which can completely disarm the security guarantees of SGX. Dark-ROP exploits a memory corruption vulnerability in the enclave software through return-oriented programming (ROP), but it differs significantly in a sense that the target enclave runs under a solid hardware protection. We overcome this problem by exploiting SGX-specific properties and obstacles by formulating a novel ROP attack scheme against SGX under practical assumptions. Specifically, we have built several oracles that tell the status of enclave execution to the attacker in order to enable launching of ROP attack while both code and data are hidden. Additionally, we exfiltrate the enclave\u2019s code and data into a shadow application to fully control the execution environment while satisfying all security requirements of SGX. This shadow application emulates the enclave under the complete control of the attacker, using the enclave (through ROP calls) only to perform SGX operations such as reading the enclave\u2019s SGX crypto keys. The consequences of Dark-ROP are alarming; the attacker can completely breach the enclave\u2019s memory protections and trick the SGX hardware into disclosing the enclave\u2019s encryption keys and producing measurement reports that defeat remote attestation. This result strongly suggests that traditional security mitigation should be taken more seriously than common directions that communities are actively taking for convince (e.g., Haven or Graphene), which essentially increase the trust computing base and the attack surface.",
            "keywords": [
                "Trusted Execution Environments",
                "Intel SGX",
                "Memory Corruption",
                "Return-oriented Programming",
                "Dark-ROP"
            ]
        },
        "url": "URL#3511177",
        "sema_paperId": "40b438a3c824c396dd1cb50a3dbad9df5ce2d8d5"
    },
    {
        "@score": "1",
        "@id": "3511178",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "40/2903",
                        "text": "Zhihao Li"
                    },
                    {
                        "@pid": "205/2182",
                        "text": "Stephen Herwig"
                    },
                    {
                        "@pid": "03/6428",
                        "text": "Dave Levin"
                    }
                ]
            },
            "title": "DeTor: Provably Avoiding Geographic Regions in Tor.",
            "venue": "USENIX Security Symposium",
            "pages": "343-359",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/LiHL17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/li",
            "url": "https://dblp.org/rec/conf/uss/LiHL17",
            "abstract": "Large, routing-capable adversaries such as nationstates have the ability to censor and launch powerful deanonymization attacks against Tor circuits that traverse their borders. Tor allows users to specify a set of countries to exclude from circuit selection, but this provides merely the illusion of control, as it does not preclude those countries from being on the path between nodes in a circuit. For instance, we find that circuits excluding US Tor nodes definitively avoid the US 12% of the time. This paper presents DeTor, a set of techniques for proving when a Tor circuit has avoided user-specified geographic regions. DeTor extends recent work on using speed-of-light constraints to prove that a round-trip of communication physically could not have traversed certain geographic regions. As such, DeTor does not require modifications to the Tor protocol, nor does it require a map of the Internet\u2019s topology. We show how DeTor can be used to avoid censors (by never transiting the censor once) and to avoid timing-based deanonymization attacks (by never transiting a geographic region twice). We analyze DeTor\u2019s success at finding avoidance circuits through simulation using real latencies from Tor.",
            "keywords": [
                "Tor Network",
                "Geographic Avoidance",
                "Censorship Resistance",
                "Deanonymization Attacks",
                "Latency-based Circuit Selection"
            ]
        },
        "url": "URL#3511178",
        "sema_paperId": "8ded49b2c999db2ac00fe0d4f1600060a2b629c6"
    },
    {
        "@score": "1",
        "@id": "3511179",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "67/5807",
                        "text": "Fang Liu"
                    },
                    {
                        "@pid": "42/675",
                        "text": "Chun Wang"
                    },
                    {
                        "@pid": "169/1376",
                        "text": "Andres Pico"
                    },
                    {
                        "@pid": "y/DanfengYao",
                        "text": "Danfeng Yao"
                    },
                    {
                        "@pid": "71/4292-11",
                        "text": "Gang Wang 0011"
                    }
                ]
            },
            "title": "Measuring the Insecurity of Mobile Deep Links of Android.",
            "venue": "USENIX Security Symposium",
            "pages": "953-969",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/LiuWPYW17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/liu",
            "url": "https://dblp.org/rec/conf/uss/LiuWPYW17",
            "abstract": "Mobile deep links are URIs that point to specific locations within apps, which are instrumental to web-to-app communications. Existing \u201cscheme URLs\u201d are known to have hijacking vulnerabilities where one app can freely register another app\u2019s schemes to hijack the communication. Recently, Android introduced two new methods \u201cApp links\u201d and \u201cIntent URLs\u201d which were designed with security features, to replace scheme URLs. While the new mechanisms are secure in theory, little is known about how effective they are in practice. In this paper, we conduct the first empirical measurement on various mobile deep links across apps and websites. Our analysis is based on the deep links extracted from two snapshots of 160,000+ top Android apps from Google Play (2014 and 2016), and 1 million webpages from Alexa top domains. We find that the new linking methods (particularly App links) not only failed to deliver the security benefits as designed, but significantly worsen the situation. First, App links apply link verification to prevent hijacking. However, only 194 apps (2.2% out of 8,878 apps with App links) can pass the verification due to incorrect (or no) implementations. Second, we identify a new vulnerability in App link\u2019s preference setting, which allows a malicious app to intercept arbitrary HTTPS URLs in the browser without raising any alerts. Third, we identify more hijacking cases on App links than existing scheme URLs among both apps and websites. Many of them are targeting popular sites such as online social networks. Finally, Intent URLs have little impact in mitigating hijacking risks due to a low adoption rate on the web.",
            "keywords": [
                "Mobile Deep Links",
                "Android Security",
                "App Links",
                "Hijacking Vulnerabilities",
                "Intent URLs"
            ]
        },
        "url": "URL#3511179",
        "sema_paperId": "d666f8eb537d93580fb2a372d967263ba10ee580"
    },
    {
        "@score": "1",
        "@id": "3511181",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "145/1016",
                        "text": "Loi Luu"
                    },
                    {
                        "@pid": "54/9298",
                        "text": "Yaron Velner"
                    },
                    {
                        "@pid": "77/1405",
                        "text": "Jason Teutsch"
                    },
                    {
                        "@pid": "90/105",
                        "text": "Prateek Saxena"
                    }
                ]
            },
            "title": "SmartPool: Practical Decentralized Pooled Mining.",
            "venue": "USENIX Security Symposium",
            "pages": "1409-1426",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/LuuVTS17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/luu",
            "url": "https://dblp.org/rec/conf/uss/LuuVTS17",
            "abstract": "Cryptocurrencies such as Bitcoin and Ethereum are operated by a handful of mining pools. Nearly 95% of Bitcoin\u2019s and 80% of Ethereum\u2019s mining power resides with less than ten and six mining pools respectively. Although miners benefit from low payout variance in pooled mining, centralized mining pools require members to trust that pool operators will remunerate them fairly. Furthermore, centralized pools pose the risk of transaction censorship from pool operators, and open up possibilities for collusion between pools for perpetrating severe attacks.\nIn this work, we propose SMARTPOOL, a novel protocol design for a decentralized mining pool. Our protocol shows how one can leverage smart contracts, autonomous blockchain programs, to decentralize cryptocurrency mining. SMARTPOOL gives transaction selection control back to miners while yielding low-variance payouts. SMARTPOOL incurs mining fees lower than centralized mining pools and is designed to scale to a large number of miners. We implemented and deployed a robust SMARTPOOL implementation on the Ethereum and Ethereum Classic networks. To date, our deployed pools have handled a peak hashrate of 30 GHs from Ethereum miners, resulting in 105 blocks, costing miners a mere 0:6% of block rewards in transaction fees.",
            "pdf_url": "",
            "keywords": [
                "Decentralized Mining Pools",
                "Smart Contracts",
                "Cryptocurrency Mining",
                "Transaction Censorship",
                "Low-Variance Payouts"
            ]
        },
        "url": "URL#3511181"
    },
    {
        "@score": "1",
        "@id": "3511182",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "172/8745",
                        "text": "Shiqing Ma"
                    },
                    {
                        "@pid": "154/5678",
                        "text": "Juan Zhai"
                    },
                    {
                        "@pid": "52/3194-1",
                        "text": "Fei Wang 0001"
                    },
                    {
                        "@pid": "31/9698",
                        "text": "Kyu Hyung Lee"
                    },
                    {
                        "@pid": "95/3760-1",
                        "text": "Xiangyu Zhang 0001"
                    },
                    {
                        "@pid": "59/5539",
                        "text": "Dongyan Xu"
                    }
                ]
            },
            "title": "MPI: Multiple Perspective Attack Investigation with Semantic Aware Execution Partitioning.",
            "venue": "USENIX Security Symposium",
            "pages": "1111-1128",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/MaZ0LZX17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/ma",
            "url": "https://dblp.org/rec/conf/uss/MaZ0LZX17",
            "abstract": "Traditional auditing techniques generate large and inaccurate causal graphs. To overcome such limitations, researchers proposed to leverage execution partitioning to improve analysis granularity and hence precision. However, these techniques rely on a low level programming paradigm (i.e., event handling loops) to partition execution, which often results in low level graphs with a lot of redundancy. This not only leads to space inefficiency and noises in causal graphs, but also makes it difficult to understand attack provenance. Moreover, these techniques require training to detect low level memory dependencies across partitions. Achieving correctness and completeness in the training is highly challenging. In this paper, we propose a semantics aware program annotation and instrumentation technique to partition execution based on the application specific high level task structures. It avoids training, generates execution partitions with rich semantic information and provides multiple perspectives of an attack. We develop a prototype and integrate it with three different provenance systems: the Linux Audit system, ProTracer and the LPM-HiFi system. The evaluation results show that our technique generates cleaner attack graphs with rich high-level semantics and has much lower space and time overheads, when compared with the event loop based partitioning techniques BEEP and ProTracer.",
            "keywords": [
                "Execution Partitioning",
                "Causal Graphs",
                "Attack Provenance",
                "Semantic Aware Annotation",
                "Multiple Perspective Analysis"
            ]
        },
        "url": "URL#3511182",
        "sema_paperId": "ddedcc47d8326c4dc4dad226fa432b1b91389796"
    },
    {
        "@score": "1",
        "@id": "3511183",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "133/8188",
                        "text": "Aravind Machiry"
                    },
                    {
                        "@pid": "30/7555",
                        "text": "Chad Spensky"
                    },
                    {
                        "@pid": "205/2073",
                        "text": "Jake Corina"
                    },
                    {
                        "@pid": "182/4668",
                        "text": "Nick Stephens"
                    },
                    {
                        "@pid": "k/ChristopherKruegel",
                        "text": "Christopher Kruegel"
                    },
                    {
                        "@pid": "v/GiovanniVigna",
                        "text": "Giovanni Vigna"
                    }
                ]
            },
            "title": "DR. CHECKER: A Soundy Analysis for Linux Kernel Drivers.",
            "venue": "USENIX Security Symposium",
            "pages": "1007-1024",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/MachirySCSKV17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/machiry",
            "url": "https://dblp.org/rec/conf/uss/MachirySCSKV17",
            "abstract": "While kernel drivers have long been know to poses huge security risks, due to their privileged access and lower code quality, bug-finding tools for drivers are still greatly lacking both in quantity and effectiveness. This is because the pointer-heavy code in these drivers present some of the hardest challenges to static analysis, and their tight coupling with the hardware make dynamic analysis infeasible in most cases. In this work, we present DR. CHECKER, a soundy (i.e., mostly sound) bug-finding tool for Linux kernel drivers that is based on well-known program analysis techniques. We are able to overcome many of the inherent limitations of static analysis by scoping our analysis to only the most bug-prone parts of the kernel (i.e., the drivers), and by only sacrificing soundness in very few cases to ensure that our technique is both scalable and precise. DR. CHECKER is a fully-automated static analysis tool capable of performing general bug finding using both pointer and taint analyses that are flow-sensitive, context-sensitive, and fieldsensitive on kernel drivers. To demonstrate the scalability and efficacy of DR. CHECKER, we analyzed the drivers of nine production Linux kernels (3.1 million LOC), where it correctly identified 158 critical zero-day bugs with an overall precision of 78%.",
            "keywords": [
                "Linux Kernel Drivers",
                "Static Analysis",
                "Bug Finding",
                "Pointer Analysis",
                "Zero-Day Bugs"
            ]
        },
        "url": "URL#3511183",
        "sema_paperId": "329307382d223c34805a99c7c1dd447a79cf314f"
    },
    {
        "@score": "1",
        "@id": "3511185",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "180/8226",
                        "text": "Ian D. Markwood"
                    },
                    {
                        "@pid": "195/7475",
                        "text": "Dakun Shen"
                    },
                    {
                        "@pid": "64/424-7",
                        "text": "Yao Liu 0007"
                    },
                    {
                        "@pid": "41/4718",
                        "text": "Zhuo Lu"
                    }
                ]
            },
            "title": "PDF Mirage: Content Masking Attack Against Information-Based Online Services.",
            "venue": "USENIX Security Symposium",
            "pages": "833-847",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/MarkwoodSLL17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/markwood",
            "url": "https://dblp.org/rec/conf/uss/MarkwoodSLL17",
            "abstract": "We present a new class of content masking attacks against the Adobe PDF standard, causing documents to appear to humans dissimilar to the underlying content extracted by information-based services. We show three attack variants with notable impact on real-world systems. Our \ufb01rst attack allows academic paper writers and re-viewers to collude via subverting the automatic reviewer assignment systems in current use by academic conferences including INFOCOM, which we reproduced. Our second attack renders ineffective plagiarism detection software, particularly Turnitin, targeting speci\ufb01c small plagiarism similarity scores to appear natural and evade detection. In our \ufb01nal attack, we place masked content into the indexes for Bing, Yahoo!, and DuckDuckGo which renders as information entirely different from the keywords used to locate it, enabling spam, profane, or possibly illegal content to go unnoticed by these search engines but still returned in unrelated search results. Lastly, as these systems eschew optical character recognition (OCR) for its overhead, we offer a comprehensive and lightweight alternative mitigation method.",
            "keywords": [
                "Content Masking Attacks",
                "PDF Standard",
                "Information-Based Services",
                "Plagiarism Detection Evasion",
                "Search Engine Manipulation"
            ]
        },
        "url": "URL#3511185",
        "sema_paperId": "86a121b4eeca562de19ebc13dbf50f2a8a1b4d2f"
    },
    {
        "@score": "1",
        "@id": "3511186",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "120/1940",
                        "text": "Sinisa Matetic"
                    },
                    {
                        "@pid": "42/3969",
                        "text": "Mansoor Ahmed"
                    },
                    {
                        "@pid": "42/2366",
                        "text": "Kari Kostiainen"
                    },
                    {
                        "@pid": "118/3376",
                        "text": "Aritra Dhar"
                    },
                    {
                        "@pid": "241/9505",
                        "text": "David M. Sommer 0001"
                    },
                    {
                        "@pid": "138/9020",
                        "text": "Arthur Gervais"
                    },
                    {
                        "@pid": "j/AriJuels",
                        "text": "Ari Juels"
                    },
                    {
                        "@pid": "51/1639",
                        "text": "Srdjan Capkun"
                    }
                ]
            },
            "title": "ROTE: Rollback Protection for Trusted Execution.",
            "venue": "USENIX Security Symposium",
            "pages": "1289-1306",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/MateticAKDSGJC17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/matetic",
            "url": "https://dblp.org/rec/conf/uss/MateticAKDSGJC17",
            "abstract": "Security architectures such as Intel SGX need protection against rollback attacks, where the adversary violates the integrity of a protected application state by replaying old persistently stored data or by starting multiple application instances. Successful rollback attacks have serious consequences on applications such as financial services. In this paper, we propose a new approach for rollback protection on SGX. The intuition behind our approach is simple. A single platform cannot efficiently prevent rollback, but in many practical scenarios, multiple processors can be enrolled to assist each other. We design and implement a rollback protection system called ROTE that realizes integrity protection as a distributed system. We construct a model that captures adversarial ability to schedule enclave execution and show that our solution achieves a strong security property: the only way to violate integrity is to reset all participating platforms to their initial state. We implement ROTE and demonstrate that distributed rollback protection can provide significantly better performance than previously known solutions based on local non-volatile memory.",
            "keywords": [
                "Trusted Execution",
                "Rollback Protection",
                "Intel SGX",
                "Distributed Systems",
                "Integrity Violation"
            ]
        },
        "url": "URL#3511186",
        "sema_paperId": "633e28ae5adebf7f6ec5ad40755388a3937c0be6"
    },
    {
        "@score": "1",
        "@id": "3511187",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "168/7919",
                        "text": "David McCann"
                    },
                    {
                        "@pid": "48/4127",
                        "text": "Elisabeth Oswald"
                    },
                    {
                        "@pid": "56/9958",
                        "text": "Carolyn Whitnall"
                    }
                ]
            },
            "title": "Towards Practical Tools for Side Channel Aware Software Engineering: &apos;Grey Box&apos; Modelling for Instruction Leakages.",
            "venue": "USENIX Security Symposium",
            "pages": "199-216",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/McCannOW17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/mccann",
            "url": "https://dblp.org/rec/conf/uss/McCannOW17",
            "abstract": "Power (along with EM, cache and timing) leaks are of considerable concern for developers who have to deal with cryptographic components as part of their over-all software implementation, in particular in the context of embedded devices. Whilst there exist some compiler tools to detect timing leaks, similar progress towards pinpointing power and EM leaks has been hampered by limits on the amount of information available about the physical components from which such leaks originate. We suggest a novel modelling technique capable of producing high-quality instruction-level power (and/or EM) models without requiring a detailed hardware description of a processor nor information about the used process technology (access to both of which is typically restricted). We show that our methodology is e\ufb00ective at capturing di\ufb00erential data-dependent e\ufb00ects as neighbouring instructions in a sequence vary. We also explore register e\ufb00ects, and verify our models across several measurement boards to comment on board e\ufb00ects and portability. We con\ufb01rm its versatility by demonstrating the basic technique on two processors (the ARM Cortex-M0 and M4), and use the M0 models to develop ELMO, the \ufb01rst leakage simulator for the ARM Cortex M0.",
            "keywords": [
                "Side Channel Attacks",
                "Power Leakage",
                "EM Leakage",
                "Instruction-Level Modelling",
                "Leakage Simulation"
            ]
        },
        "url": "URL#3511187",
        "sema_paperId": "fa49d089a20f99de8985924dffe404942513ef51"
    },
    {
        "@score": "1",
        "@id": "3511188",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "130/0483",
                        "text": "Susan E. McGregor"
                    },
                    {
                        "@pid": "196/3030",
                        "text": "Elizabeth Anne Watkins"
                    },
                    {
                        "@pid": "119/5033",
                        "text": "Mahdi Nasrullah Al-Ameen"
                    },
                    {
                        "@pid": "87/1275",
                        "text": "Kelly Caine"
                    },
                    {
                        "@pid": "23/2758",
                        "text": "Franziska Roesner"
                    }
                ]
            },
            "title": "When the Weakest Link is Strong: Secure Collaboration in the Case of the Panama Papers.",
            "venue": "USENIX Security Symposium",
            "pages": "505-522",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/McGregorWACR17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/mcgregor",
            "url": "https://dblp.org/rec/conf/uss/McGregorWACR17",
            "abstract": "Success stories in usable security are rare. In this paper, however, we examine one notable security success: the year-long collaborative investigation of more than two terabytes of leaked documents during the \"Panama Papers\" project. During this effort, a large, diverse group of globally-distributed journalists met and maintained critical security goals-including protecting the source of the leaked documents and preserving the secrecy of the project until the desired launch date-all while hundreds of journalists collaborated remotely on a near-daily basis. \n \nThrough survey data from 118 participating journalists, as well as in-depth, semi-structured interviews with the designers and implementers of the systems underpinning the collaboration, we investigate the factors that supported this effort. We find that the tools developed for the project were both highly useful and highly usable, motivating journalists to use the secure communication platforms provided instead of seeking workarounds. We also found that, despite having little prior computer security experience, journalists adopted--and even appreciated--the strict security requirements imposed by the project leads. We also find that a shared sense of community and responsibility contributed to participants' motivation to meet and maintain security requirements. From these and other findings, we distill lessons for socio-technical systems with strong security requirements and identify opportunities for future work.",
            "keywords": [
                "Usable Security",
                "Collaborative Investigation",
                "Secure Communication",
                "Journalistic Ethics",
                "Panama Papers"
            ]
        },
        "url": "URL#3511188",
        "sema_paperId": "2c02c0162722b8d93d41ec03bc39d0dc99fc7e26"
    },
    {
        "@score": "1",
        "@id": "3511189",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "161/0164",
                        "text": "Aastha Mehta"
                    },
                    {
                        "@pid": "13/10811",
                        "text": "Eslam Elnikety"
                    },
                    {
                        "@pid": "205/2133",
                        "text": "Katura Harvey"
                    },
                    {
                        "@pid": "45/6786-1",
                        "text": "Deepak Garg 0001"
                    },
                    {
                        "@pid": "d/PDruschel",
                        "text": "Peter Druschel"
                    }
                ]
            },
            "title": "Qapla: Policy compliance for database-backed systems.",
            "venue": "USENIX Security Symposium",
            "pages": "1463-1479",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/MehtaEH0D17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/mehta",
            "url": "https://dblp.org/rec/conf/uss/MehtaEH0D17",
            "abstract": "Many database-backed systems store confidential data that is accessed on behalf of users with different privileges. Policies governing access are often fine-grained, being specific to users, time, accessed columns and rows, values in the database (e.g., user roles), and operators used in queries (e.g., aggregators, group by, and join). Today, applications are often relied upon to issue policy compliant queries or filter the results of non-compliant queries, which is vulnerable to application errors. Qapla provides an alternate approach to policy enforcement that neither depends on application correctness, nor on specialized database support. In Qapla, policies are specific to rows and columns and may additionally refer to the querier\u2019s identity and time, are specified in SQL, and stored in the database itself. We prototype Qapla in a database adapter, and evaluate it by enforcing applicable policies in the HotCRP conference management system and a system for managing academic job applications.",
            "keywords": [
                "Database Policy Compliance",
                "Access Control Policies",
                "Confidential Data Management",
                "Query Enforcement",
                "Application Vulnerabilities"
            ]
        },
        "url": "URL#3511189",
        "sema_paperId": "e36ae4d80202c0ca02dec9fcda89715ffb6a42ad"
    },
    {
        "@score": "1",
        "@id": "3511190",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "09/6585-2",
                        "text": "Jiang Ming 0002"
                    },
                    {
                        "@pid": "158/2764",
                        "text": "Dongpeng Xu 0001"
                    },
                    {
                        "@pid": "88/9308",
                        "text": "Yufei Jiang"
                    },
                    {
                        "@pid": "54/2696",
                        "text": "Dinghao Wu"
                    }
                ]
            },
            "title": "BinSim: Trace-based Semantic Binary Diffing via System Call Sliced Segment Equivalence Checking.",
            "venue": "USENIX Security Symposium",
            "pages": "253-270",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/MingXJW17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/ming",
            "url": "https://dblp.org/rec/conf/uss/MingXJW17",
            "abstract": "Detecting differences between two binary executables (binary diffing), first derived from patch analysis, have been widely employed in various software security analysis tasks, such as software plagiarism detection and malware lineage inference. Especially when analyzing malware variants, pervasive code obfuscation techniques have driven recent work towards determining semantic similarity in spite of ostensible difference in syntax. Existing ways rely on either comparing runtime behaviors or modeling code snippet semantics with symbolic execution. However, neither approach delivers the expected precision. In this paper, we propose system call sliced segment equivalence checking, a hybrid method to identify fine-grained semantic similarities or differences between two execution traces. We perform enhanced dynamic slicing and symbolic execution to compare the logic of instructions that impact on the observable behaviors. Our approach improves existing semantics-based binary diffing by 1) inferring whether two executable binaries\u2019 behaviors are conditionally equivalent; 2) detecting the similarities or differences, whose effects spread across multiple basic blocks. We have developed a prototype, called BinSim, and performed empirical evaluations against sophisticated obfuscation combinations and more than 1,000 recent malware samples, including now-infamous crypto ransomware. Our experimental results show that BinSim can successfully identify finegrained relations between obfuscated binaries, and outperform existing binary diffing tools in terms of better resilience and accuracy.",
            "keywords": [
                "Binary Diffing",
                "Malware Analysis",
                "Semantic Similarity",
                "Obfuscation Techniques",
                "Execution Trace Comparison"
            ]
        },
        "url": "URL#3511190",
        "sema_paperId": "dd32037186342c16001e95715bd28eefc09cf288"
    },
    {
        "@score": "1",
        "@id": "3511192",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "203/1926",
                        "text": "Peter Ney"
                    },
                    {
                        "@pid": "64/4300",
                        "text": "Karl Koscher"
                    },
                    {
                        "@pid": "205/2081",
                        "text": "Lee Organick"
                    },
                    {
                        "@pid": "95/5263",
                        "text": "Luis Ceze"
                    },
                    {
                        "@pid": "k/TadayoshiKohno",
                        "text": "Tadayoshi Kohno"
                    }
                ]
            },
            "title": "Computer Security, Privacy, and DNA Sequencing: Compromising Computers with Synthesized DNA, Privacy Leaks, and More.",
            "venue": "USENIX Security Symposium",
            "pages": "765-779",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/NeyKOCK17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/ney",
            "url": "https://dblp.org/rec/conf/uss/NeyKOCK17",
            "abstract": "The rapid improvement in DNA sequencing has sparked a big data revolution in genomic sciences, which has in turn led to a proliferation of bioinformatics tools. To date, these tools have encountered little adversarial pressure. This paper evaluates the robustness of such tools if (or when) adversarial attacks manifest. We demonstrate, for the \ufb01rst time, the synthesis of DNA which\u2014when sequenced and processed\u2014gives an attacker arbitrary remote code execution. To study the feasibility of creating and synthesizing a DNA-based exploit, we performed our attack on a modi\ufb01ed downstream sequencing utility with a deliberately introduced vulnerability. After sequencing, we observed information leakage in our data due to sample bleeding. While this phenomena is known to the sequencing community, we provide the \ufb01rst discussion of how this leakage channel could be used adversarially to inject data or reveal sensitive information. We then evaluate the general security hygiene of common DNA processing programs, and unfortunately, \ufb01nd concrete evidence of poor security practices used throughout the \ufb01eld. Informed by our experiments and results, we develop a broad framework and guidelines to safeguard security and privacy in DNA synthesis, sequencing, and processing.",
            "keywords": [
                "DNA Sequencing Security",
                "Bioinformatics Tools",
                "Adversarial Attacks",
                "Information Leakage",
                "DNA Synthesis Exploits"
            ]
        },
        "url": "URL#3511192",
        "sema_paperId": "0e938ba10ed50dc2e18b90e6401fdb1920be97ec"
    },
    {
        "@score": "1",
        "@id": "3511193",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "203/4231",
                        "text": "Kirill Nikitin 0001"
                    },
                    {
                        "@pid": "176/5301",
                        "text": "Eleftherios Kokoris-Kogias"
                    },
                    {
                        "@pid": "14/11314",
                        "text": "Philipp Jovanovic"
                    },
                    {
                        "@pid": "176/5570",
                        "text": "Nicolas Gailly"
                    },
                    {
                        "@pid": "122/9716",
                        "text": "Linus Gasser"
                    },
                    {
                        "@pid": "176/5341",
                        "text": "Ismail Khoffi"
                    },
                    {
                        "@pid": "27/5136",
                        "text": "Justin Cappos"
                    },
                    {
                        "@pid": "f/BryanFord",
                        "text": "Bryan Ford"
                    }
                ]
            },
            "title": "CHAINIAC: Proactive Software-Update Transparency via Collectively Signed Skipchains and Verified Builds.",
            "venue": "USENIX Security Symposium",
            "pages": "1271-1287",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/NikitinKJGGKCF17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/nikitin",
            "url": "https://dblp.org/rec/conf/uss/NikitinKJGGKCF17",
            "abstract": "Software-update mechanisms are critical to the security of modern systems, but their typically centralized design presents a lucrative and frequently attacked target. In this work, we propose CHAINIAC, a decentralized software-update framework that eliminates single points of failure, enforces transparency, and provides efficient verifiability of integrity and authenticity for software-release processes. Independent witness servers collectively verify conformance of software updates to release policies, build verifiers validate the source-to-binary correspondence, and a tamper-proof release log stores collectively signed updates, thus ensuring that no release is accepted by clients before being widely disclosed and validated. The release log embodies a skipchain, a novel data structure, enabling arbitrarily out-of-date clients to efficiently validate updates and signing keys. Evaluation of our CHAINIAC prototype on reproducible Debian packages shows that the automated update process takes the average of 5 minutes per release for individual packages, and only 20 seconds for the aggregate timeline. We further evaluate the framework using real-world data from the PyPI package repository and show that it offers clients security comparable to verifying every single update themselves while consuming only one-fifth of the bandwidth and having a minimal computational overhead.",
            "keywords": [
                "Decentralized Software Updates",
                "Software Integrity",
                "Release Transparency",
                "Skipchains",
                "Verified Builds"
            ]
        },
        "url": "URL#3511193",
        "sema_paperId": "e901a418704ab9376eaccaca821aa6bea146043c"
    },
    {
        "@score": "1",
        "@id": "3511194",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "47/8535",
                        "text": "Zhenyu Ning"
                    },
                    {
                        "@pid": "20/11242",
                        "text": "Fengwei Zhang"
                    }
                ]
            },
            "title": "Ninja: Towards Transparent Tracing and Debugging on ARM.",
            "venue": "USENIX Security Symposium",
            "pages": "33-49",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/NingZ17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/ning",
            "url": "https://dblp.org/rec/conf/uss/NingZ17",
            "abstract": "Existing malware analysis platforms leave detectable \ufb01n-gerprints like uncommon string properties in QEMU, signatures in Android Java virtual machine, and artifacts in Linux kernel pro\ufb01les. Since these \ufb01ngerprints provide the malware a chance to split its behavior depending on whether the analysis system is present or not, existing analysis systems are not suf\ufb01cient to analyze the sophisticated malware. In this paper, we pro-pose N INJA , a transparent malware analysis framework on ARM platform with low artifacts. N INJA leverages a hardware-assisted isolated execution environment Trust-Zone to transparently trace and debug a target application with the help of Performance Monitor Unit and Embedded Trace Macrocell. N INJA does not modify system software and is OS-agnostic on ARM platform. We implement a prototype of N INJA (i.e., tracing and debugging subsystems), and the experiment results show that N INJA is ef\ufb01cient and transparent for malware analysis.",
            "keywords": [
                "Malware Analysis",
                "ARM Architecture",
                "Transparent Tracing",
                "Debugging Framework",
                "TrustZone"
            ]
        },
        "url": "URL#3511194",
        "sema_paperId": "856292c58844daeb64c1ae28814c6719d2625d67"
    },
    {
        "@score": "1",
        "@id": "3511196",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "37/9614",
                        "text": "Mark O&apos;Neill"
                    },
                    {
                        "@pid": "169/9742",
                        "text": "Scott Heidbrink"
                    },
                    {
                        "@pid": "124/2527",
                        "text": "Scott Ruoti"
                    },
                    {
                        "@pid": "188/6053",
                        "text": "Jordan Whitehead"
                    },
                    {
                        "@pid": "188/6460",
                        "text": "Dan Bunker"
                    },
                    {
                        "@pid": "205/2032",
                        "text": "Luke Dickinson"
                    },
                    {
                        "@pid": "169/9870",
                        "text": "Travis Hendershot"
                    },
                    {
                        "@pid": "12/4508",
                        "text": "Joshua Reynolds"
                    },
                    {
                        "@pid": "s/KentESeamons",
                        "text": "Kent E. Seamons"
                    },
                    {
                        "@pid": "71/2423",
                        "text": "Daniel Zappala"
                    }
                ]
            },
            "title": "TrustBase: An Architecture to Repair and Strengthen Certificate-based Authentication.",
            "venue": "USENIX Security Symposium",
            "pages": "609-624",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/ONeillHRWBDHRSZ17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/oneill",
            "url": "https://dblp.org/rec/conf/uss/ONeillHRWBDHRSZ17",
            "abstract": "The current state of certificate-based authentication is messy, with broken authentication in applications and proxies, along with serious flaws in the CA system. To solve these problems, we design TrustBase, an architecture that provides certificate-based authentication as an operating system service, with system administrator control over authentication policy. TrustBase transparently enforces best practices for certificate validation on all applications, while also providing a variety of authentication services to strengthen the CA system. We describe a research prototype of TrustBase for Linux, which uses a loadable kernel module to intercept traffic in the socket layer, then consults a userspace policy engine to evaluate certificate validity using a variety of plugins. We evaluate the security of TrustBase, including a threat analysis, application coverage, and hardening of the Linux prototype. We also describe prototypes of TrustBase for Android and Windows, illustrating the generality of our approach. We show that TrustBase has negligible overhead and universal compatibility with applications. We demonstrate its utility by describing eight authentication services that extend CA hardening to all applications.",
            "pdf_url": "",
            "keywords": [
                "Certificate-based Authentication",
                "TrustBase",
                "Certificate Validation",
                "Certification Authority (CA) System",
                "Authentication Policy"
            ]
        },
        "url": "URL#3511196"
    },
    {
        "@score": "1",
        "@id": "3511197",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "64/7666",
                        "text": "Jianfeng Pan"
                    },
                    {
                        "@pid": "173/3572",
                        "text": "Guanglu Yan"
                    },
                    {
                        "@pid": "205/2042",
                        "text": "Xiaocao Fan"
                    }
                ]
            },
            "title": "Digtool: A Virtualization-Based Framework for Detecting Kernel Vulnerabilities.",
            "venue": "USENIX Security Symposium",
            "pages": "149-165",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/PanYF17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/pan",
            "url": "https://dblp.org/rec/conf/uss/PanYF17",
            "abstract": "Discovering vulnerabilities in operating system (OS) kernels and patching them is crucial for OS security. However, there is a lack of effective kernel vulnerability detection tools, especially for closed-source OSes such as Microsoft Windows. In this paper, we present Digtool, an effective, binary-code-only, kernel vulnerability detection framework. Built atop a virtualization monitor we designed, Digtool successfully captures various dynamic behaviors of kernel execution, such as kernel object allocation, kernel memory access, thread scheduling, and function invoking. With these behaviors, Digtool has identified 45 zero-day vulnerabilities such as outof-bounds access, use-after-free, and time-of-check-totime-of-use among both kernel code and device drivers of recent versions of Microsoft Windows, including Windows 7 and Windows 10.",
            "keywords": [
                "Kernel Vulnerability Detection",
                "Virtualization Framework",
                "Operating System Security",
                "Zero-Day Vulnerabilities",
                "Dynamic Kernel Behavior"
            ]
        },
        "url": "URL#3511197",
        "sema_paperId": "5903977e9b2f54f5c709e3148bd684cbd87c303f"
    },
    {
        "@score": "1",
        "@id": "3511198",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "61/1749",
                        "text": "Paul Pearce"
                    },
                    {
                        "@pid": "05/7185",
                        "text": "Ben Jones"
                    },
                    {
                        "@pid": "53/10825",
                        "text": "Frank Li 0001"
                    },
                    {
                        "@pid": "24/973",
                        "text": "Roya Ensafi"
                    },
                    {
                        "@pid": "87/840",
                        "text": "Nick Feamster"
                    },
                    {
                        "@pid": "53/2937",
                        "text": "Nicholas Weaver"
                    },
                    {
                        "@pid": "p/VernPaxson",
                        "text": "Vern Paxson"
                    }
                ]
            },
            "title": "Global Measurement of DNS Manipulation.",
            "venue": "USENIX Security Symposium",
            "pages": "307-323",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/PearceJLEFWP17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/pearce",
            "url": "https://dblp.org/rec/conf/uss/PearceJLEFWP17",
            "abstract": "Despite the pervasive nature of Internet censorship and the continuous evolution of how and where censorship is applied, measurements of censorship remain comparatively sparse. Understanding the scope, scale, and evolution of Internet censorship requires global measurements, performed at regular intervals. Unfortunately, the state of the art relies on techniques that, by and large, require users to directly participate in gathering these measurements, drastically limiting their coverage and inhibiting regular data collection. To facilitate large-scale measurements that can fill this gap in understanding, we develop Iris, a scalable, accurate, and ethical method to measure global manipulation of DNS resolutions. Iris reveals widespread DNS manipulation of many domain names; our findings both confirm anecdotal or limited results from previous work and reveal new patterns in DNS manipulation.",
            "keywords": [
                "Internet Censorship",
                "DNS Manipulation",
                "Global Measurement",
                "Censorship Detection",
                "Ethical Data Collection"
            ]
        },
        "url": "URL#3511198",
        "sema_paperId": "6c5c190c57fd8264fea5e0b731c0eae5b5fc1676"
    },
    {
        "@score": "1",
        "@id": "3511199",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "119/7710",
                        "text": "Giuseppe Petracca"
                    },
                    {
                        "@pid": "174/7896",
                        "text": "Ahmad Atamli-Reineh"
                    },
                    {
                        "@pid": "139/7955",
                        "text": "Yuqiong Sun"
                    },
                    {
                        "@pid": "28/3855",
                        "text": "Jens Grossklags"
                    },
                    {
                        "@pid": "45/576",
                        "text": "Trent Jaeger"
                    }
                ]
            },
            "title": "AWare: Preventing Abuse of Privacy-Sensitive Sensors via Operation Bindings.",
            "venue": "USENIX Security Symposium",
            "pages": "379-396",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/PetraccaASGJ17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/petracca",
            "url": "https://dblp.org/rec/conf/uss/PetraccaASGJ17",
            "abstract": "System designers have long struggled with the challenge of determining how to control when untrusted applications may perform operations using privacy-sensitive sensors securely and effectively. Current systems request that users authorize such operations once (i.e., on install or first use), but malicious applications may abuse such authorizations to collect data stealthily using such sensors. Proposed research methods enable systems to infer the operations associated with user input events, but malicious applications may still trick users into allowing unexpected, stealthy operations. To prevent users from being tricked, we propose to bind applications\u2019 operation requests to the associated user input events and how they were obtained explicitly, enabling users to authorize operations on privacy-sensitive sensors unambiguously and reuse such authorizations. To demonstrate this approach, we implement the AWare authorization framework for Android, extending the Android Middleware to control access to privacy-sensitive sensors. We evaluate the effectiveness of AWare in: (1) a laboratory-based user study, finding that at most 7% of the users were tricked by examples of four types of attacks when using AWare, instead of 85% on average for prior approaches; (2) a field study, showing that the user authorization effort increases by only 2.28 decisions on average per application; (3) a compatibility study with 1,000 of the most-downloaded Android applications, demonstrating that such applications can operate effectively under AWare.",
            "keywords": [
                "Privacy-Sensitive Sensors",
                "User Authorization",
                "Malicious Applications",
                "Operation Binding",
                "AWare Framework"
            ]
        },
        "url": "URL#3511199",
        "sema_paperId": "f8844f2065931290520c9044d0ee2c9ef6381079"
    },
    {
        "@score": "1",
        "@id": "3511200",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "99/3569",
                        "text": "Anh Pham"
                    },
                    {
                        "@pid": "82/5414",
                        "text": "Italo Dacosta"
                    },
                    {
                        "@pid": "183/8472",
                        "text": "Guillaume Endignoux"
                    },
                    {
                        "@pid": "33/5866",
                        "text": "Juan Ram\u00f3n Troncoso-Pastoriza"
                    },
                    {
                        "@pid": "10/3024",
                        "text": "K\u00e9vin Huguenin"
                    },
                    {
                        "@pid": "h/JPHubaux",
                        "text": "Jean-Pierre Hubaux"
                    }
                ]
            },
            "title": "ORide: A Privacy-Preserving yet Accountable Ride-Hailing Service.",
            "venue": "USENIX Security Symposium",
            "pages": "1235-1252",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/PhamDETHH17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/pham",
            "url": "https://dblp.org/rec/conf/uss/PhamDETHH17",
            "abstract": "In recent years, ride-hailing services (RHSs) have be- come increasingly popular, serving millions of users per day. Such systems, however, raise significant privacy concerns, because service providers are able to track the precise mobility patterns of all riders and drivers. In this paper, we propose ORide (Oblivious Ride), a privacy- preserving RHS based on somewhat-homomorphic en- cryption with optimizations such as ciphertext packing and transformed processing. With ORide, a service provider can match riders and drivers without learning their identities or location information. ORide offers rid- ers with fairly large anonymity sets (e.g., several thou- sands), even in sparsely populated areas. In addition, ORide supports key RHS features such as easy payment, reputation scores, accountability, and retrieval of lost items. Using real data-sets that consist of millions of rides, we show that the computational and network over- head introduced by ORide is acceptable. For example, ORide adds only several milliseconds to ride-hailing op- erations, and the extra driving distance for a driver is less than 0.5 km in more than 75% of the cases evaluated. In short, we show that a RHS can offer strong privacy guar- antees to both riders and drivers while maintaining the convenience of its services.",
            "keywords": [
                "Privacy-Preserving Systems",
                "Ride-Hailing Services",
                "Homomorphic Encryption",
                "User Anonymity",
                "Accountability in Transportation"
            ]
        },
        "url": "URL#3511200",
        "sema_paperId": "e6a798c87cdee8036aeb6dab4cfac4272f25515f"
    },
    {
        "@score": "1",
        "@id": "3511201",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "178/2737",
                        "text": "Ania M. Piotrowska"
                    },
                    {
                        "@pid": "168/8164",
                        "text": "Jamie Hayes"
                    },
                    {
                        "@pid": "120/5844",
                        "text": "Tariq Elahi"
                    },
                    {
                        "@pid": "02/7901-1",
                        "text": "Sebastian Meiser 0001"
                    },
                    {
                        "@pid": "11/1148",
                        "text": "George Danezis"
                    }
                ]
            },
            "title": "The Loopix Anonymity System.",
            "venue": "USENIX Security Symposium",
            "pages": "1199-1216",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/PiotrowskaHEMD17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/piotrowska",
            "url": "https://dblp.org/rec/conf/uss/PiotrowskaHEMD17",
            "abstract": "We present Loopix, a low-latency anonymous communication system that provides bi-directional 'third-party' sender and receiver anonymity and unobservability. Loopix leverages cover traffic and Poisson mixing--brief independent message delays--to provide anonymity and to achieve traffic analysis resistance against, including but not limited to, a global network adversary. Mixes and clients self-monitor and protect against active attacks via self-injected loops of traffic. The traffic loops also serve as cover traffic to provide stronger anonymity and a measure of sender and receiver unobservability. Loopix is instantiated as a network of Poisson mix nodes in a stratified topology with a low number of links, which serve to further concentrate cover traffic. Service providers mediate access in and out of the network to facilitate accounting and off-line message reception. \n \nWe provide a theoretical analysis of the Poisson mixing strategy as well as an empirical evaluation of the anonymity provided by the protocol and a functional implementation that we analyze in terms of scalability by running it on AWS EC2. We show that mix nodes in Loopix can handle upwards of 300 messages per second, at a small delay overhead of less than 1.5ms on top of the delays introduced into messages to provide security. Overall message latency is on the order of seconds - which is relatively low for a mix-system. Furthermore, many mix nodes can be securely added to the stratified topology to scale throughput without sacrificing anonymity.",
            "keywords": [
                "Anonymous Communication",
                "Traffic Analysis Resistance",
                "Poisson Mixing",
                "Sender and Receiver Anonymity",
                "Low-Latency Messaging"
            ]
        },
        "url": "URL#3511201",
        "sema_paperId": "f23b7ecdd03a48d4ef41fc58af008a164010f692"
    },
    {
        "@score": "1",
        "@id": "3511202",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "183/9956",
                        "text": "Angelisa C. Plane"
                    },
                    {
                        "@pid": "141/9244",
                        "text": "Elissa M. Redmiles"
                    },
                    {
                        "@pid": "20/7983",
                        "text": "Michelle L. Mazurek"
                    },
                    {
                        "@pid": "45/1247",
                        "text": "Michael Carl Tschantz"
                    }
                ]
            },
            "title": "Exploring User Perceptions of Discrimination in Online Targeted Advertising.",
            "venue": "USENIX Security Symposium",
            "pages": "935-951",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/PlaneRMT17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/plane",
            "url": "https://dblp.org/rec/conf/uss/PlaneRMT17",
            "abstract": "Targeted online advertising now accounts for the largest share of the advertising market, beating out both TV and print ads. While targeted advertising can improve users\u2019 online shopping experiences, it can also have negative e \ufb00 ects. A plethora of recent work has found evidence that in some cases, ads may be discriminatory, leading certain groups of users to see better o \ufb00 ers (e.g., job ads) based on personal characteristics such as gender. To develop policies around advertising and guide advertisers in making ethical decisions, one thing we must better understand is what concerns users and why. In an e \ufb00 ort to answer this question, we conducted a pilot study and a multi-step main survey (n = 2,086 in total) presenting users with different discriminatory advertising scenarios. We \ufb01nd that overall, 44% of respondents were moderately or very concerned by the scenarios we presented. Respondents found the scenarios signi\ufb01cantly more problematic when discrimination took place as a result of explicit demographic targeting rather than in response to online behavior. However, our respondents\u2019 opinions did not vary based on whether a human or an algorithm was responsible for the discrimination. These \ufb01ndings suggest that future policy documents should explicitly address discrimination in targeted advertising, no matter its origin, as a signi\ufb01cant user concern, and that corporate responses that blame the algorithmic nature of the ad ecosystem may not be helpful for addressing public concerns.",
            "keywords": [
                "Targeted Advertising",
                "User Perceptions",
                "Discrimination",
                "Demographic Targeting",
                "Algorithmic Responsibility"
            ]
        },
        "url": "URL#3511202",
        "sema_paperId": "0fefca8f0bc0f81f61740ad62737bcea5904ccf2"
    },
    {
        "@score": "1",
        "@id": "3511204",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "117/3983",
                        "text": "Bradley Reaves"
                    },
                    {
                        "@pid": "183/1275",
                        "text": "Logan Blue"
                    },
                    {
                        "@pid": "205/2013",
                        "text": "Hadi Abdullah"
                    },
                    {
                        "@pid": "68/5101",
                        "text": "Luis Vargas"
                    },
                    {
                        "@pid": "14/3295",
                        "text": "Patrick Traynor"
                    },
                    {
                        "@pid": "s/ThomasShrimpton",
                        "text": "Thomas Shrimpton"
                    }
                ]
            },
            "title": "AuthentiCall: Efficient Identity and Content Authentication for Phone Calls.",
            "venue": "USENIX Security Symposium",
            "pages": "575-592",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/ReavesBAVTS17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/reaves",
            "url": "https://dblp.org/rec/conf/uss/ReavesBAVTS17",
            "abstract": "Phones are used to confirm some of our most sensitive transactions. From coordination between energy providers in the power grid to corroboration of high-value transfers with a financial institution, we rely on telephony to serve as a trustworthy communications path. However, such trust is not well placed given the widespread understanding of telephony's inability to provide end-to-end authentication between callers. In this paper, we address this problem through the AuthentiCall system. AuthentiCall not only cryptographically authenticates both parties on the call, but also provides strong guarantees of the integrity of conversations made over traditional phone networks. We achieve these ends through the use of formally verified protocols that bind low-bitrate data channels to heterogeneous audio channels. Unlike previous efforts, we demonstrate that AuthentiCall can be used to provide strong authentication before calls are answered, allowing users to ignore calls claiming a particular Caller ID that are unable or unwilling to provide proof of that assertion. Moreover, we detect 99% of tampered call audio with negligible false positives and only a worst-case 1.4 second call establishment overhead. In so doing, we argue that strong and efficient end-to-end authentication for phone networks is approaching a practical reality.",
            "keywords": [
                "Telephony Authentication",
                "End-to-End Authentication",
                "Call Integrity",
                "Caller ID Verification",
                "Tampered Call Detection"
            ]
        },
        "url": "URL#3511204",
        "sema_paperId": "d6935832a0a7c4e1bf2376e9b61d595f3914b59f"
    },
    {
        "@score": "1",
        "@id": "3511205",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "155/6800",
                        "text": "Nilo Redini"
                    },
                    {
                        "@pid": "133/8188",
                        "text": "Aravind Machiry"
                    },
                    {
                        "@pid": "90/3182-2",
                        "text": "Dipanjan Das 0002"
                    },
                    {
                        "@pid": "11/10929",
                        "text": "Yanick Fratantonio"
                    },
                    {
                        "@pid": "99/3883",
                        "text": "Antonio Bianchi"
                    },
                    {
                        "@pid": "121/1138",
                        "text": "Eric Gustafson"
                    },
                    {
                        "@pid": "119/7712",
                        "text": "Yan Shoshitaishvili"
                    },
                    {
                        "@pid": "k/ChristopherKruegel",
                        "text": "Christopher Kruegel"
                    },
                    {
                        "@pid": "v/GiovanniVigna",
                        "text": "Giovanni Vigna"
                    }
                ]
            },
            "title": "BootStomp: On the Security of Bootloaders in Mobile Devices.",
            "venue": "USENIX Security Symposium",
            "pages": "781-798",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/RediniMDFBGSKV17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/redini",
            "url": "https://dblp.org/rec/conf/uss/RediniMDFBGSKV17",
            "abstract": ",",
            "keywords": [
                "Mobile Device Security",
                "Bootloader Vulnerabilities",
                "Firmware Integrity",
                "Secure Boot",
                "Malware Prevention"
            ]
        },
        "url": "URL#3511205",
        "sema_paperId": "ac65baead69131e279b5ceebc0ed9a28568acaf5"
    },
    {
        "@score": "1",
        "@id": "3511206",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "165/2039",
                        "text": "Iskander S\u00e1nchez-Rola"
                    },
                    {
                        "@pid": "49/4967",
                        "text": "Igor Santos"
                    },
                    {
                        "@pid": "56/490",
                        "text": "Davide Balzarotti"
                    }
                ]
            },
            "title": "Extension Breakdown: Security Analysis of Browsers Extension Resources Control Policies.",
            "venue": "USENIX Security Symposium",
            "pages": "679-694",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/Sanchez-RolaSB17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/sanchez-rola",
            "url": "https://dblp.org/rec/conf/uss/Sanchez-RolaSB17",
            "abstract": "All major web browsers support browser extensions to add new features and extend their functionalities. Nevertheless, browser extensions have been the target of several attacks due to their tight relation with the browser environment. As a consequence, extensions have been abused in the past for malicious tasks such as private information gathering, browsing history retrieval, or passwords theft \u2014 leading to a number of severe targeted attacks. Even though no protection techniques existed in the past to secure extensions, all browsers now implement defensive countermeasures that, in theory, protect extensions and their resources from third party access. In this paper, we present two attacks that bypass these control techniques in every major browser family, enabling enumeration attacks against the list of installed extensions. In particular, we present a timing side-channel attack against the access control settings and an attack that takes advantage of poor programming practice, affecting a large number of Safari extensions. Due to the harmful nature of our findings, we also discuss possible countermeasures against our own attacks and reported our findings and countermeasures to the different actors involved. We believe that our study can help secure current implementations and help developers to avoid similar attacks in the future.",
            "keywords": [
                "Browser Extensions",
                "Security Analysis",
                "Access Control Policies",
                "Enumeration Attacks",
                "Timing Side-Channel Attack"
            ]
        },
        "url": "URL#3511206",
        "sema_paperId": "f516d982f54188889cbd7a9128ab60c4a7cc4bf5"
    },
    {
        "@score": "1",
        "@id": "3511208",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "205/2072",
                        "text": "Sergej Schumilo"
                    },
                    {
                        "@pid": "160/7844",
                        "text": "Cornelius Aschermann"
                    },
                    {
                        "@pid": "150/5154",
                        "text": "Robert Gawlik"
                    },
                    {
                        "@pid": "43/11495",
                        "text": "Sebastian Schinzel"
                    },
                    {
                        "@pid": "h/ThorstenHolz",
                        "text": "Thorsten Holz"
                    }
                ]
            },
            "title": "kAFL: Hardware-Assisted Feedback Fuzzing for OS Kernels.",
            "venue": "USENIX Security Symposium",
            "pages": "167-182",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/SchumiloAGSH17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/schumilo",
            "url": "https://dblp.org/rec/conf/uss/SchumiloAGSH17",
            "abstract": "Many kinds of memory safety vulnerabilities have been endangering software systems for decades. Amongst other approaches, fuzzing is a promising technique to unveil various software faults. Recently, feedback-guided fuzzing demonstrated its power, producing a steady stream of security-critical software bugs. Most fuzzing efforts\u2014especially feedback fuzzing\u2014are limited to user space components of an operating system (OS), although bugs in kernel components are more severe, because they allow an attacker to gain access to a system with full privileges. Unfortunately, kernel components are difficult to fuzz as feedback mechanisms (i.e., guided code coverage) cannot be easily applied. Additionally, non-determinism due to interrupts, kernel threads, statefulness, and similar mechanisms poses problems. Furthermore, if a process fuzzes its own kernel, a kernel crash highly impacts the performance of the fuzzer as the OS needs to reboot. In this paper, we approach the problem of coverageguided kernel fuzzing in an OS-independent and hardware-assisted way: We utilize a hypervisor and Intel\u2019s Processor Trace (PT) technology. This allows us to remain independent of the target OS as we just require a small user space component that interacts with the targeted OS. As a result, our approach introduces almost no performance overhead, even in cases where the OS crashes, and performs up to 17,000 executions per second on an off-the-shelf laptop. We developed a framework called kernel-AFL (kAFL) to assess the security of Linux, macOS, and Windows kernel components. Among many crashes, we uncovered several flaws in the ext4 driver for Linux, the HFS and APFS file system of macOS, and the NTFS driver of Windows.",
            "keywords": [
                "Kernel Fuzzing",
                "Memory Safety",
                "Feedback-guided Fuzzing",
                "OS Independence",
                "Hardware-assisted Testing"
            ]
        },
        "url": "URL#3511208",
        "sema_paperId": "30023acba3ac198a7d260228dc51fda8414b8860"
    },
    {
        "@score": "1",
        "@id": "3511209",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "180/8190",
                        "text": "Roei Schuster"
                    },
                    {
                        "@pid": "46/6275",
                        "text": "Vitaly Shmatikov"
                    },
                    {
                        "@pid": "t/EranTromer",
                        "text": "Eran Tromer"
                    }
                ]
            },
            "title": "Beauty and the Burst: Remote Identification of Encrypted Video Streams.",
            "venue": "USENIX Security Symposium",
            "pages": "1357-1374",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/SchusterST17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/schuster",
            "url": "https://dblp.org/rec/conf/uss/SchusterST17",
            "abstract": "The MPEG-DASH streaming video standard contains an information leak: even if the stream is encrypted, the segmentation prescribed by the standard causes content-dependent packet bursts. We show that many video streams are uniquely characterized by their burst patterns, and classifiers based on convolutional neural networks can accurately identify these patterns given very coarse network measurements. We demonstrate that this attack can be performed even by a Web attacker who does not directly observe the stream, e.g., a JavaScript ad confined in a Web browser on a nearby machine.",
            "keywords": [
                "Encrypted Video Streaming",
                "MPEG-DASH",
                "Packet Bursts",
                "Remote Identification",
                "Convolutional Neural Networks"
            ]
        },
        "url": "URL#3511209",
        "sema_paperId": "7f3c3b4556795161de8e704c886ab8eab5c6ff74"
    },
    {
        "@score": "1",
        "@id": "3511210",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "58/5730",
                        "text": "J\u00f6rg Schwenk"
                    },
                    {
                        "@pid": "118/7486",
                        "text": "Marcus Niemietz"
                    },
                    {
                        "@pid": "57/11338",
                        "text": "Christian Mainka"
                    }
                ]
            },
            "title": "Same-Origin Policy: Evaluation in Modern Browsers.",
            "venue": "USENIX Security Symposium",
            "pages": "713-727",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/SchwenkNM17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/schwenk",
            "url": "https://dblp.org/rec/conf/uss/SchwenkNM17",
            "abstract": "The term Same-Origin Policy (SOP) is used to denote a complex set of rules which governs the interaction of different Web Origins within a web application. A subset of these SOP rules controls the interaction between the host document and an embedded document, and this subset is the target of our research (SOP-DOM). In contrast to other important concepts like Web Origins (RFC 6454) or the Document Object Model (DOM), there is no formal specification of the SOP-DOM. In an empirical study, we ran 544 different test cases on each of the 10 major web browsers. We show that in addition to Web Origins, access rights granted by SOPDOM depend on at least three attributes: the type of the embedding element (EE), the sandbox, and CORS attributes. We also show that due to the lack of a formal specification, different browser behaviors could be detected in approximately 23% of our test cases. The issues discovered in Internet Explorer and Edge are also acknowledged by Microsoft (MSRC Case 32703). We discuss our findings in terms of read, write, and execute rights in different access control models.",
            "keywords": [
                "Same-Origin Policy",
                "Web Origins",
                "SOP-DOM",
                "Cross-Origin Resource Sharing (CORS)",
                "Browser Compatibility Issues"
            ]
        },
        "url": "URL#3511210",
        "sema_paperId": "6b497e3c8f936630af2b486aed965450d5d7f2b0"
    },
    {
        "@score": "1",
        "@id": "3511212",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "202/9503",
                        "text": "Amit Kumar Sikder"
                    },
                    {
                        "@pid": "30/10201",
                        "text": "Hidayet Aksu"
                    },
                    {
                        "@pid": "46/1500",
                        "text": "A. Selcuk Uluagac"
                    }
                ]
            },
            "title": "6thSense: A Context-aware Sensor-based Attack Detector for Smart Devices.",
            "venue": "USENIX Security Symposium",
            "pages": "397-414",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/SikderAU17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/sikder",
            "url": "https://dblp.org/rec/conf/uss/SikderAU17",
            "abstract": "Sensors (e.g., light, gyroscope, accelerotmeter) and sensing enabled applications on a smart device make the applications more user-friendly and efficient. However, the current permission-based sensor management systems of smart devices only focus on certain sensors and any App can get access to other sensors by just accessing the generic sensor API. In this way, attackers can exploit these sensors in numerous ways: they can extract or leak users' sensitive information, transfer malware, or record or steal sensitive information from other nearby devices. In this paper, we propose 6thSense, a context-aware intrusion detection system which enhances the security of smart devices by observing changes in sensor data for different tasks of users and creating a contextual model to distinguish benign and malicious behavior of sensors. 6thSense utilizes three different Machine Learning-based detection mechanisms (i.e., Markov Chain, Naive Bayes, and LMT) to detect malicious behavior associated with sensors. We implemented 6thSense on a sensor-rich Android smart device (i.e., smartphone) and collected data from typical daily activities of 50 real users. Furthermore, we evaluated the performance of 6thSense against three sensor-based threats: (1) a malicious App that can be triggered via a sensor (e.g., light), (2) a malicious App that can leak information via a sensor, and (3) a malicious App that can steal data using sensors. Our extensive evaluations show that the 6thSense framework is an effective and practical approach to defeat growing sensor-based threats with an accuracy above 96% without compromising the normal functionality of the device. Moreover, our framework costs minimal overhead.",
            "keywords": [
                "Context-aware Intrusion Detection",
                "Sensor-based Threats",
                "Smart Device Security",
                "Malicious Sensor Behavior",
                "User Activity Monitoring"
            ]
        },
        "url": "URL#3511212",
        "sema_paperId": "224af6a9329adbdff76f835d730f52d89a24d66c"
    },
    {
        "@score": "1",
        "@id": "3511213",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "180/5453",
                        "text": "Rachee Singh"
                    },
                    {
                        "@pid": "35/8412",
                        "text": "Rishab Nithyanand"
                    },
                    {
                        "@pid": "29/7562-1",
                        "text": "Sadia Afroz 0001"
                    },
                    {
                        "@pid": "61/1749",
                        "text": "Paul Pearce"
                    },
                    {
                        "@pid": "45/1247",
                        "text": "Michael Carl Tschantz"
                    },
                    {
                        "@pid": "52/2893",
                        "text": "Phillipa Gill"
                    },
                    {
                        "@pid": "p/VernPaxson",
                        "text": "Vern Paxson"
                    }
                ]
            },
            "title": "Characterizing the Nature and Dynamics of Tor Exit Blocking.",
            "venue": "USENIX Security Symposium",
            "pages": "325-341",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/SinghNAPTGP17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/singh",
            "url": "https://dblp.org/rec/conf/uss/SinghNAPTGP17",
            "abstract": "Facing abusive traffic from the Tor anonymity network, online service providers discriminate against Tor users. In this study, we characterize not only the extent of such discrimination but also the nature of the undesired traffic originating from the Tor network\u2014a task complicated by Tor\u2019s need to maintain user anonymity. We address this challenge by leveraging multiple independent data sources: email complaints sent to exit operators, commercial IP blacklists, webpage crawls via Tor, and privacy-sensitive measurements of our own Tor exit nodes. As part of our study, we also develop methods for classifying email complaints and an interactive crawler to find subtle forms of discrimination, and deploy our own exits in various configurations to understand which are prone to discrimination. We find that conservative exit policies are ineffective in preventing the blacklisting of exit relays. However, a majority of the attacks originating from Tor generate high traffic volume, suggesting the possibility of detection and prevention without violating Tor users\u2019 privacy.",
            "pdf_url": "",
            "keywords": [
                "Tor Network",
                "Anonymity",
                "Traffic Discrimination",
                "Exit Relay Blacklisting",
                "User Privacy"
            ]
        },
        "url": "URL#3511213"
    },
    {
        "@score": "1",
        "@id": "3511214",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "15/4413",
                        "text": "Marc Stevens 0001"
                    },
                    {
                        "@pid": "24/7481",
                        "text": "Daniel Shumow"
                    }
                ]
            },
            "title": "Speeding up detection of SHA-1 collision attacks using unavoidable attack conditions.",
            "venue": "USENIX Security Symposium",
            "pages": "881-897",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/StevensS17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/stevens",
            "url": "https://dblp.org/rec/conf/uss/StevensS17",
            "abstract": "textabstractCounter-cryptanalysis, the concept of using cryptanalytic techniques to detect cryptanalytic attacks, was introduced by Stevens at CRYPTO 2013 [22] with a hash collision detection algorithm. That is, an algorithm that detects whether a given single message is part of a colliding message pair constructed using a cryptanalytic collision attack on MD5 or SHA-1. The concept's utility was proven when it was used to expose the then-unknown cryptanalytic collision attack exploited by the Flame espionage supermalware. So far there is a significant cost: to detect collision attacks against SHA-1 (respectively MD5) costs the equivalent of hashing the message 15 (respectively 224) times. In this paper we present a significant performance improvement for collision detection based on the new concept of unavoidable conditions. Unavoidable conditions are conditions that are necessary for all feasible attacks in a certain attack class. As such they can be used to quickly dismiss particular attack classes that may have been used in the construction of the message. To determine an unavoidable condition one must rule out any feasible variant attack where this condition might not be necessary, otherwise adversaries aware of counter-cryptanalysis could easily bypass this improved collision detection with a carefully chosen variant attack. We provide a formal model for unavoidable conditions for collision attacks on MD5-like compression functions. Furthermore, based on a conjecture solidly supported by the current state of the art, we show how we can determine such unavoidable conditions for SHA-1. We have implemented the improved SHA-1 collision detection using such unavoidable conditions and which is about 16 times faster than without our unavoidable condition improvements. We have measured that overall our implemented SHA-1 with collision detection is only a factor 1.96 slower, on average, than SHA-1. Our work is very timely given the recently announced SHA-1 collision proving that SHA-1 is now practically broken",
            "keywords": [
                "Collision Detection",
                "SHA-1",
                "Cryptanalysis",
                "Unavoidable Conditions",
                "Hash Function Vulnerabilities"
            ]
        },
        "url": "URL#3511214",
        "sema_paperId": "8135961453ec517a5576336c8e4e9ccca1a5d889"
    },
    {
        "@score": "1",
        "@id": "3511216",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "136/8343",
                        "text": "Ben Stock"
                    },
                    {
                        "@pid": "82/359",
                        "text": "Martin Johns"
                    },
                    {
                        "@pid": "205/2051",
                        "text": "Marius Steffens"
                    },
                    {
                        "@pid": "b/MichaelBackes1",
                        "text": "Michael Backes 0001"
                    }
                ]
            },
            "title": "How the Web Tangled Itself: Uncovering the History of Client-Side Web (In)Security.",
            "venue": "USENIX Security Symposium",
            "pages": "971-987",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/StockJS017",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/stock",
            "url": "https://dblp.org/rec/conf/uss/StockJS017",
            "abstract": "While in its early days, the Web was mostly static, it has organically grown into a full-fledged technology stack. This evolution has not followed a security blueprint, resulting in many classes of vulnerabilities specific to the Web. Even though the server-side code of the past has long since vanished, the Internet Archive gives us a unique view on the historical development of the Web\u2019s client side and its (in)security. Uncovering the insights which fueled this development bears the potential to not only gain a historical perspective on client-side Web security, but also to outline better practices going forward. To that end, we examined the code and header information of the most important Web sites for each year between 1997 and 2016, amounting to 659,710 different analyzed Web documents. From the archived data, we first identify key trends in the technology deployed on the client, such as the increasing complexity of clientside Web code and the constant rise of multi-origin application scenarios. Based on these findings, we then assess the advent of corresponding vulnerability classes, investigate their prevalence over time, and analyze the security mechanisms developed and deployed to mitigate them. Correlating these results allows us to draw a set of overarching conclusions: Along with the dawn of JavaScript-driven applications in the early years of the millennium, the likelihood of client-side injection vulnerabilities has risen. Furthermore, there is a noticeable gap in adoption speed between easy-to-deploy security headers and more involved measures such as CSP. But there is also no evidence that the usage of the easy-todeploy techniques reflects on other security areas. On the contrary, our data shows for instance that sites that use HTTPonly cookies are actually more likely to have a Cross-Site Scripting problem. Finally, we observe that the rising security awareness and introduction of dedicated security technologies had no immediate impact on the overall security of the client-side Web. 1 A Historic Perspective on Web Security The Web platform is arguably one of the biggest technological successes in the area of popular computing. What modestly started in 1991 as a mere transportation mechanism for hypertext documents is now the driving force behind the majority of today\u2019s dominating technologies. However, from a security point of view, the Web\u2019s track record is less than flattering, to a point in which a common joke under security professionals was to claim that the term Web security is actually an oxymoron. Over the years, Web technologies have given birth to a multitude of novel, Web-specific vulnerability classes, such as Cross-Site Scripting (XSS) or Clickjacking, which simply did not exist before, many of them manifesting themselves on the Web\u2019s client side. These ongoing developments are due to the fact that the Web\u2019s client side is under constant change and expansion. While early Web pages were mostly styled hypertext documents with limited interaction, modern Web sites push thousands of lines of code to the browser and implement non-trivial application logic. This ongoing development shows no signs of stopping or even slowing down. The trend is also underlined by the increase in client-side APIs in the browser: while in 2006 Firefox featured only 12 APIs, it now has support for 93 different APIs ranging from accurate timing information to an API to interact with Virtual Reality devices1. This unrestricted growth led to what Zalewski [41] dubbed The Tangled Web. Now, more than 25 years into the life of the Web, it is worthwhile to take a step back and revisit the development of Web security over the years. This allows us to gain a historical perspective on the security aspects of an emerging and constantly evolving computing platform and also foreshadows future trends. Unfortunately, the majority of Web code is commercial and, thus, not open to the public. Historic server1A list of all available features in current browsers is available at http://caniuse.com/ side code that has been replaced or taken offline cannot be studied anymore. However, the Web\u2019s client side, i.e., all Web code that is pushed in the form of HTML or JavaScript to the browser is public. And thankfully, the Internet Archive has recognized the historical significance of the Web\u2019s public face early on and attempts to preserve it since 1996. Thus, while the server-side portion of old Web applications is probably gone forever, the client-side counterpart is readily available via the Internet Archive\u2019s Wayback Machine. This enables a novel approach to historical security studies: A multitude of Web security problems, such as Client-Side XSS or Clickjacking, manifest themselves on the client side exclusively. Hence, evidence of these vulnerabilities is contained in the Internet Archive and thus available for examination. Many of the current state-of-the-art security testing methods can be adapted to work on the archived version of the sites, enabling an automated and scalable security evaluation of the historic code. Thus, we find that the archived client-side Web code offers the unique opportunity to study the security evolution of one of the most important technology platforms during (almost) its entire existence, allowing us to conduct historical analyses of a plethora of properties of the Web. This way, we are not only able to investigate past Web trends, but also draw conclusions on current and future Web development trends and (in)security. In the following, we give a brief overview of our conducted study and outline our research approach. Technological Evolution of the Web\u2019s Client Side We first examine the evolution of client-side technologies, i.e., which technologies prevailed in the history of the Web. We then systematically analyze the archived code on a syntactical level. The focus of this analysis step is on observable indicators that provide evidence on how diversity, complexity, and volume of this code has developed over the years, as all these three factors have a direct impact on the likelihood of vulnerabilities. Section 3 reports on our findings in this area. The overall goal of this activity is to enable the correlation of trends in the security area with ongoing technological shifts. Resulting Security Problems With the ever-growing complexity of the deployed Web code and the constant addition of new powerful capabilities in the Web browser in the form of novel JavaScript APIs the overall amount of potential vulnerability classes has risen as well. As motivated above, several of the vulnerabilities which exclusively affect the client side have been properly archived and, thus, can be reliably detected in the historical data. We leverage this capability to assess a lower bound of vulnerable Web sites over the years. Section 4 documents our security testing methodology and highlights our key findings in the realm of preserved security vulnerabilites. Introduction of Dedicated Security Mechanisms To meet the new challenges of the steadily increasing security surface on the Web\u2019s client side, several dedicated mechanisms, such as security-centric HTTP headers or JavaScript APIs, have been introduced. We examine if and how these mechanisms have been adopted during their lifespan. This provides valuable evidence with respect how the awareness of security issues has changed over time and if this awareness manifests itself in overall improvements of the site\u2019s security characteristics. We discuss the selected mechanisms and the results of our analysis in Section 5. Overarching Implications of our Analysis Based on the findings of our 20-year-long study, we analyze the implications of our collected data in Section 6. By looking at historical trends and correlating the individual data items, we can draw a number of conclusions regarding the interdependencies of client-side technology and client-side security. Moreover, we investigate correlations between actual vulnerabilities discovered in historical Web applications and the existence of security awareness indicators at the time, and finish with a discussion of important next steps for Client-Side Web security.",
            "keywords": [
                "Client-Side Web Security",
                "Web Vulnerabilities",
                "Historical Analysis",
                "Security Mechanisms",
                "JavaScript APIs"
            ]
        },
        "url": "URL#3511216",
        "sema_paperId": "a69d94eaca7436a1bbc3a84ce09b0793e0b24e8b"
    },
    {
        "@score": "1",
        "@id": "3511218",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "17/686",
                        "text": "Yang Su"
                    },
                    {
                        "@pid": "98/8283",
                        "text": "Daniel Genkin"
                    },
                    {
                        "@pid": "56/8102",
                        "text": "Damith Chinthana Ranasinghe"
                    },
                    {
                        "@pid": "90/10901",
                        "text": "Yuval Yarom"
                    }
                ]
            },
            "title": "USB Snooping Made Easy: Crosstalk Leakage Attacks on USB Hubs.",
            "venue": "USENIX Security Symposium",
            "pages": "1145-1161",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/SuGRY17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/su",
            "url": "https://dblp.org/rec/conf/uss/SuGRY17",
            "abstract": "The Universal Serial Bus (USB) is the most prominent interface for connecting peripheral devices to computers. USB-connected input devices, such as keyboards, cardswipers and fingerprint readers, often send sensitive information to the computer. As such information is only sent along the communication path from the device to the computer, it was hitherto thought to be protected from potentially compromised devices outside this path. We have tested over 50 different computers and external hubs and found that over 90% of them suffer from a crosstalk leakage effect that allows malicious peripheral devices located off the communication path to capture and observe sensitive USB traffic. We also show that in many cases this crosstalk leakage can be observed on the USB power lines, thus defeating a common USB isolation countermeasure of using a charge-only USB cable which physically disconnects the USB data lines. Demonstrating the attack\u2019s low costs and ease of concealment, we modify a novelty USB lamp to implement an off-path attack which captures and exfiltrates USB traffic when connected to a vulnerable internal or a external USB hub.",
            "keywords": [
                "USB Security",
                "Crosstalk Leakage",
                "Data Exfiltration",
                "Malicious Peripheral Devices",
                "USB Hub Vulnerabilities"
            ]
        },
        "url": "URL#3511218",
        "sema_paperId": "4d6ae745a94cb425fa817b9442cfbd13d1c88411"
    },
    {
        "@score": "1",
        "@id": "3511220",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "68/5424",
                        "text": "Adrian Tang"
                    },
                    {
                        "@pid": "46/652",
                        "text": "Simha Sethumadhavan"
                    },
                    {
                        "@pid": "s/SalvatoreJStolfo",
                        "text": "Salvatore J. Stolfo"
                    }
                ]
            },
            "title": "CLKSCREW: Exposing the Perils of Security-Oblivious Energy Management.",
            "venue": "USENIX Security Symposium",
            "pages": "1057-1074",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/TangSS17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/tang",
            "url": "https://dblp.org/rec/conf/uss/TangSS17",
            "abstract": "The need for power- and energy-ef\ufb01cient computing has resulted in aggressive cooperative hardware-software energy management mechanisms on modern commodity devices. Most systems today, for example, allow software to control the frequency and voltage of the underlying hardware at a very \ufb01ne granularity to extend battery life. Despite their bene\ufb01ts, these software-exposed energy management mechanisms pose grave security implications that have not been studied before. In this work, we present the CLK SCREW attack, a new class of fault attacks that exploit the security-obliviousness of energy management mechanisms to break security. A novel bene\ufb01t for the attackers is that these fault attacks become more accessible since they can now be conducted without the need for physical access to the devices or fault injection equipment. We demonstrate CLK SCREW on commodity ARM/Android devices. We show that a malicious kernel driver (1) can extract secret cryptographic keys from Trustzone, and (2) can escalate its privileges by loading self-signed code into Trustzone. As the \ufb01rst work to show the security rami\ufb01cations of energy management mechanisms, we urge the community to re-examine these security-oblivious designs.",
            "keywords": [
                "Energy Management Security",
                "Fault Attacks",
                "CLK SCREW Attack",
                "Cryptographic Key Extraction",
                "Trustzone Privilege Escalation"
            ]
        },
        "url": "URL#3511220",
        "sema_paperId": "5db14b549618a98e31e2847d859495dd49863d96"
    },
    {
        "@score": "1",
        "@id": "3511222",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "39/5423-1",
                        "text": "Yuan Tian 0001"
                    },
                    {
                        "@pid": "28/6297-18",
                        "text": "Nan Zhang 0018"
                    },
                    {
                        "@pid": "42/2165",
                        "text": "Yue-Hsun Lin"
                    },
                    {
                        "@pid": "06/6268",
                        "text": "XiaoFeng Wang 0001"
                    },
                    {
                        "@pid": "66/9786",
                        "text": "Blase Ur"
                    },
                    {
                        "@pid": "199/8295",
                        "text": "Xianzheng Guo"
                    },
                    {
                        "@pid": "89/1945",
                        "text": "Patrick Tague"
                    }
                ]
            },
            "title": "SmartAuth: User-Centered Authorization for the Internet of Things.",
            "venue": "USENIX Security Symposium",
            "pages": "361-378",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/TianZLWUGT17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/tian",
            "url": "https://dblp.org/rec/conf/uss/TianZLWUGT17",
            "abstract": "Internet of Things (IoT) platforms often require users to grant permissions to third-party apps, such as the ability to control a lock. Unfortunately, because few users act based upon, or even comprehend, permission screens, malicious or careless apps can become overprivileged by requesting unneeded permissions. To meet the IoT\u2019s unique security demands, such as cross-device, context-based, and automatic operations, we present a new design that supports user-centric, semantic-based \u201csmart\u201d authorization. Our technique, called SmartAuth , automatically collects security-relevant information from an IoT app\u2019s description, code and annotations, and generates an authorization user interface to bridge the gap between the functionalities explained to the user and the operations the app actually performs. Through the interface, security policies can be generated and enforced by enhancing existing platforms. To address the unique challenges in IoT app authorization, where states of multiple devices are used to determine the operations that can happen on other devices, we devise new technologies that link a device\u2019s context (e.g., a humidity sensor in a bath room) to an activity\u2019s semantics (e.g., taking a bath) using natural language processing and program analysis. We evaluate SmartAuth through user studies, \ufb01nding participants who use SmartAuth are signi\ufb01cantly more likely to avoid overprivileged apps.",
            "keywords": [
                "Internet of Things (IoT)",
                "User-Centered Design",
                "Authorization Mechanisms",
                "Overprivileged Apps",
                "Context-Based Security"
            ]
        },
        "url": "URL#3511222",
        "sema_paperId": "354a89cada16c0b968f454ca6d20a20ab5a27e39"
    },
    {
        "@score": "1",
        "@id": "3511225",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "195/6219",
                        "text": "Pepe Vila"
                    },
                    {
                        "@pid": "10/3908",
                        "text": "Boris K\u00f6pf"
                    }
                ]
            },
            "title": "Loophole: Timing Attacks on Shared Event Loops in Chrome.",
            "venue": "USENIX Security Symposium",
            "pages": "849-864",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/VilaK17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/vila",
            "url": "https://dblp.org/rec/conf/uss/VilaK17",
            "abstract": "Event-driven programming (EDP) is the prevalent paradigm for graphical user interfaces, web clients, and it is rapidly gaining importance for server-side and network programming. Central components of EDP are event loops, which act as FIFO queues that are used by processes to store and dispatch messages received from other processes.\nIn this paper we demonstrate that shared event loops are vulnerable to side-channel attacks, where a spy process monitors the loop usage pattern of other processes by enqueueing events and measuring the time it takes for them to be dispatched. Specifically, we exhibit attacks against the two central event loops in Google\u2019s Chrome web browser: that of the I/O thread of the host process, which multiplexes all network events and user actions, and that of the main thread of the renderer processes, which handles rendering and Javascript tasks.\nFor each of these loops, we show how the usage pattern can be monitored with high resolution and low overhead, and how this can be abused for malicious purposes, such as web page identification, user behavior detection, and covert communication.",
            "pdf_url": "",
            "keywords": [
                "Event-Driven Programming",
                "Shared Event Loops",
                "Timing Attacks",
                "Side-Channel Attacks",
                "Chrome Vulnerabilities"
            ]
        },
        "url": "URL#3511225"
    },
    {
        "@score": "1",
        "@id": "3511226",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "145/3288-1",
                        "text": "Tianhao Wang 0001"
                    },
                    {
                        "@pid": "30/8037",
                        "text": "Jeremiah Blocki"
                    },
                    {
                        "@pid": "l/NinghuiLi",
                        "text": "Ninghui Li"
                    },
                    {
                        "@pid": "j/SomeshJha",
                        "text": "Somesh Jha"
                    }
                ]
            },
            "title": "Locally Differentially Private Protocols for Frequency Estimation.",
            "venue": "USENIX Security Symposium",
            "pages": "729-745",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/WangBLJ17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/wang-tianhao",
            "url": "https://dblp.org/rec/conf/uss/WangBLJ17",
            "abstract": "Protocols satisfying Local Differential Privacy (LDP) enable parties to collect aggregate information about a population while protecting each user\u2019s privacy, without relying on a trusted third party. LDP protocols (such as Google\u2019s RAPPOR) have been deployed in real-world scenarios. In these protocols, a user encodes his private information and perturbs the encoded value locally before sending it to an aggregator, who combines values that users contribute to infer statistics about the population. In this paper, we introduce a framework that generalizes several LDP protocols proposed in the literature. Our framework yields a simple and fast aggregation algorithm, whose accuracy can be precisely analyzed. Our in-depth analysis enables us to choose optimal parameters, resulting in two new protocols (i.e., Optimized Unary Encoding and Optimized Local Hashing) that provide better utility than protocols previously proposed. We present precise conditions for when each proposed protocol should be used, and perform experiments that demonstrate the advantage of our proposed protocols.",
            "keywords": [
                "Local Differential Privacy",
                "Frequency Estimation",
                "LDP Protocols",
                "Data Aggregation",
                "Privacy Preservation"
            ]
        },
        "url": "URL#3511226",
        "sema_paperId": "659996fa1d5c8eb8d5d92e94318156248cf33176"
    },
    {
        "@score": "1",
        "@id": "3511227",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "12/5838-12",
                        "text": "Tao Wang 0012"
                    },
                    {
                        "@pid": "04/6434",
                        "text": "Ian Goldberg"
                    }
                ]
            },
            "title": "Walkie-Talkie: An Efficient Defense Against Passive Website Fingerprinting Attacks.",
            "venue": "USENIX Security Symposium",
            "pages": "1375-1390",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/WangG17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/wang-tao",
            "url": "https://dblp.org/rec/conf/uss/WangG17",
            "abstract": "Website fingerprinting (WF) is a traffic analysis attack that allows an eavesdropper to determine the web activity of a client, even if the client is using privacy technologies such as proxies, VPNs, or Tor. Recent work has highlighted the threat of website fingerprinting to privacy-sensitive web users. Many previously designed defenses against website fingerprinting have been broken by newer attacks that use better classifiers. The remaining effective defenses are inefficient: they hamper user experience and burden the server with large overheads. \n \nIn this work we propose Walkie-Talkie, an effective and efficient WF defense. Walkie-Talkie modifies the browser to communicate in half-duplex mode rather than the usual full-duplex mode; half-duplex mode produces easily moldable burst sequences to leak less information to the adversary, at little additional overhead. Designed for the open-world scenario, Walkie-Talkie molds burst sequences so that sensitive and non-sensitive pages look the same. Experimentally, we show that Walkie-Talkie can defeat all known WF attacks with a bandwidth overhead of 31% and a time overhead of 34%, which is far more efficient than all effective WF defenses (often exceeding 100% for both types of overhead). In fact, we show that Walkie-Talkie cannot be defeated by any website fingerprinting attack, even hypothetical advanced attacks that use site link information, page visit rates, and intercell timing.",
            "keywords": [
                "Website Fingerprinting",
                "Traffic Analysis",
                "Privacy Protection",
                "Half-Duplex Communication",
                "Walkie-Talkie Defense"
            ]
        },
        "url": "URL#3511227",
        "sema_paperId": "d5498ce58f577bec2ec5097ea10cd6dd4360c097"
    },
    {
        "@score": "1",
        "@id": "3511228",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "90/4693-10",
                        "text": "Pengfei Wang 0010"
                    },
                    {
                        "@pid": "k/JensKrinke",
                        "text": "Jens Krinke"
                    },
                    {
                        "@pid": "31/6932",
                        "text": "Kai Lu"
                    },
                    {
                        "@pid": "28/538-2",
                        "text": "Gen Li 0002"
                    },
                    {
                        "@pid": "132/2226",
                        "text": "Steve Dodier-Lazaro"
                    }
                ]
            },
            "title": "How Double-Fetch Situations turn into Double-Fetch Vulnerabilities: A Study of Double Fetches in the Linux Kernel.",
            "venue": "USENIX Security Symposium",
            "pages": "1-16",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/WangKL0D17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/wang-pengfei",
            "url": "https://dblp.org/rec/conf/uss/WangKL0D17",
            "abstract": "We present the first static approach that systematically \ndetects potential double-fetch vulnerabilities in the Linux kernel. Using a pattern-based analysis, we identified 90 \ndouble fetches in the Linux kernel. 57 of these occur \nin drivers, which previous dynamic approaches were unable \nto detect without access to the corresponding hardware. \nWe manually investigated the 90 occurrences, and \ninferred three typical scenarios in which double fetches \noccur. We discuss each of them in detail. We further developed \na static analysis, based on the Coccinelle matching \nengine, that detects double-fetch situations which can \ncause kernel vulnerabilities. When applied to the Linux, \nFreeBSD, and Android kernels, our approach found six \npreviously unknown double-fetch bugs, four of them in \ndrivers, three of which are exploitable double-fetch vulnerabilities. \nAll of the identified bugs and vulnerabilities \nhave been confirmed and patched by maintainers. Our \napproach has been adopted by the Coccinelle team and \nis currently being integrated into the Linux kernel patch \nvetting. Based on our study, we also provide practical solutions \nfor anticipating double-fetch bugs and vulnerabilities. \nWe also provide a solution to automatically patch \ndetected double-fetch bugs.",
            "keywords": [
                "Kernel Vulnerabilities",
                "Double-Fetch",
                "Static Analysis",
                "Linux Kernel",
                "Coccinelle"
            ]
        },
        "url": "URL#3511228",
        "sema_paperId": "13848ff84a88cb3c90ba907f466f6e5b09a0aeb1"
    },
    {
        "@score": "1",
        "@id": "3511229",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "42/1503-11",
                        "text": "Shuai Wang 0011"
                    },
                    {
                        "@pid": "83/4555-7",
                        "text": "Pei Wang 0007"
                    },
                    {
                        "@pid": "82/1364-25",
                        "text": "Xiao Liu 0025"
                    },
                    {
                        "@pid": "23/3719",
                        "text": "Danfeng Zhang"
                    },
                    {
                        "@pid": "54/2696",
                        "text": "Dinghao Wu"
                    }
                ]
            },
            "title": "CacheD: Identifying Cache-Based Timing Channels in Production Software.",
            "venue": "USENIX Security Symposium",
            "pages": "235-252",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/WangWLZW17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/wang-shuai",
            "url": "https://dblp.org/rec/conf/uss/WangWLZW17",
            "abstract": "Side-channel attacks recover secret information by analyzing the physical implementation of cryptosystems based on non-functional computational characteristics, e.g. time, power, and memory usage. Among all wellknown side channels, cache-based timing channels are notoriously severe, leading to practical attacks against certain implementations of theoretically secure crypto algorithms, such as RSA, ElGamal and AES. Such attacks target the hierarchical design of the modern computer memory system, where different memory access patterns of a program can bring observable timing difference. In this work, we propose a novel technique to help software developers identify potential vulnerabilities that can lead to cache-based timing attacks. Our technique leverages symbolic execution and constraint solving to detect potential cache differences at each program point. We adopt a cache model that is general enough to capture various threat models that are employed in practical timing attacks. Our modeling and analysis are based on the formulation of cache access at different program locations along execution traces. We have implemented the proposed technique as a practical tool named CacheD (Cache Difference), and evaluated CacheD towards multiple real-world cryptosystems. CacheD takes less than 17 CPU hours to analyze 9 widely used cryptographic algorithm implementations with over 120 million instructions in total. The evaluation results show that our technique can accurately identify vulnerabilities reported by previous research. Moreover, we have successfully discovered previously unknown issues in two widely used cryptosystems, OpenSSL and Botan.",
            "keywords": [
                "Cache-Based Timing Channels",
                "Side-Channel Attacks",
                "Cryptographic Vulnerabilities",
                "Symbolic Execution",
                "CacheD Tool"
            ]
        },
        "url": "URL#3511229",
        "sema_paperId": "58ef321ef31e5385c3fc73a688ac539cebdc1ca4"
    },
    {
        "@score": "1",
        "@id": "3511233",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "19/360-24",
                        "text": "Lei Xu 0024"
                    },
                    {
                        "@pid": "68/4706-1",
                        "text": "Jeff Huang 0001"
                    },
                    {
                        "@pid": "77/7581",
                        "text": "Sungmin Hong"
                    },
                    {
                        "@pid": "49/11187",
                        "text": "Jialong Zhang"
                    },
                    {
                        "@pid": "64/1147",
                        "text": "Guofei Gu"
                    }
                ]
            },
            "title": "Attacking the Brain: Races in the SDN Control Plane.",
            "venue": "USENIX Security Symposium",
            "pages": "451-468",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/XuHHZG17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/xu-lei",
            "url": "https://dblp.org/rec/conf/uss/XuHHZG17",
            "abstract": "Software-Defined Networking (SDN) has significantly enriched network functionalities by decoupling programmable network controllers from the network hardware. Because SDN controllers are serving as the brain of the entire network, their security and reliability are of extreme importance. For the first time in the literature, we introduce a novel attack against SDN networks that can cause serious security and reliability risks by exploiting harmful race conditions in the SDN controllers, similar in spirit to classic TOCTTOU (Time of Check to Time of Use) attacks against file systems. In this attack, even a weak adversary without controlling/compromising any SDN controller/switch/app/protocol but only having malware-infected regular hosts can generate external network events to crash the SDN controllers, disrupt core services, or steal privacy information. We develop a novel dynamic framework, CONGUARD, that can effectively detect and exploit harmful race conditions. We have evaluated CONGUARD on three mainstream SDN controllers (Floodlight, ONOS, and OpenDaylight) with 34 applications. CONGUARD detected totally 15 previously unknown vulnerabilities, all of which have been confirmed by developers and 12 of them are patched with our assistance.",
            "keywords": [
                "Software-Defined Networking",
                "SDN Controllers",
                "Race Conditions",
                "Security Vulnerabilities",
                "CONGUARD Framework"
            ]
        },
        "url": "URL#3511233",
        "sema_paperId": "13d2ab0755155c59d2fb041de174a8bbdd68d556"
    },
    {
        "@score": "1",
        "@id": "3511234",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "75/4287-1",
                        "text": "Meng Xu 0001"
                    },
                    {
                        "@pid": "38/8882",
                        "text": "Taesoo Kim"
                    }
                ]
            },
            "title": "PlatPal: Detecting Malicious Documents with Platform Diversity.",
            "venue": "USENIX Security Symposium",
            "pages": "271-287",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/XuK17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/xu-meng",
            "url": "https://dblp.org/rec/conf/uss/XuK17",
            "abstract": "Due to the continued exploitation of Adobe Reader, malicious document (maldoc) detection has become a pressing problem. Although many solutions have been proposed, recent works have highlighted some common drawbacks, such as parser-confusion and classifier-evasion attacks. In response to this, we propose a new perspective for maldoc detection: platform diversity. In particular, we identify eight factors in OS design and implementation that could cause behavioral divergences under attack, ranging from syscall semantics (more obvious) to heap object metadata structure (more subtle) and further show how they can thwart attackers from finding bugs, exploiting bugs, or performing malicious activities. We further prototype PLATPAL to systematically harvest platform diversity. PLATPAL hooks into Adobe Reader to trace internal PDF processing and also uses sandboxed execution to capture a maldoc\u2019s impact on the host system. Execution traces on different platforms are compared, and maldoc detection is based on the observation that a benign document behaves the same across platforms, while a maldoc behaves differently during exploitation. Evaluations show that PLATPAL raises no false alarms in benign samples, detects a variety of behavioral discrepancies in malicious samples, and is a scalable and practical solution.",
            "keywords": [
                "Malicious Document Detection",
                "Platform Diversity",
                "Behavioral Discrepancies",
                "Exploitation Prevention",
                "Adobe Reader Security"
            ]
        },
        "url": "URL#3511234",
        "sema_paperId": "c17d5f10dc4693e397edf2b0cc9a9658fd0d9d71"
    },
    {
        "@score": "1",
        "@id": "3511235",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "90/514-24",
                        "text": "Jun Xu 0024"
                    },
                    {
                        "@pid": "187/8991",
                        "text": "Dongliang Mu"
                    },
                    {
                        "@pid": "40/6431",
                        "text": "Xinyu Xing"
                    },
                    {
                        "@pid": "21/6121-5",
                        "text": "Peng Liu 0005"
                    },
                    {
                        "@pid": "33/3675-3",
                        "text": "Ping Chen 0003"
                    },
                    {
                        "@pid": "98/5039-1",
                        "text": "Bing Mao 0001"
                    }
                ]
            },
            "title": "Postmortem Program Analysis with Hardware-Enhanced Post-Crash Artifacts.",
            "venue": "USENIX Security Symposium",
            "pages": "17-32",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/XuMX0CM17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/xu-jun",
            "url": "https://dblp.org/rec/conf/uss/XuMX0CM17",
            "abstract": "While a core dump carries a large amount of information, it barely serves as informative debugging aids in locating software faults because it carries information that indicates only a partial chronology of how program reached a crash site. Recently, this situation has been signi\ufb01cantly improved. With the emergence of hardware-assisted processor tracing, software developers and security analysts can trace program execution and integrate them into a core dump. In comparison with an ordinary core dump, the new post-crash artifact provides software developers and security analysts with more clues as to a program crash. To use it for failure diagnosis, however, it still requires strenuous manual efforts. In this work, we propose POMP , an automated tool to facilitate the analysis of post-crash artifacts. More specifically, POMP introduces a new reverse execution mechanism to construct the data \ufb02ow that a program followed prior to its crash. By using the data \ufb02ow, POMP then performs backward taint analysis and highlights those program statements that actually contribute to the crash. To demonstrate its effectiveness in pinpointing program statements truly pertaining to a program crash, we have implemented POMP for Linux system on x86-32 platform, and tested it against various program crashes resulting from 31 distinct real-world security vulnerabilities. We show that, POMP can accurately and ef\ufb01ciently pinpoint program statements that truly pertain to the crashes, making failure diagnosis signi\ufb01cantly convenient.",
            "keywords": [
                "Postmortem Analysis",
                "Core Dump",
                "Hardware-Assisted Tracing",
                "Failure Diagnosis",
                "Backward Taint Analysis"
            ]
        },
        "url": "URL#3511235",
        "sema_paperId": "3d45591c638a28aec92571a3916aae42a29ede47"
    },
    {
        "@score": "1",
        "@id": "3511236",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "68/2052-1",
                        "text": "Lei Xue 0001"
                    },
                    {
                        "@pid": "15/7381",
                        "text": "Yajin Zhou"
                    },
                    {
                        "@pid": "19/1766-2",
                        "text": "Ting Chen 0002"
                    },
                    {
                        "@pid": "53/1565",
                        "text": "Xiapu Luo"
                    },
                    {
                        "@pid": "64/1147",
                        "text": "Guofei Gu"
                    }
                ]
            },
            "title": "Malton: Towards On-Device Non-Invasive Mobile Malware Analysis for ART.",
            "venue": "USENIX Security Symposium",
            "pages": "289-306",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/XueZCLG17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/xue",
            "url": "https://dblp.org/rec/conf/uss/XueZCLG17",
            "abstract": "It\u2019s an essential step to understand malware\u2019s behaviors for developing effective solutions. Though a number of systems have been proposed to analyze Android malware, they have been limited by incomplete view of inspection on a single layer. What\u2019s worse, various new techniques (e.g., packing, anti-emulator, etc.) employed by the latest malware samples further make these systems ineffective. In this paper, we propose Malton, a novel on-device non-invasive analysis platform for the new Android runtime (i.e., the ART runtime). As a dynamic analysis tool, Malton runs on real mobile devices and provides a comprehensive view of malware\u2019s behaviors by conducting multi-layer monitoring and information flow tracking, as well as efficient path exploration. We have carefully evaluated Malton using real-world malware samples. The experimental results showed that Malton is more effective than existing tools, with the capability to analyze sophisticated malware samples and provide a comprehensive view of malicious behaviors of these samples.",
            "keywords": [
                "Mobile Malware Analysis",
                "Android Runtime",
                "Non-Invasive Analysis",
                "Malware Behavior Monitoring",
                "Information Flow Tracking"
            ]
        },
        "url": "URL#3511236",
        "sema_paperId": "be8ae75ee7051ffd79ce6349f68dabfa8db9ec69"
    },
    {
        "@score": "1",
        "@id": "3511237",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "202/6740",
                        "text": "Zhaomo Yang"
                    },
                    {
                        "@pid": "157/3785",
                        "text": "Brian Johannesmeyer"
                    },
                    {
                        "@pid": "205/2078",
                        "text": "Anders Trier Olesen"
                    },
                    {
                        "@pid": "11/3644",
                        "text": "Sorin Lerner"
                    },
                    {
                        "@pid": "l/KirillLevchenko",
                        "text": "Kirill Levchenko"
                    }
                ]
            },
            "title": "Dead Store Elimination (Still) Considered Harmful.",
            "venue": "USENIX Security Symposium",
            "pages": "1025-1040",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/YangJOLL17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/yang",
            "url": "https://dblp.org/rec/conf/uss/YangJOLL17",
            "abstract": "Dead store elimination is a widely used compiler optimization that reduces code size and improves performance. However, it can also remove seemingly useless memory writes that the programmer intended to clear sensitive data after its last use. Security-savvy developers have long been aware of this phenomenon and have devised ways to prevent the compiler from eliminating these data scrubbing operations. In this paper, we survey the set of techniques found in the wild that are intended to prevent data-scrubbing operations from being removed during dead store elimination. We evaluated the effectiveness and availability of each technique and found that some fail to protect data-scrubbing writes. We also examined eleven open source security projects to determine whether their specific memory scrubbing function was effective and whether it was used consistently. We found four of the eleven projects using flawed scrubbing techniques that may fail to scrub sensitive data and an additional four projects not using their scrubbing function consistently. We address the problem of dead store elimination removing scrubbing operations with a compiler-based approach by adding a new option to an LLVM-based compiler that retains scrubbing operations. We also synthesized existing techniques to develop a best-of-breed scrubbing function and are making it available to developers.",
            "keywords": [
                "Dead Store Elimination",
                "Compiler Optimization",
                "Memory Scrubbing",
                "Data Sensitivity",
                "LLVM Compiler"
            ]
        },
        "url": "URL#3511237",
        "sema_paperId": "5a65b02a458c40fcdca0402798ec3a9e4d8ea304"
    },
    {
        "@score": "1",
        "@id": "3511238",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "21/3626-22",
                        "text": "Fan Zhang 0022"
                    },
                    {
                        "@pid": "86/8318",
                        "text": "Ittay Eyal"
                    },
                    {
                        "@pid": "41/8609",
                        "text": "Robert Escriva"
                    },
                    {
                        "@pid": "j/AriJuels",
                        "text": "Ari Juels"
                    },
                    {
                        "@pid": "r/RvRenesse",
                        "text": "Robbert van Renesse"
                    }
                ]
            },
            "title": "REM: Resource-Efficient Mining for Blockchains.",
            "venue": "USENIX Security Symposium",
            "pages": "1427-1444",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/ZhangEEJR17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/zhang",
            "url": "https://dblp.org/rec/conf/uss/ZhangEEJR17",
            "abstract": "Blockchains show promise as potential infrastructure for financial transaction systems. The security of blockchains today, however, relies critically on Proof-of- Work (PoW), which forces participants to waste computational resources.\nWe present REM (Resource-Efficient Mining), a new blockchain mining framework that uses trusted hardware (Intel SGX). REM achieves security guarantees similar to PoW, but leverages the partially decentralized trust model inherent in SGX to achieve a fraction of the waste of PoW. Its key idea, Proof-of-Useful-Work (PoUW), involves miners providing trustworthy reporting on CPU cycles they devote to inherently useful workloads. REM flexibly allows any entity to create a useful workload. REM ensures the trustworthiness of these workloads by means of a novel scheme of hierarchical attestations that may be of independent interest.\nTo address the risk of compromised SGX CPUs, we develop a statistics-based formal security framework, also relevant to other trusted-hardware-based approaches such as Intel\u2019s Proof of Elapsed Time (PoET). We show through economic analysis that REM achieves less waste than PoET and variant schemes.\nWe implement REM and, as an example application, swap it into the consensus layer of Bitcoin core. The result is the first full implementation of an SGX-based blockchain. We experiment with four example applications as useful workloads for our implementation of REM, and report a computational overhead of 5\u201415%.",
            "pdf_url": "",
            "keywords": [
                "Blockchain Mining",
                "Resource Efficiency",
                "Proof-of-Useful-Work",
                "Trusted Hardware",
                "Intel SGX"
            ]
        },
        "url": "URL#3511238"
    },
    {
        "@score": "1",
        "@id": "3511239",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "84/9827",
                        "text": "Siqi Zhao"
                    },
                    {
                        "@pid": "99/1702",
                        "text": "Xuhua Ding"
                    },
                    {
                        "@pid": "42/1870",
                        "text": "Wen Xu"
                    },
                    {
                        "@pid": "72/1963",
                        "text": "Dawu Gu"
                    }
                ]
            },
            "title": "Seeing Through The Same Lens: Introspecting Guest Address Space At Native Speed.",
            "venue": "USENIX Security Symposium",
            "pages": "799-813",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/ZhaoDXG17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/zhao",
            "url": "https://dblp.org/rec/conf/uss/ZhaoDXG17",
            "abstract": "Software-based MMU emulation lies at the heart of outof-VM live memory introspection, an important technique in the cloud setting that applications such as live forensics and intrusion detection depend on. Due to the emulation, the software-based approach is much slower compared to nativememory accessby theguest VM. The slowness not only results in undetected transient malicious behavior, but also inconsistent memory view with theguest; both undermine theeffectivenessof introspection. We propose the immersive execution environment (ImEE) with which the guest memory is accessed at native speed without any emulation. Meanwhile, the address mappings used within the ImEE are ensured to be consistent with the guest throughout the introspection session. We have implemented a prototype of the ImEE on Linux KVM. The experiment results show that ImEE-based introspection enjoysaremarkablespeed up, performing several hundred times faster than the legacy method. Hence, this design is especially useful for realtimemonitoring, incident responseand high-intensity introspection.",
            "keywords": [
                "Memory Introspection",
                "MMU Emulation",
                "Cloud Forensics",
                "Real-time Monitoring",
                "Immersive Execution Environment (ImEE)"
            ]
        },
        "url": "URL#3511239",
        "sema_paperId": "e9b32a3afcb5332bd53864ebdacdd82bce933c8a"
    },
    {
        "@score": "1",
        "@id": "3511240",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "150/5253",
                        "text": "Sebastian Zimmeck"
                    },
                    {
                        "@pid": "205/2248",
                        "text": "Jie S. Li"
                    },
                    {
                        "@pid": "135/2642",
                        "text": "Hyungtae Kim"
                    },
                    {
                        "@pid": "00/1396",
                        "text": "Steven M. Bellovin"
                    },
                    {
                        "@pid": "43/4734",
                        "text": "Tony Jebara"
                    }
                ]
            },
            "title": "A Privacy Analysis of Cross-device Tracking.",
            "venue": "USENIX Security Symposium",
            "pages": "1391-1408",
            "year": "2017",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/uss/ZimmeckLKBJ17",
            "ee": "https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/zimmeck",
            "url": "https://dblp.org/rec/conf/uss/ZimmeckLKBJ17",
            "abstract": "Online tracking is evolving from browserand devicetracking to people-tracking. As users are increasingly accessing the Internet from multiple devices this new paradigm of tracking\u2014in most cases for purposes of advertising\u2014is aimed at crossing the boundary between a user\u2019s individual devices and browsers. It establishes a person-centric view of a user across devices and seeks to combine the input from various data sources into an individual and comprehensive user profile. By its very nature such cross-device tracking can principally reveal a complete picture of a person and, thus, become more privacy-invasive than the siloed tracking via HTTP cookies or other traditional and more limited tracking mechanisms. In this study we are exploring cross-device tracking techniques as well as their privacy implications. Particularly, we demonstrate a method to detect the occurrence of cross-device tracking, and, based on a cross-device tracking dataset that we collected from 126 Internet users, we explore the prevalence of cross-device trackers on mobile and desktop devices. We show that the similarity of IP addresses and Internet history for a user\u2019s devices gives rise to a matching rate of F-1 = 0.91 for connecting a mobile to a desktop device in our dataset. This finding is especially noteworthy in light of the increase in learning power that cross-device companies may achieve by leveraging user data from more than one device. Given these privacy implications of cross-device tracking we also examine compliance with applicable self-regulation for 40 cross-device companies and find that some are not transparent about their practices.",
            "keywords": [
                "Cross-device Tracking",
                "User Privacy",
                "Data Profiling",
                "Tracking Techniques",
                "Privacy Compliance"
            ]
        },
        "url": "URL#3511240",
        "sema_paperId": "9173dfbc04d78959cae41add10d3cf25e06cabbc"
    },
    {
        "@score": "1",
        "@id": "3524244",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "k/EnginKirda",
                        "text": "Engin Kirda"
                    },
                    {
                        "@pid": "26/3399",
                        "text": "Thomas Ristenpart"
                    }
                ]
            },
            "title": "26th USENIX Security Symposium, USENIX Security 2017, Vancouver, BC, Canada, August 16-18, 2017.",
            "venue": "USENIX Security Symposium",
            "publisher": "USENIX Association",
            "year": "2017",
            "type": "Editorship",
            "access": "open",
            "key": "conf/uss/2017",
            "ee": "https://www.usenix.org/conference/usenixsecurity17",
            "url": "https://dblp.org/rec/conf/uss/2017",
            "abstract": null
        },
        "url": "URL#3524244",
        "sema_paperId": "418bbb4361b91579e98f4912131d866be3bbf9b8"
    }
]