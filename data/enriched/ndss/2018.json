[
    {
        "@score": "1",
        "@id": "3127737",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "08/1117-1",
                        "text": "Michael Schwarz 0001"
                    },
                    {
                        "@pid": "172/1127",
                        "text": "Moritz Lipp"
                    },
                    {
                        "@pid": "166/1494",
                        "text": "Daniel Gruss"
                    }
                ]
            },
            "title": "JavaScript Zero: Real JavaScript and Zero Side-Channel Attacks.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/0001LG18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07A-3_Schwarz_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/0001LG18",
            "abstract": "Modern web browsers are ubiquitously used by billions of users, connecting them to the world wide web. From the other side, web browsers do not only provide a unified interface for businesses to reach customers, but they also provide a unified interface for malicious actors to reach users. The highly optimized scripting language JavaScript plays an important role in the modern web, as well as for browser-based attacks. These attacks include microarchitectural attacks, which exploit the design of the underlying hardware. In contrast to software bugs, there is often no easy fix for microarchitectural attacks. We propose JavaScript Zero, a highly practical and generic fine-grained permission model in JavaScript to reduce the attack surface in modern browsers. JavaScript Zero facilitates advanced features of the JavaScript language to dynamically deflect usage of dangerous JavaScript features. To implement JavaScript Zero in practice, we overcame a series of challenges to protect potentially dangerous features, guarantee the completeness of our solution, and provide full compatibility with all websites. We demonstrate that our proof-of-concept browser extension Chrome Zero protects against 11 unfixed state-of-the-art microarchitectural and sidechannel attacks. As a side effect, Chrome Zero also protects against 50 % of the published JavaScript 0-day exploits since Chrome 49. Chrome Zero has a performance overhead of 1.82% on average. In a user study, we found that for 24 websites in the Alexa Top 25, users could not distinguish browsers with and without Chrome Zero correctly, showing that Chrome Zero has no perceivable effect on most websites. Hence, JavaScript Zero is a practical solution to mitigate JavaScript-based state-of-the-art microarchitectural and side-channel attacks.",
            "keywords": [
                "JavaScript Security",
                "Microarchitectural Attacks",
                "Side-Channel Attacks",
                "Permission Model",
                "Chrome Zero"
            ]
        },
        "url": "URL#3127737",
        "sema_paperId": "9003812bc541117737e201018c4a6a6225bff33a"
    },
    {
        "@score": "1",
        "@id": "3127738",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "08/1117-1",
                        "text": "Michael Schwarz 0001"
                    },
                    {
                        "@pid": "172/1127",
                        "text": "Moritz Lipp"
                    },
                    {
                        "@pid": "166/1494",
                        "text": "Daniel Gruss"
                    },
                    {
                        "@pid": "145/2706",
                        "text": "Samuel Weiser"
                    },
                    {
                        "@pid": "136/6642",
                        "text": "Cl\u00e9mentine Maurice"
                    },
                    {
                        "@pid": "128/5169",
                        "text": "Raphael Spreitzer"
                    },
                    {
                        "@pid": "91/4831",
                        "text": "Stefan Mangard"
                    }
                ]
            },
            "title": "KeyDrown: Eliminating Software-Based Keystroke Timing Side-Channel Attacks.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/0001LGWMSM18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_04B-1_Schwarz_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/0001LGWMSM18",
            "abstract": "Besides cryptographic secrets, software-based side-channel attacks also leak sensitive user input. The most accurate attacks exploit cache timings or interrupt information to monitor keystroke timings and subsequently infer typed words and sentences. These attacks have also been demonstrated in JavaScript embedded in websites by a remote attacker. We extend the state-of-the-art with a new interrupt-based attack and the first Prime+ Probe attack on kernel interrupt handlers. Previously proposed countermeasures fail to prevent software-based keystroke timing attacks as they do not protect keystroke processing through the entire software stack. We close this gap with KeyDrown, a new defense mechanism against software-based keystroke timing attacks. KeyDrown injects a large number of fake keystrokes in the kernel, making the keystroke interrupt density uniform over time, i.e., independent of the real keystrokes. All keystrokes, including fake keystrokes, are carefully propagated through the shared library to make them indistinguishable by exploiting the specific properties of software-based side channels. We show that attackers cannot distinguish fake keystrokes from real keystrokes anymore and we evaluate KeyDrown on a commodity notebook as well as on Android smartphones. We show that KeyDrown eliminates any advantage an attacker can gain from using software-based side-channel attacks.",
            "keywords": [
                "Keystroke Timing Attacks",
                "Side-Channel Attacks",
                "Interrupt-Based Attacks",
                "KeyDrown Defense Mechanism",
                "Fake Keystrokes Injection"
            ]
        },
        "url": "URL#3127738",
        "sema_paperId": "97962f8fd1554a5db70918a2e5a866fa16de303c"
    },
    {
        "@score": "1",
        "@id": "3127739",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "99/4292-2",
                        "text": "Ding Wang 0002"
                    },
                    {
                        "@pid": "180/8231",
                        "text": "Haibo Cheng"
                    },
                    {
                        "@pid": "37/1304-3",
                        "text": "Ping Wang 0003"
                    },
                    {
                        "@pid": "37/6136",
                        "text": "Jeff Yan"
                    },
                    {
                        "@pid": "82/4944-1",
                        "text": "Xinyi Huang 0001"
                    }
                ]
            },
            "title": "A Security Analysis of Honeywords.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/0002C0YH18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_02B-2_Wang_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/0002C0YH18",
            "abstract": "Honeywords are decoy passwords associated with each user account, and they contribute a promising approach to detecting password leakage. This approach was first proposed by Juels and Rivest at CCS\u201913, and has been covered by hundreds of medias and also adopted in various research domains. The idea of honeywords looks deceptively simple, but it is a deep and sophisticated challenge to automatically generate honeywords that are hard to differentiate from real passwords. In JuelsRivest\u2019s work, four main honeyword-generation methods are suggested but only justified by heuristic security arguments. In this work, we for the first time develop a series of practical experiments using 10 large-scale datasets, a total of 104 million real-world passwords, to quantitatively evaluate the security that these four methods can provide. Our results reveal that they all fail to provide the expected security: real passwords can be distinguished with a success rate of 29.29%\u223c32.62% by our basic trawling-guessing attacker, but not the expected 5%, with just one guess (when each user account is associated with 19 honeywords as recommended). This figure reaches 34.21%\u223c49.02% under the advanced trawling-guessing attackers who make use of various state-of-the-art probabilistic password models. We further evaluate the security of Juels-Rivest\u2019s methods under a targeted-guessing attacker who can exploit the victim\u2019 personal information, and the results are even more alarming: 56.81%\u223c67.98%. Overall, our work resolves three open problems in honeyword research, as defined by Juels and Rivest.",
            "keywords": [
                "Honeywords",
                "Password Leakage",
                "Decoy Passwords",
                "Security Evaluation",
                "Password Guessing Attacks"
            ]
        },
        "url": "URL#3127739",
        "sema_paperId": "de83185c7a2b7924a110d841fcf3694a9a416e74"
    },
    {
        "@score": "1",
        "@id": "3127740",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "138/6356",
                        "text": "Yousra Aafer"
                    },
                    {
                        "@pid": "41/2181-1",
                        "text": "Jianjun Huang 0001"
                    },
                    {
                        "@pid": "65/2709-4",
                        "text": "Yi Sun 0004"
                    },
                    {
                        "@pid": "95/3760-1",
                        "text": "Xiangyu Zhang 0001"
                    },
                    {
                        "@pid": "l/NinghuiLi",
                        "text": "Ninghui Li"
                    },
                    {
                        "@pid": "94/1247-2",
                        "text": "Chen Tian 0002"
                    }
                ]
            },
            "title": "AceDroid: Normalizing Diverse Android Access Control Checks for Inconsistency Detection.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/AaferHS0LT18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_08-1_Aafer_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/AaferHS0LT18",
            "abstract": "\u2014The Android framework has raised increased security concerns with regards to its access control enforcement. Particularly, existing research efforts successfully demonstrate that framework security checks are not always consistent across app-accessible APIs. However, existing efforts fall short in addressing peculiarities that characterize the complex Android access control and the diversity introduced by the heavy vendor customization. In this paper, we develop a new analysis framework AceDroid that models Android access control in a path-sensitive manner and normalizes diverse checks to a canonical form. We applied our proposed modeling to perform inconsistency analysis for 12 images. Our tool proved to be quite effective, enabling to detect a signi\ufb01cant number of inconsistencies introduced by various vendors and to suppress substantial false alarms. Through investigating the results, we uncovered high impact attacks enabling to write a key logger, send premium sms messages, bypass user restrictions, perform a major denial of services and other critical operations.",
            "keywords": [
                "Android Access Control",
                "Inconsistency Detection",
                "Vendor Customization",
                "Path-Sensitive Analysis",
                "Security Vulnerabilities"
            ]
        },
        "url": "URL#3127740",
        "sema_paperId": "10a1fbf79b364e66d0909ed07f98f0abcc0c3307"
    },
    {
        "@score": "1",
        "@id": "3127741",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "205/3202",
                        "text": "Adil Ahmad"
                    },
                    {
                        "@pid": "78/5052",
                        "text": "Kyungtae Kim"
                    },
                    {
                        "@pid": "224/2484",
                        "text": "Muhammad Ihsanulhaq Sarfaraz"
                    },
                    {
                        "@pid": "45/7986",
                        "text": "Byoungyoung Lee"
                    }
                ]
            },
            "title": "OBLIVIATE: A Data Oblivious Filesystem for Intel SGX.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/AhmadKSL18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_06A-2_Ahmad_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/AhmadKSL18",
            "abstract": "Intel SGX provides confidentiality and integrity of a program running within the confines of an enclave, and is expected to enable valuable security applications such as private information retrieval. This paper is concerned with the security aspects of SGX in accessing a key system resource, files. Through concrete attack scenarios, we show that all existing SGX filesystems are vulnerable to either system call snooping, page fault, or cache based side-channel attacks. To address this security limitations in current SGX filesystems, we present OBLIVIATE, a data oblivious filesystem for Intel SGX. The key idea behind OBLIVIATE is in adapting the ORAM protocol to read and write data from a file within an SGX enclave. OBLIVIATE redesigns the conceptual components of ORAM for SGX environments, and it seamlessly supports an SGX program without requiring any changes in the application layer. OBLIVIATE also employs SGX-specific defenses and optimizations in order to ensure complete security with acceptable overhead. The evaluation of the prototype of OBLIVIATE demonstrated its practical effectiveness in running popular server applications such as SQLite and Lighttpd, while also achieving a throughput improvement of 2\u00d78\u00d7 over a baseline ORAM-based solution, and less than 2\u00d7 overhead over an in-memory SGX filesystem.",
            "keywords": [
                "Intel SGX",
                "Data Oblivious Filesystem",
                "ORAM Protocol",
                "Cache Side-Channel Attacks",
                "System Call Snooping"
            ]
        },
        "url": "URL#3127741",
        "sema_paperId": "1e4172ab6fb7568c6235ba05d401dcace32f88a7"
    },
    {
        "@score": "1",
        "@id": "3127742",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "184/4757",
                        "text": "Mustafa Al-Bassam"
                    },
                    {
                        "@pid": "170/4315",
                        "text": "Alberto Sonnino"
                    },
                    {
                        "@pid": "03/10812",
                        "text": "Shehar Bano"
                    },
                    {
                        "@pid": "205/2340",
                        "text": "Dave Hrycyszyn"
                    },
                    {
                        "@pid": "11/1148",
                        "text": "George Danezis"
                    }
                ]
            },
            "title": "Chainspace: A Sharded Smart Contracts Platform.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/Al-BassamSBHD18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_09-2_Al-Bassam_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/Al-BassamSBHD18",
            "abstract": "Chainspace is a decentralized infrastructure, known as a distributed ledger, that supports user defined smart contracts and executes user-supplied transactions on their objects. The correct execution of smart contract transactions is verifiable by all. The system is scalable, by sharding state and the execution of transactions, and using S-BAC, a distributed commit protocol, to guarantee consistency. Chainspace is secure against subsets of nodes trying to compromise its integrity or availability properties through Byzantine Fault Tolerance (BFT), and extremely high-auditability, non-repudiation and `blockchain' techniques. Even when BFT fails, auditing mechanisms are in place to trace malicious participants. We present the design, rationale, and details of Chainspace; we argue through evaluating an implementation of the system about its scaling and other features; we illustrate a number of privacy-friendly smart contracts for smart metering, polling and banking and measure their performance.",
            "keywords": [
                "Decentralized Infrastructure",
                "Smart Contracts",
                "Sharding",
                "Byzantine Fault Tolerance",
                "Privacy-friendly Applications"
            ]
        },
        "url": "URL#3127742"
    },
    {
        "@score": "1",
        "@id": "3127743",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "135/0878",
                        "text": "Athanasios Andreou"
                    },
                    {
                        "@pid": "127/7549",
                        "text": "Giridhari Venkatadri"
                    },
                    {
                        "@pid": "88/10325",
                        "text": "Oana Goga"
                    },
                    {
                        "@pid": "g/PKrishnaGummadi",
                        "text": "Krishna P. Gummadi"
                    },
                    {
                        "@pid": "10/7062",
                        "text": "Patrick Loiseau"
                    },
                    {
                        "@pid": "31/3833",
                        "text": "Alan Mislove"
                    }
                ]
            },
            "title": "Investigating Ad Transparency Mechanisms in Social Media: A Case Study of Facebooks Explanations.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/AndreouVGGLM18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_10-1_Andreou_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/AndreouVGGLM18",
            "abstract": "\u2014Targeted advertising has been subject to many privacy complaints from both users and policy makers. Despite this attention, users still have little understanding of what data the advertising platforms have about them and why they are shown particular ads. To address such concerns, Facebook recently introduced two transparency mechanisms: a \u201cWhy am I seeing this?\u201d button that provides users with an explanation of why they were shown a particular ad (ad explanations), and an Ad Preferences Page that provides users with a list of attributes Facebook has inferred about them and how (data explanations). In this paper, we investigate the level of transparency provided by these two mechanisms. We \ufb01rst de\ufb01ne a number of key properties of explanations and then evaluate empirically whether Facebook\u2019s explanations satisfy them. For our experiments, we develop a browser extension that collects the ads users receive every time they browse Facebook, their respective explanations, and the attributes listed on the Ad Preferences Page; we then use controlled experiments where we create our own ad campaigns and target the users that installed our extension. Our results show that ad explanations are often incomplete and sometimes mis-leading while data explanations are often incomplete and vague . Taken together, our \ufb01ndings have signi\ufb01cant implications for users, policy makers, and regulators as social media advertising services mature.",
            "keywords": [
                "Ad Transparency",
                "Social Media Advertising",
                "User Privacy",
                "Data Explanations",
                "Ad Explanations"
            ]
        },
        "url": "URL#3127743",
        "sema_paperId": "24441a0d34b2b2c149fd485bc84de6fe107414b7"
    },
    {
        "@score": "1",
        "@id": "3127744",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "139/5759",
                        "text": "Zhongjie Ba"
                    },
                    {
                        "@pid": "169/9729",
                        "text": "Sixu Piao"
                    },
                    {
                        "@pid": "49/2189",
                        "text": "Xinwen Fu"
                    },
                    {
                        "@pid": "92/442",
                        "text": "Dimitrios Koutsonikolas"
                    },
                    {
                        "@pid": "70/2832",
                        "text": "Aziz Mohaisen"
                    },
                    {
                        "@pid": "20/6179-1",
                        "text": "Kui Ren 0001"
                    }
                ]
            },
            "title": "ABC: Enabling Smartphone Authentication with Built-in Camera.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/BaPFKM018",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03B-3_Ba_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/BaPFKM018",
            "abstract": "In this paper, we propose ABC, a real-time smartphone Authentication protocol utilizing the photo-response nonuniformity (PRNU) of the Built-in Camera. In contrast to previous works that require tens of images to build reliable PRNU features for conventional cameras, we are the first to observe that one image alone can uniquely identify a smartphone due to the unique PRNU of a smartphone image sensor. This new discovery makes the use of PRNU practical for smartphone authentication. While most existing hardware fingerprints are vulnerable against forgery attacks, ABC defeats forgery attacks by verifying a smartphone\u2019s PRNU identity through a challenge response protocol using a visible light communication channel. A user captures two time-variant QR codes and sends the two images to a server, which verifies the identity by fingerprint and image content matching. The time-variant QR codes can also defeat replay attacks. Our experiments with 16,000 images over 40 smartphones show that ABC can efficiently authenticate user devices with an error rate less than 0.5%.",
            "keywords": [
                "Smartphone Authentication",
                "Photo-Response Nonuniformity (PRNU)",
                "Forgery Attacks",
                "Challenge Response Protocol",
                "Visible Light Communication"
            ]
        },
        "url": "URL#3127744",
        "sema_paperId": "c9bfc4ededd765b9a457fee08acd13dec4b5a683"
    },
    {
        "@score": "1",
        "@id": "3127745",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "160/9772",
                        "text": "Erick Bauman"
                    },
                    {
                        "@pid": "49/4102-1",
                        "text": "Zhiqiang Lin 0001"
                    },
                    {
                        "@pid": "60/1400",
                        "text": "Kevin W. Hamlen"
                    }
                ]
            },
            "title": "Superset Disassembly: Statically Rewriting x86 Binaries Without Heuristics.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/BaumanLH18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05A-4_Bauman_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/BaumanLH18",
            "abstract": "Static binary rewriting is a core technology for many systems and security applications, including profiling, optimization, and software fault isolation. While many static binary rewriters have been developed over the past few decades, most make various assumptions about the binary, such as requiring correct disassembly, cooperation from compilers, or access to debugging symbols or relocation entries. This paper presents MULTIVERSE, a new binary rewriter that is able to rewrite Intel CISC binaries without these assumptions. Two fundamental techniques are developed to achieve this: (1) a superset disassembly that completely disassembles the binary code into a superset of instructions in which all legal instructions fall, and (2) an instruction rewriter that is able to relocate all instructions to any other location by mediating all indirect control flow transfers and redirecting them to the correct new addresses. A prototype implementation of MULTIVERSE and evaluation on SPECint 2006 benchmarks shows that MULTIVERSE is able to rewrite all of the testing binaries with a reasonable runtime overhead for the new rewritten binaries. Simple static instrumentation using MULTIVERSE and its comparison with dynamic instrumentation shows that the approach achieves better average performance. Finally, the security applications of MULTIVERSE are exhibited by using it to implement a shadow stack.",
            "keywords": [
                "Static Binary Rewriting",
                "Binary Disassembly",
                "Instruction Relocation",
                "CISC Architecture",
                "MULTIVERSE"
            ]
        },
        "url": "URL#3127745",
        "sema_paperId": "2d8911f9c275464d0484ae7da317cf9f081495f4"
    },
    {
        "@score": "1",
        "@id": "3127746",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "189/1352",
                        "text": "Daniela Becker"
                    },
                    {
                        "@pid": "51/3023",
                        "text": "Jorge Guajardo"
                    },
                    {
                        "@pid": "05/5338",
                        "text": "Karl-Heinz Zimmermann"
                    }
                ]
            },
            "title": "Revisiting Private Stream Aggregation: Lattice-Based PSA.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/BeckerGZ18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_02B-3_Becker_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/BeckerGZ18",
            "abstract": "In this age of massive data gathering for purposes of personalization, targeted ads, etc. there is an increased need for technology that allows for data analysis in a privacy-preserving manner. Private Stream Aggregation as introduced by Shi et al. (NDSS 2011) allows for the execution of aggregation operations over privacy-critical data from multiple data sources without placing trust in the aggregator and while maintaining differential privacy guarantees. We propose a generic PSA scheme, LaPS, based on the Learning With Error problem, which allows for a flexible choice of the utilized privacy-preserving mechanism while maintaining post-quantum security. We overcome the limitations of earlier schemes by relaxing previous assumptions in the security model and provide an efficient and compact scheme with high scalability. Our scheme is practical, for a plaintext space of 2 and 1000 participants we achieve a performance gain in decryption of roughly 150 times compared to previous results in Shi et al. (NDSS 2011).",
            "keywords": [
                "Private Stream Aggregation",
                "Differential Privacy",
                "Lattice-Based Cryptography",
                "Post-Quantum Security",
                "Scalability"
            ]
        },
        "url": "URL#3127746",
        "sema_paperId": "810eb064e3f914554dfc46d0ab0ded71a62bff73"
    },
    {
        "@score": "1",
        "@id": "3127747",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "43/1791",
                        "text": "Rohit Bhatia"
                    },
                    {
                        "@pid": "150/5240",
                        "text": "Brendan Saltaformaggio"
                    },
                    {
                        "@pid": "132/8319",
                        "text": "Seung Jei Yang"
                    },
                    {
                        "@pid": "214/9435",
                        "text": "Aisha I. Ali-Gombe"
                    },
                    {
                        "@pid": "95/3760-1",
                        "text": "Xiangyu Zhang 0001"
                    },
                    {
                        "@pid": "59/5539",
                        "text": "Dongyan Xu"
                    },
                    {
                        "@pid": "64/775",
                        "text": "Golden G. Richard III"
                    }
                ]
            },
            "title": "Tipped Off by Your Memory Allocator: Device-Wide User Activity Sequencing from Android Memory Images.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/BhatiaSYA0XR18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_04B-4_Bhatia_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/BhatiaSYA0XR18",
            "abstract": "An essential forensic capability is to infer the sequence of actions performed by a suspect in the commission of a crime. Unfortunately, for cyber investigations, user activity timeline reconstruction remains an open research challenge, currently requiring manual identification of datable artifacts/logs and heuristic-based temporal inference. In this paper, we propose a memory forensics capability to address this challenge. We present Timeliner, a forensics technique capable of automatically inferring the timeline of user actions on an Android device across all apps, from a single memory image acquired from the device. Timeliner is inspired by the observation that Android app Activity launches leave behind key self-identifying data structures. More importantly, this collection of data structures can be temporally ordered, owing to the predictable manner in which they were allocated and distributed in memory. Based on these observations, Timeliner is designed to (1) identify and recover these residual data structures, (2) infer the user-induced transitions between their corresponding Activities, and (3) reconstruct the devicewide, cross-app Activity timeline. Timeliner is designed to leverage the memory image of Android\u2019s centralized ActivityManager service. Hence, it is able to sequence Activity launches across all apps \u2014 even those which have terminated. Our evaluation shows that Timeliner can reveal substantial evidence (up to an hour) across a variety of apps on different Android platforms.",
            "keywords": [
                "Memory Forensics",
                "Android Forensics",
                "User Activity Sequencing",
                "Activity Timeline Reconstruction",
                "Timeliner"
            ]
        },
        "url": "URL#3127747",
        "sema_paperId": "209afb1748339b1b7c67a49d69c320a6f5b0681e"
    },
    {
        "@score": "1",
        "@id": "3127748",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "99/3883",
                        "text": "Antonio Bianchi"
                    },
                    {
                        "@pid": "11/10929",
                        "text": "Yanick Fratantonio"
                    },
                    {
                        "@pid": "133/8188",
                        "text": "Aravind Machiry"
                    },
                    {
                        "@pid": "k/ChristopherKruegel",
                        "text": "Christopher Kruegel"
                    },
                    {
                        "@pid": "v/GiovanniVigna",
                        "text": "Giovanni Vigna"
                    },
                    {
                        "@pid": "38/860",
                        "text": "Simon Pak Ho Chung"
                    },
                    {
                        "@pid": "29/5976",
                        "text": "Wenke Lee"
                    }
                ]
            },
            "title": "Broken Fingers: On the Usage of the Fingerprint API in Android.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/BianchiFMKVCL18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03B-1_Bianchi_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/BianchiFMKVCL18",
            "abstract": "Smartphones are increasingly used for very important tasks such as mobile payments. Correspondingly, new technologies are emerging to provide better security on smartphones. One of the most recent and most interesting is the ability to recognize fingerprints, which enables mobile apps to use biometric-based authentication and authorization to protect security-sensitive operations. In this paper, we present the first systematic analysis of the fingerprint API in Android, and we show that this API is not well understood and often misused by app developers. To make things worse, there is currently confusion about which threat model the fingerprint API should be resilient against. For example, although there is no official reference, we argue that the fingerprint API is designed to protect from attackers that can completely compromise the untrusted OS. After introducing several relevant threat models, we identify common API usage patterns and show how inappropriate choices can make apps vulnerable to multiple attacks. We then design and implement a new static analysis tool to automatically analyze the usage of the fingerprint API in Android apps. Using this tool, we perform the first systematic study on how the fingerprint API is used. The results are worrisome: Our tool indicates that 53.69% of the analyzed apps do not use any cryptographic check to ensure that the user actually touched the fingerprint sensor. Depending on the specific use case scenario of a given app, it is not always possible to make use of cryptographic checks. However, a manual investigation on a subset of these apps revealed that 80% of them could have done so, preventing multiple attacks. Furthermore, the tool indicates that only the 1.80% of the analyzed apps use this API in the most secure way possible, while many others, including extremely popular apps such as Google Play Store and Square Cash, use it in weaker ways. To make things worse, we find issues and inconsistencies even in the samples provided by the official Google documentation. We end this work by suggesting various improvements to the fingerprint API to prevent some of these problematic attacks.",
            "keywords": [
                "Fingerprint API",
                "Android Security",
                "Biometric Authentication",
                "API Misuse",
                "Cryptographic Checks"
            ]
        },
        "url": "URL#3127748",
        "sema_paperId": "87ded2b4978e2ccfebaf99d3a8d7f9396f4aa756"
    },
    {
        "@score": "1",
        "@id": "3127749",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "223/9913",
                        "text": "Andrea Biondo"
                    },
                    {
                        "@pid": "82/4386",
                        "text": "Mauro Conti"
                    },
                    {
                        "@pid": "172/6530",
                        "text": "Daniele Lain"
                    }
                ]
            },
            "title": "Back To The Epilogue: Evading Control Flow Guard via Unaligned Targets.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/BiondoCL18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05A-3_Biondo_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/BiondoCL18",
            "abstract": "Attackers use memory corruption vulnerabilities to compromise systems by hijacking control flow towards attacker-controlled code. Over time, researchers proposed several countermeasures, such as Address Space Layout Randomization, Write XOR Execute and Control Flow Integrity (CFI). CFI is one of the most promising solutions, enforcing control flow to adhere to statically determined valid execution paths. To trade with the execution and storage overhead, practical CFI implementations enforce coarser version of CFI. One of the most widely deployed implementations of CFI is the one proposed by Microsoft, named Control Flow Guard (CFG). CFG is currently in place on all Windows operating systems, from Windows 8.1 to the most recent update of Windows 10 (at the time of writing), accounting for more than 500 million machines. In this paper, we show a significant design vulnerability in Windows CFG and propose a specific attack to exploit it: the Back to The Epilogue (BATE) attack. We show that with BATE an attacker can completely evade from CFG and transfer control to any location, thus obtaining arbitrary code execution. BATE leverages the tradeoff of CFG between precision, performance, and backwards compatibility; in particular, the latter one motivates 16-byte address granularity in some circumstances. This vulnerability, inherent to the CFG design, allows us to call portions of code (gadgets) that should not be allowed, and that we can chain together to escape CFG. These gadgets are very common: we ran a thorough evaluation of Windows system libraries, and found many high value targets \u2013 exploitable gadgets in code loaded by almost all the applications on 32-bit systems and by web browsers on 64-bit. We also demonstrate the real-world feasibility of our attack by using it to build a remote code execution exploit against the Microsoft Edge web browser running on 64-bit Windows 10. Finally, we discuss possible countermeasures to BATE.",
            "keywords": [
                "Control Flow Integrity",
                "Memory Corruption",
                "Control Flow Guard",
                "BATE Attack",
                "Arbitrary Code Execution"
            ]
        },
        "url": "URL#3127749",
        "sema_paperId": "449af184acc3cc190ca900e3699d69e7f64fb8c9"
    },
    {
        "@score": "1",
        "@id": "3127750",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "136/8461",
                        "text": "Kevin Borgolte"
                    },
                    {
                        "@pid": "150/5174",
                        "text": "Tobias Fiebig"
                    },
                    {
                        "@pid": "07/6713-1",
                        "text": "Shuang Hao 0001"
                    },
                    {
                        "@pid": "k/ChristopherKruegel",
                        "text": "Christopher Kruegel"
                    },
                    {
                        "@pid": "v/GiovanniVigna",
                        "text": "Giovanni Vigna"
                    }
                ]
            },
            "title": "Cloud Strife: Mitigating the Security Risks of Domain-Validated Certificates.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/BorgolteFHKV18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_06A-4_Borgolte_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/BorgolteFHKV18",
            "abstract": "Infrastructure-as-a-Service (IaaS), more generally the \"cloud,\" changed the landscape of system operations on the Internet. Clouds' elasticity allow operators to rapidly allocate and use resources as needed, from virtual machines, to storage, to IP addresses, which is what made clouds popular. We show that the dynamic component paired with developments in trust-based ecosystems (e.g., TLS certificates) creates so far unknown attacks. We demonstrate that it is practical to allocate IP addresses to which stale DNS records point. Considering the ubiquity of domain validation in trust ecosystems, like TLS, an attacker can then obtain a valid and trusted certificate. The attacker can then impersonate the service, exploit residual trust for phishing, or might even distribute malicious code. Even worse, an aggressive attacker could succeed in less than 70 seconds, well below common time-to-live (TTL) for DNS. In turn, she could exploit normal service migrations to obtain a valid certificate, and, worse, she might not be bound by DNS records being (temporarily) stale. We introduce a new authentication method for trust-based domain validation, like IETF's automated certificate management environment (ACME), that mitigates staleness issues without incurring additional certificate requester effort by incorporating the existing trust of a name into the validation process. Based on previously published work [1]. [1] Kevin Borgolte, Tobias Fiebig, Shuang Hao, Christopher Kruegel, Giovanni Vigna. February 2018. Cloud Strife: Mitigating the Security Risks of Domain-Validated Certificates. In Proceedings of the 25th Network and Distributed Systems Security Symposium (NDSS '18). Internet Society (ISOC). DOI: 10.14722/ndss.2018.23327. URL: https://doi.org/10.14722/nd",
            "keywords": [
                "Cloud Security",
                "Domain-Validated Certificates",
                "DNS Staleness",
                "Certificate Validation",
                "Impersonation Attacks"
            ]
        },
        "url": "URL#3127750",
        "sema_paperId": "5b6491e50924b78cb44e0836a9fe0087c30ec976"
    },
    {
        "@score": "1",
        "@id": "3127751",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "157/1187",
                        "text": "Nathan Burow"
                    },
                    {
                        "@pid": "210/2383",
                        "text": "Derrick Paul McKee"
                    },
                    {
                        "@pid": "176/5333",
                        "text": "Scott A. Carr"
                    },
                    {
                        "@pid": "31/1273",
                        "text": "Mathias Payer"
                    }
                ]
            },
            "title": "CFIXX: Object Type Integrity for C++.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/BurowMCP18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05A-2_Burow_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/BurowMCP18",
            "abstract": "\u2014C++ relies on object type information for dynamic dispatch and casting. The association of type information to an object is implemented via the virtual table pointer, which is stored in the object itself. As C++ has neither memory nor type safety, adversaries may therefore overwrite an object\u2019s type. If the corrupted type is used for dynamic dispatch, the attacker has hijacked the application\u2019s control \ufb02ow. This vulnerability is widespread and commonly exploited. Firefox, Chrome, and other major C++ applications are network facing, commonly attacked, and make signi\ufb01cant use of dynamic dispatch. Control-Flow Integrity (CFI) is the state of the art policy for ef\ufb01cient mitigation of control-\ufb02ow hijacking attacks. CFI mechanisms determine statically (i.e., at compile time) the set of functions that are valid at a given call site, based on C++ semantics. We propose an orthogonal policy, Object Type Integrity (OTI), that dynamically tracks object types. Consequently, instead of allowing a set of targets for each dynamic dispatch on an object, only the single, correct target for the object\u2019s type is allowed. To show the ef\ufb01cacy of OTI, we present CFIXX, which enforces OTI. CFIXX enforces OTI by dynamically tracking the type of each object and enforcing its integrity against arbitrary writes. CFIXX has minimal overhead on CPU bound applications such as SPEC CPU2006 \u2014 4.98%. On key applications like Chromium, CFIXX has negligible overhead on JavaScript benchmarks: 2.03% on Octane, 1.99% on Kraken, and 2.80% on JetStream. We show that CFIXX can be deployed in conjunction with CFI, providing a signi\ufb01cant security improvement.",
            "keywords": [
                "C++ Security",
                "Control-Flow Integrity",
                "Object Type Integrity",
                "Dynamic Dispatch",
                "Type Information Integrity"
            ]
        },
        "url": "URL#3127751",
        "sema_paperId": "c0a22ed35dbd4f079007067b11c5cda473be04ce"
    },
    {
        "@score": "1",
        "@id": "3127752",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "116/4680",
                        "text": "Aylin Caliskan"
                    },
                    {
                        "@pid": "66/11094",
                        "text": "Fabian Yamaguchi"
                    },
                    {
                        "@pid": "153/5813",
                        "text": "Edwin Dauber"
                    },
                    {
                        "@pid": "119/2618",
                        "text": "Richard E. Harang"
                    },
                    {
                        "@pid": "64/4020",
                        "text": "Konrad Rieck"
                    },
                    {
                        "@pid": "93/655",
                        "text": "Rachel Greenstadt"
                    },
                    {
                        "@pid": "08/3080",
                        "text": "Arvind Narayanan"
                    }
                ]
            },
            "title": "When Coding Style Survives Compilation: De-anonymizing Programmers from Executable Binaries.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/CaliskanYDHRGN18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_06B-2_Caliskan_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/CaliskanYDHRGN18",
            "abstract": "The ability to identify authors of computer programs based on their coding style is a direct threat to the privacy and anonymity of programmers. While recent work found that source code can be attributed to authors with high accuracy, attribution of executable binaries appears to be much more difficult. Many distinguishing features present in source code, e.g. variable names, are removed in the compilation process, and compiler optimization may alter the structure of a program, further obscuring features that are known to be useful in determining authorship. We examine programmer de-anonymization from the standpoint of machine learning, using a novel set of features that include ones obtained by decompiling the executable binary to source code. We adapt a powerful set of techniques from the domain of source code authorship attribution along with stylistic representations embedded in assembly, resulting in successful de-anonymization of a large set of programmers. We evaluate our approach on data from the Google Code Jam, obtaining attribution accuracy of up to 96% with 100 and 83% with 600 candidate programmers. We present an executable binary authorship attribution approach, for the first time, that is robust to basic obfuscations, a range of compiler optimization settings, and binaries that have been stripped of their symbol tables. We perform programmer de-anonymization using both obfuscated binaries, and real-world code found 'in the wild' in single-author GitHub repositories and the recently leaked this http URL hacker forum. We show that programmers who would like to remain anonymous need to take extreme countermeasures to protect their privacy.",
            "keywords": [
                "Executable Binary Attribution",
                "Programmer De-anonymization",
                "Coding Style Analysis",
                "Compiler Optimization Effects",
                "Obfuscation Resistance"
            ]
        },
        "url": "URL#3127752"
    },
    {
        "@score": "1",
        "@id": "3127753",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "224/2480",
                        "text": "Jiongyi Chen"
                    },
                    {
                        "@pid": "149/2350",
                        "text": "Wenrui Diao"
                    },
                    {
                        "@pid": "156/1033",
                        "text": "Qingchuan Zhao"
                    },
                    {
                        "@pid": "149/8302",
                        "text": "Chaoshun Zuo"
                    },
                    {
                        "@pid": "49/4102-1",
                        "text": "Zhiqiang Lin 0001"
                    },
                    {
                        "@pid": "06/6268",
                        "text": "XiaoFeng Wang 0001"
                    },
                    {
                        "@pid": "l/WingCheongLau",
                        "text": "Wing Cheong Lau"
                    },
                    {
                        "@pid": "192/8009",
                        "text": "Menghan Sun"
                    },
                    {
                        "@pid": "130/6977",
                        "text": "Ronghai Yang"
                    },
                    {
                        "@pid": "66/6560",
                        "text": "Kehuan Zhang"
                    }
                ]
            },
            "title": "IoTFuzzer: Discovering Memory Corruptions in IoT Through App-based Fuzzing.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/ChenDZZL0LSYZ18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01A-1_Chen_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/ChenDZZL0LSYZ18",
            "abstract": "\u2014With more IoT devices entering the consumer market, it becomes imperative to detect their security vulnerabilities before an attacker does. Existing binary analysis based approaches only work on \ufb01rmware, which is less accessible except for those equipped with special tools for extracting the code from the device. To address this challenge in IoT security analysis, we present in this paper a novel automatic fuzzing framework, called I O TF UZZER , which aims at \ufb01nding memory corruption vulnerabilities in IoT devices without access to their \ufb01rmware images. The key idea is based upon the observation that most IoT devices are controlled through their of\ufb01cial mobile apps, and such an app often contains rich information about the protocol it uses to communicate with its device. Therefore, by identifying and reusing program-speci\ufb01c logic (e.g., encryption) to mutate the test case (particularly message \ufb01elds), we are able to effectively probe IoT targets without relying on any knowledge about its protocol speci\ufb01cations. In our research, we implemented I O TF UZZER and evaluated 17 real-world IoT devices running on different protocols, and our approach successfully identi\ufb01ed 15 memory corruption vulnerabilities (including 8 previously unknown ones).",
            "keywords": [
                "IoT Security",
                "Fuzzing Framework",
                "Memory Corruption",
                "Mobile App Vulnerabilities",
                "Protocol Analysis"
            ]
        },
        "url": "URL#3127753",
        "sema_paperId": "778338a5d75dd82f45a7c4877222525ec670308b"
    },
    {
        "@score": "1",
        "@id": "3127754",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "89/8201",
                        "text": "Yaohui Chen"
                    },
                    {
                        "@pid": "48/1646",
                        "text": "Yuping Li"
                    },
                    {
                        "@pid": "81/7428",
                        "text": "Long Lu"
                    },
                    {
                        "@pid": "224/2477",
                        "text": "Yueh-Hsun Lin"
                    },
                    {
                        "@pid": "24/687",
                        "text": "Hayawardh Vijayakumar"
                    },
                    {
                        "@pid": "95/6543-4",
                        "text": "Zhi Wang 0004"
                    },
                    {
                        "@pid": "41/4686",
                        "text": "Xinming Ou"
                    }
                ]
            },
            "title": "InstaGuard: Instantly Deployable Hot-patches for Vulnerable System Programs on Android.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/ChenLLLVWO18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/03/ndss2018_08-2_Chen_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/ChenLLLVWO18",
            "abstract": "\u2014Hot-patches, easier to develop and faster to deploy than permanent patches, are used to timely (and temporarily) block exploits of newly discovered vulnerabilities while perma- nent patches are being developed and tested. Researchers recently proposed to apply hot-patching techniques to system programs on Android as a quick mitigation against critical vulnerabilities. However, existing hot-patching techniques, though widely used in conventional computers, are rarely adopted by Android OS or device vendors in reality. Our study uncovers a major hurdle that prevents existing hot-patching methods from being effective on mobile devices: after being developed, hot-patches for mobile devices have to go through lengthy compatibility tests that Android device partners impose on all system code updates. This testing and release process can take months, and therefore, erase the key bene\ufb01t of hot-patches (i.e., quickly deployable). We propose InstaGuard, a new approach to hot-patch for mobile devices that allows for instant deployment of patches (i.e., \u201ccarrier-passthrough\u201d) and fast patch development for device vendors. Unlike existing hot-patching techniques, InstaGuard avoids injecting new code to programs being patched. Instead, it enforces instantly updatable rules that contain no code (i.e., no carrier test is needed) to block exploits of unpatched vulnerabili- ties in a timely fashion. When designing InstaGuard, we overcame two major challenges that previous hot-patching methods did not face. First, since no code addition is allowed, InstaGuard needs a rule language that is expressive enough to mitigate various kinds of vulnerabilities and ef\ufb01cient to be enforced on mobile devices. Second, rule generation cannot require special skills or much efforts from human users. We designed a new language for hot-patches and an enforcement mechanism based on the basic debugging primitives supported by ARM CPUs. We also built RuleMaker, a tool for automatically generating rules for InstaGuard based on high-level, easy-to-write vulnerability descriptions. We have implemented InstaGuard on Google Nexus 5X phones. To demonstrate the coverage of InstaGuard, we show that InstaGuard can handle all critical CVEs from Android Security Bulletins reported in 2016. We also conduct unit tests using critical vulnerabilities from 4 different categories. On average, InstaGuard increases program memory footprint by 1.69% and slows down program execution by 2.70%, which are unnoticeable to device users in practice.",
            "keywords": [
                "Hot-patching",
                "Android Security",
                "Vulnerability Mitigation",
                "Instant Deployment",
                "InstaGuard"
            ]
        },
        "url": "URL#3127754",
        "sema_paperId": "c7c3155acfa4ca362422e15fcb2ba00f71cb96ab"
    },
    {
        "@score": "1",
        "@id": "3127755",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "150/3272",
                        "text": "Qi Alfred Chen"
                    },
                    {
                        "@pid": "224/2470",
                        "text": "Yucheng Yin"
                    },
                    {
                        "@pid": "214/8299",
                        "text": "Yiheng Feng"
                    },
                    {
                        "@pid": "91/584",
                        "text": "Z. Morley Mao"
                    },
                    {
                        "@pid": "45/2939",
                        "text": "Henry X. Liu"
                    }
                ]
            },
            "title": "Exposing Congestion Attack on Emerging Connected Vehicle based Traffic Signal Control.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/ChenYFML18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01B-2_Chen_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/ChenYFML18",
            "abstract": "\u2014Connected vehicle (CV",
            "keywords": [
                "Connected Vehicles",
                "Traffic Signal Control",
                "Congestion Attack",
                "Network Vulnerabilities",
                "Vehicle Communication"
            ]
        },
        "url": "URL#3127755",
        "sema_paperId": "20b3a2eead097d98a11b0024ff2f30873c2bb8fc"
    },
    {
        "@score": "1",
        "@id": "3127756",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "205/3579",
                        "text": "Omer Deutsch"
                    },
                    {
                        "@pid": "224/2315",
                        "text": "Neta Rozen Schiff"
                    },
                    {
                        "@pid": "d/DannyDolev",
                        "text": "Danny Dolev"
                    },
                    {
                        "@pid": "15/5634",
                        "text": "Michael Schapira"
                    }
                ]
            },
            "title": "Preventing (Network) Time Travel with Chronos.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/DeutschSDS18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_02A-2_Deutsch_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/DeutschSDS18",
            "abstract": "The Network Time Protocol (NTP) synchronizes time across computer systems over the Internet. Unfortunately, NTP is highly vulnerable to \"time shifting attacks\", in which the attacker's goal is to shift forward/backward the local time at an NTP client. This has severe implications for the correctness and safety of time-sensitive applications and for security mechanisms. Importantly, time shifting attacks on NTP are possible even if all NTP communications are encrypted and authenticated. We present Chronos, a new NTP client that achieves good synchronization even in the presence of powerful man-in-the-middle attackers. Chronos is backwards compatible with legacy NTP and involves no changes whatsoever to NTP servers. In addition, Chronos is carefully engineered to minimize communication overhead so as to avoid overloading NTP servers. We evaluate Chronos' security and network efficiency guarantees via a combination of theoretical analyses and experiments with a prototype implementation. Our results indicate that to succeed in shifting time at a Chronos client by over 100ms from the UTC, even a powerful man-in-the-middle attacker requires over 20 years of effort in expectation. Based on work published at [1].",
            "keywords": [
                "Network Time Protocol",
                "Time Synchronization",
                "Time Shifting Attacks",
                "Man-in-the-Middle Attacks",
                "Chronos Client"
            ]
        },
        "url": "URL#3127756",
        "sema_paperId": "d1be607eba8fd09a843ecc4b96fe5a9eeba57aed"
    },
    {
        "@score": "1",
        "@id": "3127757",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "10/9994",
                        "text": "Yue Duan"
                    },
                    {
                        "@pid": "28/3341-1",
                        "text": "Mu Zhang 0001"
                    },
                    {
                        "@pid": "172/8802",
                        "text": "Abhishek Vasisht Bhaskar"
                    },
                    {
                        "@pid": "77/6178-1",
                        "text": "Heng Yin 0001"
                    },
                    {
                        "@pid": "77/11207",
                        "text": "Xiaorui Pan"
                    },
                    {
                        "@pid": "140/7353",
                        "text": "Tongxin Li"
                    },
                    {
                        "@pid": "60/8494",
                        "text": "Xueqiang Wang"
                    },
                    {
                        "@pid": "06/6268",
                        "text": "XiaoFeng Wang 0001"
                    }
                ]
            },
            "title": "Things You May Not Know About Android (Un)Packers: A Systematic Study based on Whole-System Emulation.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/DuanZBYPLWW18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_04A-4_Duan_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/DuanZBYPLWW18",
            "abstract": "The prevalent usage of runtime packers has complicated Android malware analysis, as both legitimate and malicious apps are leveraging packing mechanisms to protect themselves against reverse engineer. Although recent efforts have been made to analyze particular packing techniques, little has been done to study the unique characteristics of Android packers. In this paper, we report the first systematic study on mainstream Android packers, in an attempt to understand their security implications. For this purpose, we developed DROIDUNPACK, a whole-system emulation based Android packing analysis framework, which compared with existing tools, relies on intrinsic characteristics of Android runtime (rather than heuristics), and further enables virtual machine inspection to precisely recover hidden code and reveal packing behaviors. Running our tool on 6 major commercial packers, 93,910 Android malware samples and 3 existing state-of-the-art unpackers, we found that not only are commercial packing services abused to encrypt malicious or plagiarized contents, they themselves also introduce securitycritical vulnerabilities to the apps being packed. Our study further reveals the prevalence and rapid evolution of custom packers used by malware authors, which cannot be defended against using existing techniques, due to their design weaknesses.",
            "keywords": [
                "Android Malware Analysis",
                "Runtime Packers",
                "Whole-System Emulation",
                "DROIDUNPACK Framework",
                "Custom Packing Techniques"
            ]
        },
        "url": "URL#3127757",
        "sema_paperId": "b872fd4c57e14458e68dfb81ce25c4afe15176af"
    },
    {
        "@score": "1",
        "@id": "3127758",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "16/10817",
                        "text": "Earlence Fernandes"
                    },
                    {
                        "@pid": "125/0358",
                        "text": "Amir Rahmati"
                    },
                    {
                        "@pid": "j/JaeyeonJung",
                        "text": "Jaeyeon Jung"
                    },
                    {
                        "@pid": "p/AtulPrakash",
                        "text": "Atul Prakash 0001"
                    }
                ]
            },
            "title": "Decentralized Action Integrity for Trigger-Action IoT Platforms.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/FernandesRJP18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01A-3_Fernandes_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/FernandesRJP18",
            "abstract": "\u2014Trigger-Action platforms are web-based systems that enable users to create automation rules by stitching together online services representing digital and physical resources using OAuth tokens. Unfortunately, these platforms introduce a long-range large-scale security risk: If they are compromised, an attacker can misuse the OAuth tokens belonging to a large number of users to arbitrarily manipulate their devices and data. We introduce Decentralized Action Integrity , a security principle that prevents an untrusted trigger-action platform from misusing compromised OAuth tokens in ways that are inconsistent with any given user\u2019s set of trigger-action rules. We present the design and evaluation of Decentralized Trigger-Action Platform (DTAP), a trigger-action platform that implements this principle by over-coming practical challenges. DTAP splits currently monolithic platform designs into an untrusted cloud service, and a set of user clients (each user only trusts their client). Our design introduces the concept of Transfer Tokens (XTokens) to practically use \ufb01ne-grained rule-speci\ufb01c tokens without increasing the number of OAuth permission prompts compared to current platforms. Our evaluation indicates that DTAP poses negligible overhead: it adds less than 15 ms of latency to rule execution time, and reduces throughput by 2 . 5% .",
            "keywords": [
                "IoT Automation",
                "Trigger-Action Platforms",
                "OAuth Token Security",
                "Decentralized Systems",
                "Transfer Tokens (XTokens)"
            ]
        },
        "url": "URL#3127758",
        "sema_paperId": "4ce90f1734b12aa1cb7d8256c2368476cd301a0b"
    },
    {
        "@score": "1",
        "@id": "3127759",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "f/RiccardoFocardi",
                        "text": "Riccardo Focardi"
                    },
                    {
                        "@pid": "185/2395",
                        "text": "Francesco Palmarini"
                    },
                    {
                        "@pid": "117/7980",
                        "text": "Marco Squarcina"
                    },
                    {
                        "@pid": "12/612",
                        "text": "Graham Steel"
                    },
                    {
                        "@pid": "154/7915",
                        "text": "Mauro Tempesta"
                    }
                ]
            },
            "title": "Mind Your Keys? A Security Evaluation of Java Keystores.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/FocardiPSST18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_02B-1_Focardi_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/FocardiPSST18",
            "abstract": "Cryptography is complex and variegate and requires to combine different algorithms and mechanisms in nontrivial ways. This complexity is often source of vulnerabilities. Secure key management is one of the most critical aspects, since leaking a cryptographic key vanishes any advantage of using cryptography. In this paper we analyze Java keystores, the standard way to manage and securely store keys in Java applications. We consider seven keystore implementations from Oracle JDK and Bouncy Castle, a widespread cryptographic library. We describe, in detail, how the various keystores enforce confidentiality and integrity of the stored keys through passwordbased cryptography and we show that many of the implementations do not adhere to state-of-the-art cryptographic standards. We investigate the resistance to offline attacks and we show that, for non-compliant keystores, brute-forcing can be up to three orders of magnitude faster with respect to the most compliant keystore. Additionally, when an attacker can tamper with the keystore file, some implementations are vulnerable to denial of service attacks or, in the worst case, arbitrary code execution. Finally we discuss the fixes implemented by Oracle and Bouncy Castle developers following our responsible disclosure.",
            "keywords": [
                "Java Keystores",
                "Cryptographic Key Management",
                "Confidentiality and Integrity",
                "Offline Attacks",
                "Denial of Service Vulnerabilities"
            ]
        },
        "url": "URL#3127759",
        "sema_paperId": "397d0ae673e2fcbc583e72531be1135a9a9f62e8"
    },
    {
        "@score": "1",
        "@id": "3127760",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "87/4866-1",
                        "text": "Xing Gao 0001"
                    },
                    {
                        "@pid": "61/7820",
                        "text": "Zhang Xu"
                    },
                    {
                        "@pid": "81/4036-1",
                        "text": "Haining Wang 0001"
                    },
                    {
                        "@pid": "53/2189-64",
                        "text": "Li Li 0064"
                    },
                    {
                        "@pid": "18/5637",
                        "text": "Xiaorui Wang"
                    }
                ]
            },
            "title": "Reduced Cooling Redundancy: A New Security Vulnerability in a Hot Data Center.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/GaoXWLW18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_06A-1_Gao_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/GaoXWLW18",
            "abstract": "Data centers have been growing rapidly in recent years to meet the surging demand of cloud services. However, the expanding scale and powerful servers generate a great amount of heat, resulting in significant cooling costs. A trend in modern data centers is to raise the temperature and maintain all servers in a relatively hot environment. While this can save on cooling costs given benign workloads running in servers, the hot environment increases the risk of cooling failure. In this paper, we unveil a new vulnerability of existing data centers with aggressive cooling energy saving policies. Such a vulnerability might be exploited to launch thermal attacks that could severely worsen the thermal conditions in a data center. Specifically, we conduct thermal measurements and uncover effective thermal attack vectors at the server, rack, and data center levels. We also present damage assessments of thermal attacks. Our results demonstrate that thermal attacks can (1) largely increase the temperature of victim servers degrading their performance and reliability, (2) negatively impact on thermal conditions of neighboring servers causing local hotspots, (3) raise the cooling cost, and (4) even lead to cooling failures. Finally, we propose effective defenses to prevent thermal attacks from becoming a serious security threat to data centers.",
            "keywords": [
                "Data Center Security",
                "Thermal Attacks",
                "Cooling Redundancy",
                "Thermal Vulnerability",
                "Cooling Failure Risks"
            ]
        },
        "url": "URL#3127760",
        "sema_paperId": "a19ec45705c707650b154f012bebf41716b6ad83"
    },
    {
        "@score": "1",
        "@id": "3127761",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "164/2771",
                        "text": "David Gens"
                    },
                    {
                        "@pid": "61/3969",
                        "text": "Simon Schmitt"
                    },
                    {
                        "@pid": "73/7564",
                        "text": "Lucas Davi"
                    },
                    {
                        "@pid": "s/AhmadRezaSadeghi",
                        "text": "Ahmad-Reza Sadeghi"
                    }
                ]
            },
            "title": "K-Miner: Uncovering Memory Corruption in Linux.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/GensSDS18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05A-1_Gens_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/GensSDS18",
            "abstract": "Operating system kernels are appealing attack targets: compromising the kernel usually allows attackers to bypass all deployed security mechanisms and take control over the entire system. Commodity kernels, like Linux, are written in low-level programming languages that offer only limited type and memory-safety guarantees, enabling adversaries to launch sophisticated run-time attacks against the kernel by exploiting memory-corruption vulnerabilities. Many defenses have been proposed to protect operating systems at run time, such as control-flow integrity (CFI). However, the goal of these run-time monitors is to prevent exploitation as a symptom of memory corruption, rather than eliminating the underlying root cause, i.e., bugs in the kernel code. While finding bugs can be automated, e.g., using static analysis, all existing approaches are limited to local, intra-procedural checks, and face severe scalability challenges due to the large kernel code base. Consequently, there currently exist no tools for conducting global static analysis of operating system kernels. In this paper, we present K-Miner, a new framework to efficiently analyze large, commodity operating system kernels like Linux. Our novel approach exploits the highly standardized interface structure of the kernel code to enable scalable pointer analysis and conduct global, context-sensitive analysis. Through our inter-procedural analysis we show that K-Miner systematically and reliably uncovers several different classes of memory-corruption vulnerabilities, such as dangling pointers, user-after-free, double-free, and double-lock vulnerabilities. We thoroughly evaluate our extensible analysis framework, which leverages the popular and widely used LLVM compiler suite, for the current Linux kernel and demonstrate its effectiveness by reporting several memory-corruption vulnerabilities.",
            "keywords": [
                "Operating System Kernel",
                "Memory Corruption",
                "Static Analysis",
                "Pointer Analysis",
                "Linux Vulnerabilities"
            ]
        },
        "url": "URL#3127761",
        "sema_paperId": "da32aa29334ab197cd76fac5f9bd3c7c0acb3811"
    },
    {
        "@score": "1",
        "@id": "3127762",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "57/10300",
                        "text": "Wookhyun Han"
                    },
                    {
                        "@pid": "183/1504",
                        "text": "Byunggill Joe"
                    },
                    {
                        "@pid": "45/7986",
                        "text": "Byoungyoung Lee"
                    },
                    {
                        "@pid": "69/6818",
                        "text": "Chengyu Song"
                    },
                    {
                        "@pid": "45/4154",
                        "text": "Insik Shin"
                    }
                ]
            },
            "title": "Enhancing Memory Error Detection for Large-Scale Applications and Fuzz Testing.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/HanJLSS18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05A-5_Han_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/HanJLSS18",
            "abstract": "Memory errors are one of the most common vulnerabilities for the popularity of memory unsafe languages including C and C++. Once exploited, it can easily lead to system crash (i.e., denial-of-service attacks) or allow adversaries to fully compromise the victim system. This paper proposes MEDS, a practical memory error detector. MEDS significantly enhances its detection capability by approximating two ideal properties, called an infinite gap and an infinite heap. The approximated infinite gap of MEDS setups large inaccessible memory region between objects (i.e., 4 MB), and the approximated infinite heap allows MEDS to fully utilize virtual address space (i.e., 45-bits memory space). The key idea of MEDS in achieving these properties is a novel user-space memory allocation mechanism, MEDSALLOC. MEDSALLOC leverages a page aliasing mechanism, which allows MEDS to maximize the virtual memory space utilization but minimize the physical memory uses. To highlight the detection capability and practical impacts of MEDS, we evaluated and then compared to Google\u2019s state-of-the-art detection tool, AddressSanitizer. MEDS showed three times better detection rates on four real-world vulnerabilities in Chrome and Firefox. More importantly, when used for a fuzz testing, MEDS was able to identify 68.3% more memory errors than AddressSanitizer for the same amount of a testing time, highlighting its practical aspects in the software testing area. In terms of performance overhead, MEDS slowed down 108% and 86% compared to native execution and AddressSanitizer, respectively, on real-world applications including Chrome, Firefox, Apache, Nginx, and OpenSSL.",
            "keywords": [
                "Memory Error Detection",
                "Fuzz Testing",
                "Memory Safety",
                "Memory Management",
                "MEDS"
            ]
        },
        "url": "URL#3127762",
        "sema_paperId": "6658f08b743f31bded4c71717d37cdc6f2e9687f"
    },
    {
        "@score": "1",
        "@id": "3127763",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "184/8086",
                        "text": "Wajih Ul Hassan"
                    },
                    {
                        "@pid": "205/2270",
                        "text": "Mark Lemay"
                    },
                    {
                        "@pid": "224/2485",
                        "text": "Nuraini Aguse"
                    },
                    {
                        "@pid": "126/3601",
                        "text": "Adam Bates 0001"
                    },
                    {
                        "@pid": "13/4686",
                        "text": "Thomas Moyer"
                    }
                ]
            },
            "title": "Towards Scalable Cluster Auditing through Grammatical Inference over Provenance Graphs.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/HassanLABM18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07B-1_Hassan_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/HassanLABM18",
            "abstract": "Investigating the nature of system intrusions in large distributed systems remains a notoriously difficult challenge. While monitoring tools (e.g., Firewalls, IDS) provide preliminary alerts through easy-to-use administrative interfaces, attack reconstruction still requires that administrators sift through gigabytes of system audit logs stored locally on hundreds of machines. At present, two fundamental obstacles prevent synergy between system-layer auditing and modern cluster monitoring tools: 1) the sheer volume of audit data generated in a data center is prohibitively costly to transmit to a central node, and 2) systemlayer auditing poses a \u201cneedle-in-a-haystack\u201d problem, such that hundreds of employee hours may be required to diagnose a single intrusion. This paper presents Winnower, a scalable system for auditbased cluster monitoring that addresses these challenges. Our key insight is that, for tasks that are replicated across nodes in a distributed application, a model can be defined over audit logs to succinctly summarize the behavior of many nodes, thus eliminating the need to transmit redundant audit records to a central monitoring node. Specifically, Winnower parses audit records into provenance graphs that describe the actions of individual nodes, then performs grammatical inference over individual graphs using a novel adaptation of Deterministic Finite Automata (DFA) Learning to produce a behavioral model of many nodes at once. This provenance model can be efficiently transmitted to a central node and used to identify anomalous events in the cluster. We implement Winnower for Docker Swarm container clusters and evaluate our system against real-world applications and attacks. We show that Winnower dramatically reduces storage and network overhead associated with aggregating system audit logs, by as much as 98%, without sacrificing the important information needed for attack investigation. Winnower thus represents a significant step forward for security monitoring in distributed systems.",
            "keywords": [
                "Cluster Monitoring",
                "Audit Logs",
                "Provenance Graphs",
                "Anomaly Detection",
                "Scalable Security Auditing"
            ]
        },
        "url": "URL#3127763",
        "sema_paperId": "348191516284b7ed3c21a4172976576c31c1d5cc"
    },
    {
        "@score": "1",
        "@id": "3127764",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "183/1751",
                        "text": "Byeongdo Hong"
                    },
                    {
                        "@pid": "83/7954",
                        "text": "Sangwook Bae"
                    },
                    {
                        "@pid": "20/6892",
                        "text": "Yongdae Kim"
                    }
                ]
            },
            "title": "GUTI Reallocation Demystified: Cellular Location Tracking with Changing Temporary Identifier.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/HongBK18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_02A-4_Hong_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/HongBK18",
            "abstract": "To keep subscribers\u2019 identity confidential, a cellular network operator must use a temporary identifier instead of a permanent one according to the 3GPP standard. Temporary identifiers include Temporary Mobile Subscriber Identity (TMSI) and Globally Unique Temporary Identifier (GUTI) for GSM/3G and Long-Term Evolution (LTE) networks, respectively. Unfortunately, recent studies have shown that carriers fail to protect subscribers in both GSM/3G and LTE mainly because these identifiers have static and persistent values. These identifiers can be used to track subscribers\u2019 locations. These studies have suggested that temporary identifiers must be reallocated frequently to solve this privacy problem. The only mechanism to update the temporary identifier in current LTE implementations is called GUTI reallocation. We investigate whether the current implementation of the GUTI reallocation mechanism can provide enough security to protect subscribers\u2019 privacy. To do this, we collect data by performing GUTI reallocation more than 30,000 times with 28 carriers across 11 countries using 78 SIM cards. Then, we investigate whether (1) these reallocated GUTIs in each carrier show noticeable patterns and (2) if they do, these patterns are consistent among different SIM cards within each carrier. Among 28 carriers, 19 carriers have easily predictable and consistent patterns in their GUTI reallocation mechanisms. Among the remaining 9 carriers, we revisit 4 carriers to investigate them in greater detail. For all these 4 carriers, we could find interesting yet predictable patterns after invoking GUTI reallocation multiple times within a short time period. By using this predictability, we show that an adversary can track subscribers\u2019 location as in previous studies. Finally, we present a lightweight and unpredictable GUTI reallocation mechanism as a solution.",
            "keywords": [
                "Cellular Network Privacy",
                "Temporary Identifier",
                "GUTI Reallocation",
                "Location Tracking",
                "Subscriber Identity Protection"
            ]
        },
        "url": "URL#3127764",
        "sema_paperId": "824d85c546fd1fba732a90b2f97b123b67d13182"
    },
    {
        "@score": "1",
        "@id": "3127765",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "83/7270",
                        "text": "Syed Rafiul Hussain"
                    },
                    {
                        "@pid": "53/7986",
                        "text": "Omar Chowdhury"
                    },
                    {
                        "@pid": "196/8838",
                        "text": "Shagufta Mehnaz"
                    },
                    {
                        "@pid": "b/ElisaBertino",
                        "text": "Elisa Bertino"
                    }
                ]
            },
            "title": "LTEInspector: A Systematic Approach for Adversarial Testing of 4G LTE.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/HussainCMB18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_02A-3_Hussain_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/HussainCMB18",
            "abstract": "\u0097In this paper, we investigate the security and privacy of the three critical procedures of the 4G LTE protocol (i.e., attach, detach, and paging), and in the process, uncover potential design aws of the protocol and unsafe practices employed by the stakeholders. For exposing vulnerabilities, we propose a modelbased testing approachLTEInspector which lazily combines a symbolic model checker and a cryptographic protocol veri er in the symbolic attacker model. Using LTEInspector, we have uncovered 10 new attacks along with 9 prior attacks, categorized into three abstract classes (i.e., security, user privacy, and disruption of service), in the three procedures of 4G LTE. Notable among our ndings is the authentication relay attackthat enables an adversary to spoof the location of a legitimate user to the core network without possessing appropriate credentials. To ensure that the exposed attacks pose real threats and are indeed realizable in practice, we have validated8 of the 10 new attacks and their accompanying adversarial assumptions through experimentation in a real testbed.",
            "keywords": [
                "4G LTE Security",
                "Adversarial Testing",
                "Model-Based Testing",
                "Authentication Relay Attack",
                "User Privacy Vulnerabilities"
            ]
        },
        "url": "URL#3127765",
        "sema_paperId": "7dc3945a4b49bface53bef6aa396e76f7d9b13e2"
    },
    {
        "@score": "1",
        "@id": "3127766",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "14/7561",
                        "text": "Rob Jansen"
                    },
                    {
                        "@pid": "129/8347",
                        "text": "Marc Juarez"
                    },
                    {
                        "@pid": "307/3902",
                        "text": "Rafa G\u00e1lvez"
                    },
                    {
                        "@pid": "120/5844",
                        "text": "Tariq Elahi"
                    },
                    {
                        "@pid": "d/ClaudiaDiaz",
                        "text": "Claudia D\u00edaz"
                    }
                ]
            },
            "title": "Inside Job: Applying Traffic Analysis to Measure Tor from Within.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/JansenJGED18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_10-2_Jansen_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/JansenJGED18",
            "abstract": "In this paper, we explore traffic analysis attacks on Tor that are conducted solely with middle relays rather than with relays from the entry or exit positions. We create a methodology to apply novel Tor circuit and website fingerprinting from middle relays to detect onion service usage; that is, we are able to identify websites with hidden network addresses by their traffic patterns. We also carry out the first privacypreserving popularity measurement of a single social networking website hosted as an onion service by deploying our novel circuit and website fingerprinting techniques in the wild. Our results show: (i) that the middle position enables wide-scale monitoring and measurement not possible from a comparable resource deployment in other relay positions, (ii) that traffic fingerprinting techniques are as effective from the middle relay position as prior works show from a guard relay, and (iii) that an adversary can use our fingerprinting methodology to discover the popularity of onion services, or as a filter to target specific nodes in the network, such as particular guard relays.",
            "keywords": [
                "Traffic Analysis",
                "Tor Network",
                "Onion Services",
                "Fingerprinting Techniques",
                "Privacy-Preserving Measurement"
            ]
        },
        "url": "URL#3127766",
        "sema_paperId": "37ebe111afefd898e842beebff32b9004fc53cd4"
    },
    {
        "@score": "1",
        "@id": "3127767",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "164/3347",
                        "text": "Samuel Jero"
                    },
                    {
                        "@pid": "31/1431",
                        "text": "Md. Endadul Hoque"
                    },
                    {
                        "@pid": "48/6854",
                        "text": "David R. Choffnes"
                    },
                    {
                        "@pid": "31/3833",
                        "text": "Alan Mislove"
                    },
                    {
                        "@pid": "n/CristinaNitaRotaru",
                        "text": "Cristina Nita-Rotaru"
                    }
                ]
            },
            "title": "Automated Attack Discovery in TCP Congestion Control Using a Model-guided Approach.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/JeroHCMN18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_02A-1_Jero_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/JeroHCMN18",
            "abstract": "In this work, we propose an automated method to find attacks against TCP congestion control implementations that combines the generality of implementation-agnostic fuzzing with the precision of runtime analysis. It uses a model-guided approach to generate abstract attack strategies by leveraging a state machine model of congestion control to find vulnerable state machine paths that an attacker could exploit to increase or decrease the throughput of a connection. These abstract strategies are then mapped to concrete attack strategies, which consist of sequences of actions such as injection or modification of acknowledgements. We design and implement a virtualized platform, TCPwn, that consists of a proxy-based attack injector to inject these concrete attack strategies. We evaluated 5 TCP implementations from 4 Linux distributions and Windows 8.1. Overall, we found 11 classes of attacks, of which 8 are new.",
            "keywords": [
                "TCP Congestion Control",
                "Automated Attack Discovery",
                "Model-guided Approach",
                "Fuzzing",
                "Attack Strategies"
            ]
        },
        "url": "URL#3127767",
        "sema_paperId": "6b7c488bffd039b0e1369499365718ebfd002cec"
    },
    {
        "@score": "1",
        "@id": "3127768",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "190/1205",
                        "text": "Sukrit Kalra"
                    },
                    {
                        "@pid": "157/8328",
                        "text": "Seep Goel"
                    },
                    {
                        "@pid": "09/4218",
                        "text": "Mohan Dhawan"
                    },
                    {
                        "@pid": "77/1586-1",
                        "text": "Subodh Sharma 0001"
                    }
                ]
            },
            "title": "ZEUS: Analyzing Safety of Smart Contracts.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/KalraGDS18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_09-1_Kalra_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/KalraGDS18",
            "abstract": "A smart contract is hard to patch for bugs once it is deployed, irrespective of the money it holds. A recent bug caused losses worth around $50 million of cryptocurrency. We present ZEUS\u2014a framework to verify the correctness and validate the fairness of smart contracts. We consider correctness as adherence to safe programming practices, while fairness is adherence to agreed upon higher-level business logic. ZEUS leverages both abstract interpretation and symbolic model checking, along with the power of constrained horn clauses to quickly verify contracts for safety. We have built a prototype of ZEUS for Ethereum and Fabric blockchain platforms, and evaluated it with over 22.4K smart contracts. Our evaluation indicates that about 94.6% of contracts (containing cryptocurrency worth more than $0.5 billion) are vulnerable. ZEUS is sound with zero false negatives and has a low false positive rate, with an order of magnitude improvement in analysis time as compared to prior art.",
            "keywords": [
                "Smart Contract Verification",
                "Blockchain Safety",
                "Fairness in Smart Contracts",
                "Vulnerability Analysis",
                "ZEUS Framework"
            ]
        },
        "url": "URL#3127768",
        "sema_paperId": "f3f927adf4aac1146c9587fa646864a040c94fa6"
    },
    {
        "@score": "1",
        "@id": "3127769",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "122/2010",
                        "text": "Chung Hwan Kim"
                    },
                    {
                        "@pid": "157/5259",
                        "text": "Taegyu Kim"
                    },
                    {
                        "@pid": "185/1609",
                        "text": "Hongjun Choi"
                    },
                    {
                        "@pid": "76/10471",
                        "text": "Zhongshu Gu"
                    },
                    {
                        "@pid": "45/7986",
                        "text": "Byoungyoung Lee"
                    },
                    {
                        "@pid": "95/3760-1",
                        "text": "Xiangyu Zhang 0001"
                    },
                    {
                        "@pid": "59/5539",
                        "text": "Dongyan Xu"
                    }
                ]
            },
            "title": "Securing Real-Time Microcontroller Systems through Customized Memory View Switching.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/KimKCGL0X18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_04B-2_Kim_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/KimKCGL0X18",
            "abstract": "Real-time microcontrollers have been widely adopted in cyber-physical systems that require both real-time and security guarantees. Unfortunately, security is sometimes traded for real-time performance in such systems. Notably, memory isolation, which is one of the most established security features in modern computer systems, is typically not available in many real-time microcontroller systems due to its negative impacts on performance and violation of real-time constraints. As such, the memory space of these systems has created an open, monolithic attack surface that attackers can target to subvert the entire systems. In this paper, we present MINION, a security architecture that intends to virtually partition the memory space and enforce memory access control of a real-time microcontroller. MINION can automatically identify the reachable memory regions of realtime processes through off-line static analysis on the system\u2019s firmware and conduct run-time memory access control through hardware-based enforcement. Our evaluation results demonstrate that, by significantly reducing the memory space that each process can access, MINION can effectively protect a microcontroller from various attacks that were previously viable. In addition, unlike conventional memory isolation mechanisms that might incur substantial performance overhead, the lightweight design of MINION is able to maintain the real-time properties of the microcontroller.",
            "keywords": [
                "Real-Time Microcontroller Security",
                "Memory Access Control",
                "Memory Partitioning",
                "Cyber-Physical Systems",
                "Attack Surface Reduction"
            ]
        },
        "url": "URL#3127769",
        "sema_paperId": "f58d0058a467d15481ca518f1ff4d643163b3625"
    },
    {
        "@score": "1",
        "@id": "3127770",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "139/7034",
                        "text": "Yonghwi Kwon 0001"
                    },
                    {
                        "@pid": "52/3194-1",
                        "text": "Fei Wang 0001"
                    },
                    {
                        "@pid": "57/7210-1",
                        "text": "Weihang Wang 0001"
                    },
                    {
                        "@pid": "31/9698",
                        "text": "Kyu Hyung Lee"
                    },
                    {
                        "@pid": "09/4057",
                        "text": "Wen-Chuan Lee"
                    },
                    {
                        "@pid": "172/8745",
                        "text": "Shiqing Ma"
                    },
                    {
                        "@pid": "95/3760-1",
                        "text": "Xiangyu Zhang 0001"
                    },
                    {
                        "@pid": "59/5539",
                        "text": "Dongyan Xu"
                    },
                    {
                        "@pid": "j/SomeshJha",
                        "text": "Somesh Jha"
                    },
                    {
                        "@pid": "21/4435",
                        "text": "Gabriela F. Ciocarlie"
                    },
                    {
                        "@pid": "21/2267",
                        "text": "Ashish Gehani"
                    },
                    {
                        "@pid": "75/3570",
                        "text": "Vinod Yegneswaran"
                    }
                ]
            },
            "title": "MCI : Modeling-based Causality Inference in Audit Logging for Attack Investigation.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/Kwon0WLLM0XJCGY18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07B-2_Kwon_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/Kwon0WLLM0XJCGY18",
            "abstract": "In this paper, we develop a model based causality inference technique for audit logging that does not require any application instrumentation or kernel modification. It leverages a recent dynamic analysis, dual execution (LDX), that can infer precise causality between system calls but unfortunately requires doubling the resource consumption such as CPU time and memory consumption. For each application, we use LDX to acquire precise causal models for a set of primitive operations. Each model is a sequence of system calls that have inter-dependences, some of them caused by memory operations and hence implicit at the system call level. These models are described by a language that supports various complexity such as regular, context-free, and even context-sensitive. In production run, a novel parser is deployed to parse audit logs (without any enhancement) to model instances and hence derive causality. Our evaluation on a set of real-world programs shows that the technique is highly effective. The generated models can recover causality with 0% false-positives (FP) and false-negatives (FN) for most programs and only 8.3% FP and 5.2% FN in the worst cases. The models also feature excellent composibility, meaning that the models derived from primitive operations can be composed together to describe causality for large and complex real world missions. Applying our technique to attack investigation shows that the system-wide attack causal graphs are highly precise and concise, having better quality than the state-of-the-art.",
            "keywords": [
                "Causality Inference",
                "Audit Logging",
                "Dynamic Analysis",
                "Attack Investigation",
                "System Call Inter-dependences"
            ]
        },
        "url": "URL#3127770",
        "sema_paperId": "b53c3f6c1440aa00bc140fa4b1ac3c8a20de1785"
    },
    {
        "@score": "1",
        "@id": "3127771",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "78/6080",
                        "text": "Jaeho Lee"
                    },
                    {
                        "@pid": "w/DanSWallach",
                        "text": "Dan S. Wallach"
                    }
                ]
            },
            "title": "Removing Secrets from Android&apos;s TLS.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/LeeW18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01B-3_Lee_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/LeeW18",
            "abstract": "Cryptographic libraries that implement Transport Layer Security (TLS) have a responsibility to delete cryptographic keys once they're no longer in use. Any key that's left in memory can potentially be recovered through the actions of an attacker, up to and including the physical capture and forensic analysis of a device's memory. This paper describes an analysis of the TLS library stack used in recent Android distributions, combining a C language core (BoringSSL) with multiple layers of Java code (Conscrypt, OkHttp, and Java Secure Sockets). We first conducted a black-box analysis of virtual machine images, allowing us to discover keys that might remain recoverable. After identifying several such keys, we subsequently pinpointed undesirable interactions across these layers, where the higher-level use of BoringSSL's reference counting features, from Java code, prevented BoringSSL from cleaning up its keys. This interaction poses a threat to all Android applications built on standard HTTPS libraries, exposing master secrets to memory disclosure attacks. We found all versions we investigated from Android 4 to the latest Android 8 are vulnerable, showing that this problem has been long overlooked. The Android Chrome application is proven to be particularly problematic. We suggest modest changes to the Android codebase to mitigate these issues, and have reported these to Google to help them patch the vulnerability in future Android systems.",
            "keywords": [
                "Transport Layer Security (TLS)",
                "Android Security",
                "Cryptographic Key Management",
                "Memory Disclosure Attacks",
                "BoringSSL Vulnerabilities"
            ]
        },
        "url": "URL#3127771",
        "sema_paperId": "52e76bc759f6da650dbfcb5ff4e7f052dcb13c3f"
    },
    {
        "@score": "1",
        "@id": "3127772",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "50/3402-58",
                        "text": "Bo Li 0058"
                    },
                    {
                        "@pid": "133/2036",
                        "text": "Phani Vadrevu"
                    },
                    {
                        "@pid": "31/9698",
                        "text": "Kyu Hyung Lee"
                    },
                    {
                        "@pid": "60/6768",
                        "text": "Roberto Perdisci"
                    }
                ]
            },
            "title": "JSgraph: Enabling Reconstruction of Web Attacks via Efficient Tracking of Live In-Browser JavaScript Executions.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/LiVLP18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07B-4_Li_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/LiVLP18",
            "abstract": "In this paper, we propose JSgraph, a forensic engine that is able to efficiently record fine-grained details pertaining to the execution of JavaScript (JS) programs within the browser, with particular focus on JS-driven DOM modifications. JSgraph\u2019s main goal is to enable a detailed, post-mortem reconstruction of ephemeral JS-based web attacks experienced by real network users. In particular, we aim to enable the reconstruction of social engineering attacks that result in the download of malicious executable files or browser extensions, among other attacks. We implement JSgraph by instrumenting Chromium\u2019s code base at the interface between Blink and V8, the rendering and JavaScript engines. We design JSgraph to be lightweight, highly portable, and to require low storage capacity for its fine-grained audit logs. Using a variety of both in-the-wild and lab-reproduced web attacks, we demonstrate how JSgraph can aid the forensic investigation process. We then show that JSgraph introduces acceptable overhead, with a median overhead on popular website page loads between 3.2% and 3.9%.",
            "keywords": [
                "JavaScript Forensics",
                "Web Attack Reconstruction",
                "DOM Modifications",
                "Social Engineering Attacks",
                "Malicious Executables"
            ]
        },
        "url": "URL#3127772",
        "sema_paperId": "a151603fe8c80417eff69510734ebb00a5d3bd22"
    },
    {
        "@score": "1",
        "@id": "3127773",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "74/2397-27",
                        "text": "Zhen Li 0027"
                    },
                    {
                        "@pid": "z/DeqingZou",
                        "text": "Deqing Zou"
                    },
                    {
                        "@pid": "78/2715",
                        "text": "Shouhuai Xu"
                    },
                    {
                        "@pid": "153/2318",
                        "text": "Xinyu Ou"
                    },
                    {
                        "@pid": "98/4156",
                        "text": "Hai Jin 0001"
                    },
                    {
                        "@pid": "64/3545",
                        "text": "Sujuan Wang"
                    },
                    {
                        "@pid": "18/10184",
                        "text": "Zhijun Deng"
                    },
                    {
                        "@pid": "201/9321",
                        "text": "Yuyi Zhong"
                    }
                ]
            },
            "title": "VulDeePecker: A Deep Learning-Based System for Vulnerability Detection.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/LiZXO0WDZ18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03A-2_Li_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/LiZXO0WDZ18",
            "abstract": "The automatic detection of software vulnerabilities is an important research problem. However, existing solutions to this problem rely on human experts to define features and often miss many vulnerabilities (i.e., incurring high false negative rate). In this paper, we initiate the study of using deep learning-based vulnerability detection to relieve human experts from the tedious and subjective task of manually defining features. Since deep learning is motivated to deal with problems that are very different from the problem of vulnerability detection, we need some guiding principles for applying deep learning to vulnerability detection. In particular, we need to find representations of software programs that are suitable for deep learning. For this purpose, we propose using code gadgets to represent programs and then transform them into vectors, where a code gadget is a number of (not necessarily consecutive) lines of code that are semantically related to each other. This leads to the design and implementation of a deep learning-based vulnerability detection system, called Vulnerability Deep Pecker (VulDeePecker). In order to evaluate VulDeePecker, we present the first vulnerability dataset for deep learning approaches. Experimental results show that VulDeePecker can achieve much fewer false negatives (with reasonable false positives) than other approaches. We further apply VulDeePecker to 3 software products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which are not reported in the National Vulnerability Database but were \u201csilently\u201d patched by the vendors when releasing later versions of these products; in contrast, these vulnerabilities are almost entirely missed by the other vulnerability detection systems we experimented with.",
            "keywords": [
                "Vulnerability Detection",
                "Deep Learning",
                "Code Gadgets",
                "False Negatives",
                "Software Vulnerabilities"
            ]
        },
        "url": "URL#3127773"
    },
    {
        "@score": "1",
        "@id": "3127774",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "92/10048",
                        "text": "Yingqi Liu"
                    },
                    {
                        "@pid": "172/8745",
                        "text": "Shiqing Ma"
                    },
                    {
                        "@pid": "138/6356",
                        "text": "Yousra Aafer"
                    },
                    {
                        "@pid": "09/4057",
                        "text": "Wen-Chuan Lee"
                    },
                    {
                        "@pid": "154/5678",
                        "text": "Juan Zhai"
                    },
                    {
                        "@pid": "57/7210-1",
                        "text": "Weihang Wang 0001"
                    },
                    {
                        "@pid": "95/3760-1",
                        "text": "Xiangyu Zhang 0001"
                    }
                ]
            },
            "title": "Trojaning Attack on Neural Networks.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/LiuMALZW018",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03A-5_Liu_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/LiuMALZW018",
            "abstract": "\u2014With the fast spread of machine learning tech- niques, sharing and adopting public machine learning models become very popular. This gives attackers many new opportunities. In this paper, we propose a trojaning attack on neural networks. As the models are not intuitive for human to understand, the attack features stealthiness. Deploying trojaned models can cause various severe consequences including endangering human lives (in applications like autonomous driving). We \ufb01rst inverse the neural network to generate a general trojan trigger , and then retrain the model with reversed engineered training data to inject malicious behaviors to the model. The malicious behaviors are only activated by inputs stamped with the trojan trigger. In our attack, we do not need to tamper with the original training process, which usually takes weeks to months. Instead, it takes minutes to hours to apply our attack. Also, we do not require the datasets that are used to train the model. In practice, the datasets are usually not shared due to privacy or copyright concerns. We use \ufb01ve different applications to demonstrate the power of our attack, and perform a deep analysis on the possible factors that affect the attack. The results show that our attack is highly effective and ef\ufb01cient. The trojaned behaviors can be successfully triggered (with nearly 100% possibility) without affecting its test accuracy for normal input and even with better accuracy on public dataset. Also, it only takes a small amount of time to attack a complex neuron network model. In the end, we also discuss possible defense against such attacks.",
            "keywords": [
                "Trojaning Attack",
                "Neural Network Manipulation",
                "Malicious Behavior Injection",
                "Stealthy Attacks",
                "Trigger Activation"
            ]
        },
        "url": "URL#3127774",
        "sema_paperId": "08f7ac64b420210aa46fcbbdb0f206215f2e0644"
    },
    {
        "@score": "1",
        "@id": "3127775",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "129/9145-4",
                        "text": "Yushan Liu 0004"
                    },
                    {
                        "@pid": "28/3341-1",
                        "text": "Mu Zhang 0001"
                    },
                    {
                        "@pid": "95/7132-1",
                        "text": "Ding Li 0001"
                    },
                    {
                        "@pid": "36/10085",
                        "text": "Kangkook Jee"
                    },
                    {
                        "@pid": "55/4022",
                        "text": "Zhichun Li"
                    },
                    {
                        "@pid": "87/6581-3",
                        "text": "Zhenyu Wu 0003"
                    },
                    {
                        "@pid": "27/5932",
                        "text": "Junghwan Rhee"
                    },
                    {
                        "@pid": "39/6266",
                        "text": "Prateek Mittal"
                    }
                ]
            },
            "title": "Towards a Timely Causality Analysis for Enterprise Security.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/LiuZLJLWRM18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07B-3_Liu_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/LiuZLJLWRM18",
            "abstract": "\u2014The increasingly sophisticated Advanced Persistent Threat (APT) attacks have become a serious challenge for enterprise IT security. Attack causality analysis , which tracks multi-hop causal relationships between \ufb01les and processes to diagnose attack provenances and consequences, is the \ufb01rst step towards under- standing APT attacks and taking appropriate responses. Since attack causality analysis is a time-critical mission, it is essential to design causality tracking systems that extract useful attack information in a timely manner. However, prior work is limited in serving this need. Existing approaches have largely focused on pruning causal dependencies totally irrelevant to the attack, but fail to differentiate and prioritize abnormal events from numerous relevant, yet benign and complicated system operations, resulting in long investigation time and slow responses. To address this problem, we propose P RIO T RACKER , a back- ward and forward causality tracker that automatically prioritizes the investigation of abnormal causal dependencies in the tracking process. Speci\ufb01cally, to assess the priority of a system event, we consider its rareness and topological features in the causality graph. To distinguish unusual operations from normal system events, we quantify the rareness of each event by developing a reference model which records common routine activities in corporate computer systems. We implement P RIO T RACKER , in 20K lines of Java code, and a reference model builder in 10K lines of Java code. We evaluate our tool by deploying both systems in a real enterprise IT environment, where we collect 1TB of 2.5 billion OS events from 150 machines in one week. Experimental results show that P RIO T RACKER can capture attack traces that are missed by existing trackers and reduce the analysis time by up to two orders of magnitude .",
            "keywords": [
                "Advanced Persistent Threats",
                "Causality Analysis",
                "Attack Provenance",
                "Event Prioritization",
                "Causality Tracking System"
            ]
        },
        "url": "URL#3127775",
        "sema_paperId": "0109ca467d6739ed88ef0bedc0df2c91c95d51d1"
    },
    {
        "@score": "1",
        "@id": "3127776",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "138/1005",
                        "text": "William Melicher"
                    },
                    {
                        "@pid": "84/5118-1",
                        "text": "Anupam Das 0001"
                    },
                    {
                        "@pid": "136/8393",
                        "text": "Mahmood Sharif"
                    },
                    {
                        "@pid": "32/3440",
                        "text": "Lujo Bauer"
                    },
                    {
                        "@pid": "89/161-1",
                        "text": "Limin Jia 0001"
                    }
                ]
            },
            "title": "Riding out DOMsday: Towards Detecting and Preventing DOM Cross-Site Scripting.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/MelicherDSBJ18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07A-4_Melicher_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/MelicherDSBJ18",
            "abstract": "\u2014Cross-site scripting (XSS) vulnerabilities are the most frequently reported web application vulnerability. As complex JavaScript applications become more widespread, DOM (Document Object Model) XSS vulnerabilities\u2014a type of XSS vulnerability where the vulnerability is located in client-side JavaScript, rather than server-side code\u2014are becoming more common. As the \ufb01rst contribution of this work, we empirically assess the impact of DOM XSS on the web using a browser with taint tracking embedded in the JavaScript engine. Building on the methodology used in a previous study that crawled popular websites, we collect a current dataset of potential DOM XSS vulnerabilities. We improve on the methodology for con\ufb01rming XSS vulnerabilities, and using this improved methodology, we \ufb01nd 83% more vulnerabilities than previous methodology applied to the same dataset. As a second contribution, we identify the causes of and discuss how to prevent DOM XSS vulnerabilities. One example of our \ufb01ndings is that custom HTML templating designs\u2014a design pattern that could prevent DOM XSS vulnerabilities analogous to parameterized SQL\u2014can be buggy in practice, allowing DOM XSS attacks. As our third contribution, we evaluate the error rates of three static-analysis tools to detect DOM XSS vulnerabilities found with dynamic analysis techniques using in-the-wild examples. We \ufb01nd static-analysis tools to miss 90% of bugs found by our dynamic analysis, though some tools can have very few false positives and at the same time \ufb01nd vulnerabilities not found using the dynamic analysis.",
            "keywords": [
                "DOM XSS",
                "Cross-Site Scripting",
                "Taint Tracking",
                "Static Analysis Tools",
                "Dynamic Analysis Techniques"
            ]
        },
        "url": "URL#3127776",
        "sema_paperId": "e8116b9b471ad137a9fb7392a6c214341eb04931"
    },
    {
        "@score": "1",
        "@id": "3127777",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "160/6851",
                        "text": "Yisroel Mirsky"
                    },
                    {
                        "@pid": "215/4893",
                        "text": "Tomer Doitshman"
                    },
                    {
                        "@pid": "38/4086",
                        "text": "Yuval Elovici"
                    },
                    {
                        "@pid": "56/5380",
                        "text": "Asaf Shabtai"
                    }
                ]
            },
            "title": "Kitsune: An Ensemble of Autoencoders for Online Network Intrusion Detection.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/MirskyDES18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03A-3_Mirsky_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/MirskyDES18",
            "abstract": "Neural networks have become an increasingly popular solution for network intrusion detection systems (NIDS). Their capability of learning complex patterns and behaviors make them a suitable solution for differentiating between normal traffic and network attacks. However, a drawback of neural networks is the amount of resources needed to train them. Many network gateways and routers devices, which could potentially host an NIDS, simply do not have the memory or processing power to train and sometimes even execute such models. More importantly, the existing neural network solutions are trained in a supervised manner. Meaning that an expert must label the network traffic and update the model manually from time to time. \nIn this paper, we present Kitsune: a plug and play NIDS which can learn to detect attacks on the local network, without supervision, and in an efficient online manner. Kitsune's core algorithm (KitNET) uses an ensemble of neural networks called autoencoders to collectively differentiate between normal and abnormal traffic patterns. KitNET is supported by a feature extraction framework which efficiently tracks the patterns of every network channel. Our evaluations show that Kitsune can detect various attacks with a performance comparable to offline anomaly detectors, even on a Raspberry PI. This demonstrates that Kitsune can be a practical and economic NIDS.",
            "keywords": [
                "Network Intrusion Detection",
                "Autoencoders",
                "Unsupervised Learning",
                "Online Detection",
                "Feature Extraction"
            ]
        },
        "url": "URL#3127777",
        "sema_paperId": "fa11df1157486731afe86af7d6d9254a82d4dbd6"
    },
    {
        "@score": "1",
        "@id": "3127778",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "185/2352",
                        "text": "Marius Muench"
                    },
                    {
                        "@pid": "224/2488",
                        "text": "Jan Stijohann"
                    },
                    {
                        "@pid": "46/306",
                        "text": "Frank Kargl"
                    },
                    {
                        "@pid": "88/3343",
                        "text": "Aur\u00e9lien Francillon"
                    },
                    {
                        "@pid": "56/490",
                        "text": "Davide Balzarotti"
                    }
                ]
            },
            "title": "What You Corrupt Is Not What You Crash: Challenges in Fuzzing Embedded Devices.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/MuenchSKFB18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01A-4_Muench_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/MuenchSKFB18",
            "abstract": "\u2014As networked embedded systems are becoming more ubiquitous, their security is becoming critical to our daily life. While manual or automated large scale analysis of those systems regularly uncover new vulnerabilities, the way those systems are analyzed follows often the same approaches used on desktop systems. More speci\ufb01cally, traditional testing approaches relies on observable crashes of a program, and binary instrumentation techniques are used to improve the detection of those faulty states. In this paper, we demonstrate that memory corruptions, a common class of security vulnerabilities, often result in different behavior on embedded devices than on desktop systems. In particular, on embedded devices, effects of memory corruption are often less visible. This reduces signi\ufb01cantly the effectiveness of traditional dynamic testing techniques in general, and fuzzing in particular. Additionally, we analyze those differences in several categories of embedded devices and show the resulting impact on \ufb01rmware analysis. We further describe and evaluate relatively simple heuristics which can be applied at run time (on an execution trace or in an emulator), during the analysis of an embedded device to detect previously undetected memory corruptions.",
            "keywords": [
                "Embedded Systems Security",
                "Fuzzing",
                "Memory Corruption",
                "Dynamic Testing Techniques",
                "Firmware Analysis"
            ]
        },
        "url": "URL#3127778",
        "sema_paperId": "c7cfb5aa26c232a799a993ad39b327733a39e3b8"
    },
    {
        "@score": "1",
        "@id": "3127779",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "167/0416",
                        "text": "Yuhong Nan"
                    },
                    {
                        "@pid": "55/11040",
                        "text": "Zhemin Yang"
                    },
                    {
                        "@pid": "06/6268",
                        "text": "Xiaofeng Wang 0006"
                    },
                    {
                        "@pid": "48/2168-9",
                        "text": "Yuan Zhang 0009"
                    },
                    {
                        "@pid": "70/5222",
                        "text": "Donglai Zhu"
                    },
                    {
                        "@pid": "02/1640-2",
                        "text": "Min Yang 0002"
                    }
                ]
            },
            "title": "Finding Clues for Your Secrets: Semantics-Driven, Learning-Based Privacy Discovery in Mobile Apps.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/NanY0ZZ018",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05B-1_Nan_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/NanY0ZZ018",
            "abstract": "\u2014A long-standing challenge in analyzing information leaks within mobile apps is to automatically identify the code operating on sensitive data. With all existing solutions relying on System APIs (e.g., IMEI, GPS location) or features of user interfaces (UI), the content from app servers, like user\u2019s Facebook pro\ufb01le, payment history, fall through the crack. Finding such content is important given the fact that most apps today are web applications, whose critical data are often on the server side. In the meantime, operations on the data within mobile apps are often hard to capture, since all server-side information is delivered to the app in the same way, sensitive or not. A unique observation of our research is that in modern apps, a program is essentially a semantics-rich documentation carrying meaningful program elements such as method names, variables and constants that reveal the sensitive data involved, even when the program is under moderate obfuscation. Leveraging this observation, we develop a novel semantics-driven solution for automatic discovery of sensitive user data, including those from the server side. Our approach utilizes natural language processing (NLP) to automatically locate the program elements (variables, methods, etc.) of interest, and then performs a learning-based program structure analysis to accurately identify those indeed carrying sensitive content. Using this new technique, we analyzed 445,668 popular apps, an unprecedented scale for this type of research. Our work brings to light the pervasiveness of information leaks, and the channels through which the leaks happen, including unintentional over-sharing across libraries and aggressive data acquisition behaviors. Further we found that many high-pro\ufb01le apps and libraries are involved in such leaks. Our \ufb01ndings contribute to a better understanding of the privacy",
            "keywords": [
                "Mobile App Privacy",
                "Information Leakage",
                "Sensitive Data Discovery",
                "Natural Language Processing",
                "Program Structure Analysis"
            ]
        },
        "url": "URL#3127779",
        "sema_paperId": "ec9e58e620c48dee02c0abfac153fceb8d6f6d3d"
    },
    {
        "@score": "1",
        "@id": "3127780",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "221/2301",
                        "text": "Parimarjan Negi"
                    },
                    {
                        "@pid": "224/2474",
                        "text": "Prafull Sharma"
                    },
                    {
                        "@pid": "29/4326",
                        "text": "Vivek Jain"
                    },
                    {
                        "@pid": "56/8262",
                        "text": "Bahman Bahmani"
                    }
                ]
            },
            "title": "K-means++ vs. Behavioral Biometrics: One Loop to Rule Them All.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/NegiSJB18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03B-2_Negi_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/NegiSJB18",
            "abstract": "Behavioral biometrics, a field that studies patterns in an individual\u2019s unique behavior, has been researched actively as a means of authentication for decades. Recently, it has even been adopted in many real world scenarios. In this paper, we study keystroke dynamics, the most researched of such behavioral biometrics, from the perspective of an adversary. We designed two adversarial agents with a standard accuracy convenience tradeoff: Targeted K-means++, which is an expensive, but extremely effective adversarial agent, and Indiscriminate K-means++, which is slightly less powerful, but adds no overhead cost to the attacker. With Targeted K-means++ we could compromise the security of 40-70% of users within ten tries. In contrast, with Indiscriminate K-means++, the security of 30-50% of users was compromised. Therefore, we conclude that while keystroke dynamics has potential, it is not ready for security critical applications yet. Future keystroke dynamics research should use such adversaries to benchmark the performance of the detection algorithms, and design better algorithms to foil these. Finally, we show that the K-means++ adversarial agent generalizes well to even other types of behavioral biometrics data by applying it on a dataset of touchscreen swipes.",
            "keywords": [
                "Behavioral Biometrics",
                "Keystroke Dynamics",
                "Adversarial Agents",
                "K-means++",
                "Security Compromise"
            ]
        },
        "url": "URL#3127780",
        "sema_paperId": "f46c48c38f5537727b0f440ee94fedff3fe425f0"
    },
    {
        "@score": "1",
        "@id": "3127781",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "147/1554",
                        "text": "Alexandra-Mihaela Olteanu"
                    },
                    {
                        "@pid": "10/3024",
                        "text": "K\u00e9vin Huguenin"
                    },
                    {
                        "@pid": "82/5414",
                        "text": "Italo Dacosta"
                    },
                    {
                        "@pid": "h/JPHubaux",
                        "text": "Jean-Pierre Hubaux"
                    }
                ]
            },
            "title": "Consensual and Privacy-Preserving Sharing of Multi-Subject and Interdependent Data.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/OlteanuHDH18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/07/ndss2018_06B-1_Olteanu_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/OlteanuHDH18",
            "abstract": "Individuals share increasing amounts of personal data online. This data often involves\u2013or at least has privacy implications for\u2013data subjects other than the individuals who shares it (e.g., photos, genomic data) and the data is shared without their consent. A sadly popular example, with dramatic consequences, is revenge pornography. In this paper, we propose ConsenShare, a system for sharing, in a consensual (wrt the data subjects) and privacy-preserving (wrt both service providers and other individuals) way, data involving subjects other than the uploader. We describe a complete design and implementation of ConsenShare for photos, which relies on image processing and cryptographic techniques, as well as on a two-tier architecture (one entity for detecting the data subjects and contacting them; one entity for hosting the data and for collecting consent). We benchmark the performance (CPU and bandwidth) of ConsenShare by using a dataset of 20k photos from Flickr. We also conduct a survey targeted at Facebook users (N = 321). Our results are quite encouraging: The experimental results demonstrate the feasibility of our approach (i.e., acceptable overheads) and the survey results demonstrate a potential interest from the users.",
            "keywords": [
                "Privacy-Preserving Data Sharing",
                "Consensual Data Sharing",
                "Multi-Subject Data",
                "Image Processing",
                "User Consent"
            ]
        },
        "url": "URL#3127781",
        "sema_paperId": "88cc230c6755dc88e110032a5ef96b6251c03917"
    },
    {
        "@score": "1",
        "@id": "3127782",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "164/5196",
                        "text": "Sharbani Pandit"
                    },
                    {
                        "@pid": "60/6768",
                        "text": "Roberto Perdisci"
                    },
                    {
                        "@pid": "73/3162",
                        "text": "Mustaque Ahamad"
                    },
                    {
                        "@pid": "26/7361",
                        "text": "Payas Gupta"
                    }
                ]
            },
            "title": "Towards Measuring the Effectiveness of Telephony Blacklists.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/PanditPAG18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_04A-3_Pandit_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/PanditPAG18",
            "abstract": "Presented on February 9, 2018 at 12:00 p.m. in the Klaus Advanced Computing Building, Room 1116W.",
            "keywords": [
                "Telephony Blacklists",
                "Spam Detection",
                "Call Filtering",
                "Effectiveness Measurement",
                "Telecommunications Security"
            ]
        },
        "url": "URL#3127782",
        "sema_paperId": "8c0c100cc2dad3d266692be6b60aded5948ef6eb"
    },
    {
        "@score": "1",
        "@id": "3127783",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "66/7821",
                        "text": "Apostolos Pyrgelis"
                    },
                    {
                        "@pid": "01/4825",
                        "text": "Carmela Troncoso"
                    },
                    {
                        "@pid": "36/6225",
                        "text": "Emiliano De Cristofaro"
                    }
                ]
            },
            "title": "Knock Knock, Who&apos;s There? Membership Inference on Aggregate Location Data.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/PyrgelisTC18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05B-5_Pyrgelis_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/PyrgelisTC18",
            "abstract": "Aggregate location data is often used to support smart services and applications, e.g., generating live traffic maps or predicting visits to businesses. In this paper, we present the first study on the feasibility of membership inference attacks on aggregate location time-series. We introduce a game-based definition of the adversarial task, and cast it as a classification problem where machine learning can be used to distinguish whether or not a target user is part of the aggregates. We empirically evaluate the power of these attacks on both raw and differentially private aggregates using two mobility datasets. We find that membership inference is a serious privacy threat, and show how its effectiveness depends on the adversary's prior knowledge, the characteristics of the underlying location data, as well as the number of users and the timeframe on which aggregation is performed. Although differentially private mechanisms can indeed reduce the extent of the attacks, they also yield a significant loss in utility. Moreover, a strategic adversary mimicking the behavior of the defense mechanism can greatly limit the protection they provide. Overall, our work presents a novel methodology geared to evaluate membership inference on aggregate location data in real-world settings and can be used by providers to assess the quality of privacy protection before data release or by regulators to detect violations.",
            "keywords": [
                "Aggregate Location Data",
                "Membership Inference",
                "Privacy Threats",
                "Differential Privacy",
                "Adversarial Attacks"
            ]
        },
        "url": "URL#3127783"
    },
    {
        "@score": "1",
        "@id": "3127784",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "149/9322",
                        "text": "Abbas Razaghpanah"
                    },
                    {
                        "@pid": "35/8412",
                        "text": "Rishab Nithyanand"
                    },
                    {
                        "@pid": "50/7563",
                        "text": "Narseo Vallina-Rodriguez"
                    },
                    {
                        "@pid": "47/8093",
                        "text": "Srikanth Sundaresan"
                    },
                    {
                        "@pid": "31/1402",
                        "text": "Mark Allman"
                    },
                    {
                        "@pid": "62/7004",
                        "text": "Christian Kreibich"
                    },
                    {
                        "@pid": "52/2893",
                        "text": "Phillipa Gill"
                    }
                ]
            },
            "title": "Apps, Trackers, Privacy, and Regulators: A Global Study of the Mobile Tracking Ecosystem.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/RazaghpanahNVSA18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05B-3_Razaghpanah_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/RazaghpanahNVSA18",
            "abstract": "Third-party services form an integral part of the mobile ecosystem: they ease application development and enable features such as analytics, social network integration, and app monetization through ads. However, aided by the general opacity of mobile systems, such services are also largely invisible to users. This has negative consequences for user privacy as third-party services can potentially track users without their consent, even across multiple applications. Using real-world mobile traffic data gathered by the Lumen Privacy Monitor (Lumen), a privacy enhancing app with the ability to analyze network traffic on mobile devices in user space, we present insights into the mobile advertising and tracking ecosystem and its stakeholders. In this study, we develop automated methods to detect third-party advertising and tracking services at the traffic level. Using this technique we identify 2,121 such services, of which 233 were previously unknown to other popular advertising and tracking blacklists. We then uncover the business relationships between the providers of these services and characterize them by their prevalence in the mobile and Web ecosystem. Our analysis of the privacy policies of the largest advertising and tracking service providers shows that sharing harvested data with subsidiaries and third-party affiliates is the norm. Finally, we seek to identify the services likely to be most impacted by privacy regulations such as the European General Data Protection Regulation (GDPR) and ePrivacy directives.",
            "keywords": [
                "Mobile Tracking Ecosystem",
                "Third-Party Services",
                "User Privacy",
                "Advertising and Tracking",
                "Privacy Regulations"
            ]
        },
        "url": "URL#3127784",
        "sema_paperId": "cebd1aa91ee72e63180eeb0f671c8e7d15e2551e"
    },
    {
        "@score": "1",
        "@id": "3127785",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "60/10581",
                        "text": "Jingjing Ren"
                    },
                    {
                        "@pid": "81/10928",
                        "text": "Martina Lindorfer"
                    },
                    {
                        "@pid": "69/7759",
                        "text": "Daniel J. Dubois"
                    },
                    {
                        "@pid": "43/1348",
                        "text": "Ashwin Rao"
                    },
                    {
                        "@pid": "48/6854",
                        "text": "David R. Choffnes"
                    },
                    {
                        "@pid": "50/7563",
                        "text": "Narseo Vallina-Rodriguez"
                    }
                ]
            },
            "title": "Bug Fixes, Improvements, ... and Privacy Leaks - A Longitudinal Study of PII Leaks Across Android App Versions.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/RenLDRCV18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05B-2_Ren_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/RenLDRCV18",
            "abstract": "Is mobile privacy getting better or worse over time? In this paper, we address this question by studying privacy leaks from historical and current versions of 512 popular Android apps, covering 7,665 app releases over 8 years of app version history. Through automated and scripted interaction with apps and analysis of the network traffic they generate on real mobile devices, we identify how privacy changes over time for individual apps and in aggregate. We find several trends that include increased collection of personally identifiable information (PII) across app versions, slow adoption of HTTPS to secure the information sent to other parties, and a large number of third parties being able to link user activity and locations across apps. Interestingly, while privacy is getting worse in aggregate, we find that the privacy risk of individual apps varies greatly over time, and a substantial fraction of apps see little change or even improvement in privacy. Given these trends, we propose metrics for quantifying privacy risk and for providing this risk assessment proactively to help users balance the risks and benefits of installing new versions of apps.",
            "keywords": [
                "Mobile Privacy",
                "Android Apps",
                "Personally Identifiable Information (PII)",
                "Privacy Leaks",
                "App Version History"
            ]
        },
        "url": "URL#3127785",
        "sema_paperId": "8d98e8beed02e46fd172004ac6cb5c7b77db842a"
    },
    {
        "@score": "1",
        "@id": "3127786",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "205/2438",
                        "text": "Vera Rimmer"
                    },
                    {
                        "@pid": "12/603",
                        "text": "Davy Preuveneers"
                    },
                    {
                        "@pid": "129/8347",
                        "text": "Marc Juarez"
                    },
                    {
                        "@pid": "147/2244",
                        "text": "Tom van Goethem"
                    },
                    {
                        "@pid": "13/86",
                        "text": "Wouter Joosen"
                    }
                ]
            },
            "title": "Automated Website Fingerprinting through Deep Learning.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/RimmerPJGJ18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03A-1_Rimmer_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/RimmerPJGJ18",
            "abstract": "Several studies have shown that the network traffic that is generated by a visit to a website over Tor reveals information specific to the website through the timing and sizes of network packets. By capturing traffic traces between users and their Tor entry guard, a network eavesdropper can leverage this meta-data to reveal which website Tor users are visiting. The success of such attacks heavily depends on the particular set of traffic features that are used to construct the fingerprint. Typically, these features are manually engineered and, as such, any change introduced to the Tor network can render these carefully constructed features ineffective. In this paper, we show that an adversary can automate the feature engineering process, and thus automatically deanonymize Tor traffic by applying our novel method based on deep learning. We collect a dataset comprised of more than three million network traces, which is the largest dataset of web traffic ever used for website fingerprinting, and find that the performance achieved by our deep learning approaches is comparable to known methods which include various research efforts spanning over multiple years. The obtained success rate exceeds 96% for a closed world of 100 websites and 94% for our biggest closed world of 900 classes. In our open world evaluation, the most performant deep learning model is 2% more accurate than the state-of-the-art attack. Furthermore, we show that the implicit features automatically learned by our approach are far more resilient to dynamic changes of web content over time. We conclude that the ability to automatically construct the most relevant traffic features and perform accurate traffic recognition makes our deep learning based approach an efficient, flexible and robust technique for website fingerprinting.",
            "keywords": [
                "Website Fingerprinting",
                "Traffic Analysis",
                "Tor Network",
                "Deep Learning Feature Engineering",
                "Traffic Deanonymization"
            ]
        },
        "url": "URL#3127786",
        "sema_paperId": "9084735e7d0bff1949c2a1c59b2bfa9f8baa82b8"
    },
    {
        "@score": "1",
        "@id": "3127787",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "r/HubertRitzdorf",
                        "text": "Hubert Ritzdorf"
                    },
                    {
                        "@pid": "181/1565",
                        "text": "Karl W\u00fcst"
                    },
                    {
                        "@pid": "138/9020",
                        "text": "Arthur Gervais"
                    },
                    {
                        "@pid": "201/6441",
                        "text": "Guillaume Felley"
                    },
                    {
                        "@pid": "51/1639",
                        "text": "Srdjan Capkun"
                    }
                ]
            },
            "title": "TLS-N: Non-repudiation over TLS Enablign Ubiquitous Content Signing.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/RitzdorfWGFC18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_09-4_Ritzdorf_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/RitzdorfWGFC18",
            "abstract": "An internet user wanting to share observed content is typically restricted to primitive techniques such as screenshots, web caches or share button-like solutions. These acclaimed proofs, however, are either trivial to falsify or require trust in centralized entities (e.g., search engine caches). This motivates the need for a seamless and standardized internet-wide non-repudiation mechanism, allowing users to share data from news sources, social websites or financial data feeds in a provably secure manner. Additionally, blockchain oracles that enable data-rich smart contracts typically rely on a trusted third party (e.g., TLSNotary or Intel SGX). A decentralized method to transfer web-based content into a permissionless blockchain without additional trusted third party would allow for smart contract applications to flourish. In this work, we present TLS-N, the first TLS extension that provides secure non-repudiation and solves both of the mentioned challenges. TLS-N generates non-interactive proofs about the content of a TLS session that can be efficiently verified by third parties and blockchain based smart contracts. As such, TLS-N increases the accountability for content provided on the web and enables a practical and decentralized blockchain oracle for web content. TLS-N is compatible with TLS 1.3 and adds a minor overhead to a typical TLS session. When a proof is generated, parts of the TLS session (e.g., passwords, cookies) can be hidden for privacy reasons, while the remaining content can be verified.",
            "keywords": [
                "TLS Extension",
                "Non-repudiation",
                "Decentralized Content Signing",
                "Blockchain Oracles",
                "Content Verification"
            ]
        },
        "url": "URL#3127787"
    },
    {
        "@score": "1",
        "@id": "3127788",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "121/9557",
                        "text": "Marc Roeschlin"
                    },
                    {
                        "@pid": "87/2623",
                        "text": "Ivan Martinovic"
                    },
                    {
                        "@pid": "01/4779",
                        "text": "Kasper Bonne Rasmussen"
                    }
                ]
            },
            "title": "Device Pairing at the Touch of an Electrode.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/RoeschlinMR18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03B-4_Roeschlin_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/RoeschlinMR18",
            "abstract": "Device pairing is the problem of having two devices securely establish a key that can be used to secure subsequent communication. The problem arises every time two devices that do not already share a secret need to bootstrap a secure communication channel. Many solutions exist, all suited to different situations, and all with their own strengths and weaknesses. In this paper, we propose a novel approach to device pairing that applies whenever a user wants to pair two devises that can be physically touched at the same time. The pairing process is easy to perform, even for novice users. A central problem for a device (Alice) running a device pairing protocol, is determining whether the other party (Bob) is in fact the device that we are supposed to establish a key with. Our scheme is based on the idea that two devices can perform device pairing, if they are physically held by the same person (at the same time). In order to pair two devices, a person touches a conductive surface on each device. While the person is in contact with both devices, the human body acts as a transmission medium for intra-body communication and the two devices can communicate through the body. This body channel is used as part of a pairing protocol which allows the devices to agree on a mutual secret and, at the same time, extract physical features to verify that they are being held by the same person. We prove that our device pairing protocol is secure in our threat model and we build a proof of concept set-up and conduct experiments with 15 people to verify the idea in practice.",
            "keywords": [
                "Device Pairing",
                "Intra-body Communication",
                "Secure Key Establishment",
                "Physical Authentication",
                "User-initiated Pairing"
            ]
        },
        "url": "URL#3127788",
        "sema_paperId": "c68d471e850a5df666c6d200424a22b2e2174a95"
    },
    {
        "@score": "1",
        "@id": "3127789",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "57/8781",
                        "text": "Stefanie Roos"
                    },
                    {
                        "@pid": "128/4640",
                        "text": "Pedro Moreno-Sanchez"
                    },
                    {
                        "@pid": "12/3394",
                        "text": "Aniket Kate"
                    },
                    {
                        "@pid": "04/6434",
                        "text": "Ian Goldberg"
                    }
                ]
            },
            "title": "Settling Payments Fast and Private: Efficient Decentralized Routing for Path-Based Transactions.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/RoosMKG18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_09-3_Roos_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/RoosMKG18",
            "abstract": "Decentralized path-based transaction (PBT) networks maintain local payment channels between participants. Pairs of users leverage these channels to settle payments via a path of intermediaries without the need to record all transactions in a global blockchain. PBT networks such as Bitcoin\u2019s Lightning Network and Ethereum\u2019s Raiden Network are the most prominent examples of this emergent area of research. Both networks overcome scalability issues of widely used cryptocurrencies by replacing expensive and slow on-chain blockchain operations with inexpensive and fast off-chain transfers. At the core of a decentralized PBT network is a routing algorithm that discovers transaction paths between sender and receiver. In recent years, a number of routing algorithms have been proposed, including landmark routing, utilized in the decentralized IOU credit network SilentWhispers, and Flare, a link state algorithm for the Lightning Network. However, the existing efforts lack either efficiency or privacy, as well as the comprehensive analysis that is indispensable to ensure the success of PBT networks in practice. In this work, we first identify several efficiency concerns in existing routing algorithms for decentralized PBT networks. Armed with this knowledge, we design and evaluate SpeedyMurmurs, a novel routing algorithm for decentralized PBT networks using efficient and flexible embedding-based path discovery and on-demand efficient stabilization to handle the dynamics of a PBT network. Our simulation study, based on real-world data from the currently deployed Ripple credit network, indicates that SpeedyMurmurs reduces the overhead of stabilization by up to two orders of magnitude and the overhead of routing a transaction by more than a factor of two. Furthermore, using SpeedyMurmurs maintains at least the same success ratio as decentralized landmark routing, while providing lower delays. Finally, SpeedyMurmurs achieves key privacy goals for routing in decentralized PBT networks.",
            "keywords": [
                "Decentralized Payment Networks",
                "Path-Based Transactions",
                "Routing Algorithms",
                "Efficiency and Privacy",
                "SpeedyMurmurs"
            ]
        },
        "url": "URL#3127789"
    },
    {
        "@score": "1",
        "@id": "3127790",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "201/6430",
                        "text": "Sajin Sasy"
                    },
                    {
                        "@pid": "117/3299-1",
                        "text": "Sergey Gorbunov 0001"
                    },
                    {
                        "@pid": "25/8166",
                        "text": "Christopher W. Fletcher"
                    }
                ]
            },
            "title": "ZeroTrace : Oblivious Memory Primitives from Intel SGX.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/SasyGF18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_02B-4_Sasy_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/SasyGF18",
            "abstract": "We are witnessing a confluence between applied cryptography and secure hardware systems in enabling secure cloud computing. On one hand, work in applied cryptography has enabled efficient, oblivious data-structures and memory primitives. On the other, secure hardware and the emergence of Intel SGX has enabled a low-overhead and mass market mechanism for isolated execution. By themselves these technologies have their disadvantages. Oblivious memory primitives carry high performance overheads, especially when run non-interactively. Intel SGX, while more efficient, suffers from numerous software-based side-channel attacks, high context switching costs, and bounded memory size.In this work we build a new library of oblivious memory primitives, which we call ZeroTrace. ZeroTrace is designed to carefully combine state-of-the-art oblivious RAM techniques and SGX, while mitigating individual disadvantages of these technologies. To the best of our knowledge, ZeroTrace represents the first oblivious memory primitives running on a real secure hardware platform. ZeroTrace simultaneously enables a dramatic speed-up over pure cryptography and protection from software-based side-channel attacks. The core of our design is an efficient and flexible block-level memory controller that provides oblivious execution against any active software adversary, and across asynchronous SGX enclave terminations. Performance-wise, the memory controller can service requests for 4 B blocks in 1.2 ms and 1 KB blocks in 3.4 ms (given a 10 GB dataset). On top of our memory controller, we evaluate Set/Dictionary/List interfaces which can all perform basic operations (e.g., get/put/insert).",
            "keywords": [
                "Oblivious Memory Primitives",
                "Intel SGX",
                "Secure Cloud Computing",
                "Side-Channel Attacks",
                "Oblivious RAM Techniques"
            ]
        },
        "url": "URL#3127790"
    },
    {
        "@score": "1",
        "@id": "3127791",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "179/8624",
                        "text": "Cristian-Alexandru Staicu"
                    },
                    {
                        "@pid": "25/2188",
                        "text": "Michael Pradel"
                    },
                    {
                        "@pid": "46/2924",
                        "text": "Benjamin Livshits"
                    }
                ]
            },
            "title": "SYNODE: Understanding and Automatically Preventing Injection Attacks on NODE.JS.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/StaicuPL18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07A-2_Staicu_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/StaicuPL18",
            "abstract": "The Node.js ecosystem has lead to the creation of many modern applications, such as serverside web applications and desktop applications. Unlike client-side JavaScript code, Node.js applications can interact freely with the operating system without the benefits of a security sandbox. As a result, command injection attacks can cause significant harm, which is compounded by the fact that independently developed Node.js modules interact in uncontrolled ways. This paper presents a large-scale study across 235,850 Node.js modules to explore injection vulnerabilities. We show that injection vulnerabilities are prevalent in practice, both due to eval, which was previously studied for browser code, and due to the powerful exec API introduced in Node.js. Our study suggests that thousands of modules may be vulnerable to command injection attacks and that fixing them takes a long time, even for popular projects. Motivated by these findings, we present Synode, an automatic mitigation technique that combines static analysis and runtime enforcement of security policies to use vulnerable modules in a safe way. The key idea is to statically compute a template of values passed to APIs that are prone to injections, and to synthesize a grammar-based runtime policy from these templates. Our mechanism is easy to deploy: it does not require any modification of the Node.js platform, it is fast (sub-millisecond runtime overhead), and it protects against attacks of vulnerable modules, while inducing very few false positives (less than 10%).",
            "keywords": [
                "Node.js Security",
                "Injection Attacks",
                "Command Injection",
                "Static Analysis",
                "Runtime Enforcement"
            ]
        },
        "url": "URL#3127791",
        "sema_paperId": "36a034581f73dcc3862348ca845ab965b78f15de"
    },
    {
        "@score": "1",
        "@id": "3127792",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "136/8343",
                        "text": "Ben Stock"
                    },
                    {
                        "@pid": "35/9056",
                        "text": "Giancarlo Pellegrino"
                    },
                    {
                        "@pid": "53/10825",
                        "text": "Frank Li 0001"
                    },
                    {
                        "@pid": "b/MichaelBackes1",
                        "text": "Michael Backes 0001"
                    },
                    {
                        "@pid": "08/6024",
                        "text": "Christian Rossow"
                    }
                ]
            },
            "title": "Didn&apos;t You Hear Me? - Towards More Successful Web Vulnerability Notifications.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/StockPL0R18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01B-1_Stock_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/StockPL0R18",
            "abstract": "\u2014After treating the noti\ufb01cation of vulnerable par- ties as mere side-notes in research, the security community has recently put more focus on how to conduct vulnerability disclosure at scale. The \ufb01rst works in this area have shown that while noti\ufb01cations are helpful to a signi\ufb01cant fraction of operators, the vast majority of systems remain unpatched. In this paper, we build on these previous works, aiming to understand why the effects are not more signi\ufb01cant. To that end, we report on a noti\ufb01cation experiment targeting more than 24,000 domains, which allowed us to analyze what technical and human aspects are roadblocks to a successful campaign. As part of this experiment, we explored potential alternative noti\ufb01cation channels beyond email, including social media and phone. In addition, we conducted an anonymous survey with the noti\ufb01ed operators, investigating their perspectives on our noti\ufb01cations. We show the pitfalls of email-based communications, such as the impact of anti-spam \ufb01lters, the lack of trust by recipients, and the hesitation in \ufb01xing vulnerabilities despite awareness. However, our exploration of alternative communication channels did not suggest a more promising medium. Seeing these results, we pinpoint future directions in improving security noti\ufb01cations.",
            "keywords": [
                "Vulnerability Disclosure",
                "Web Security Notifications",
                "Notification Channels",
                "Email Communication Issues",
                "Operator Perspectives"
            ]
        },
        "url": "URL#3127792",
        "sema_paperId": "0ae3898a07fda8c327cc99195dcf1ca70ab11b22"
    },
    {
        "@score": "1",
        "@id": "3127793",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "144/4578",
                        "text": "Dean Sullivan"
                    },
                    {
                        "@pid": "163/3685",
                        "text": "Orlando Arias"
                    },
                    {
                        "@pid": "177/5203",
                        "text": "Travis Meade"
                    },
                    {
                        "@pid": "34/756",
                        "text": "Yier Jin"
                    }
                ]
            },
            "title": "Microarchitectural Minefields: 4K-Aliasing Covert Channel and Multi-Tenant Detection in Iaas Clouds.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/SullivanAMJ18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_06A-3_Sullivan_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/SullivanAMJ18",
            "abstract": "We introduce a new microarchitectural timing covert channel using the processor memory order buffer (MOB). Specifically, we show how an adversary can infer the state of a spy process on the Intel 64 and IA-32 architectures when predicting dependent loads through the store buffer, called 4K-aliasing. The 4K-aliasing event is a side-effect of memory disambiguation misprediction while handling write-after-read data hazards wherein the lower 12-bits of a load address will falsely match with store addresses resident in the MOB. In this work, we extensively analyze 4K-aliasing and demonstrate a new timing channel measureable across processes when executed as hyperthreads. We then use 4K-aliasing to build a robust covert communication channel on both the Amazon EC2 and Google Compute Engine capable of communicating at speeds of 1.28 Mbps and 1.49 Mbps, respectively. In addition, we show that 4K-aliasing can also be used to reliably detect multi-tenancy.",
            "keywords": [
                "Microarchitectural Timing Covert Channel",
                "4K-Aliasing",
                "Memory Order Buffer",
                "Multi-Tenancy Detection",
                "Hyperthreading Communication"
            ]
        },
        "url": "URL#3127793",
        "sema_paperId": "8dd0a69294df77a3f339f2b468b01e9661a433d9"
    },
    {
        "@score": "1",
        "@id": "3127794",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "77/1147-1",
                        "text": "Di Tang 0001"
                    },
                    {
                        "@pid": "34/6503-1",
                        "text": "Zhe Zhou 0001"
                    },
                    {
                        "@pid": "77/3503",
                        "text": "Yinqian Zhang"
                    },
                    {
                        "@pid": "66/6560",
                        "text": "Kehuan Zhang"
                    }
                ]
            },
            "title": "Face Flashing: a Secure Liveness Detection Protocol based on Light Reflections.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/TangZZZ18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2018_03B-5_Tang_paper-updated.pdf",
            "url": "https://dblp.org/rec/conf/ndss/TangZZZ18",
            "abstract": "Face authentication systems are becoming increasingly prevalent, especially with the rapid development of Deep Learning technologies. However, human facial information is easy to be captured and reproduced, which makes face authentication systems vulnerable to various attacks. Liveness detection is an important defense technique to prevent such attacks, but existing solutions did not provide clear and strong security guarantees, especially in terms of time.To overcome these limitations, we propose a new liveness detection protocol called Face Flashing that significantly increases the bar for launching successful attacks on face authentication systems. By randomly flashing well-designed pictures on a screen and analyzing the reflected light, our protocol has leveraged physical characteristics of human faces: reflection processing at the speed of light, unique textual features, and uneven 3D shapes. Cooperating with working mechanism of the screen and digital cameras, our protocol is able to detect subtle traces left by an attacking process.To demonstrate the effectiveness of Face Flashing, we implemented a prototype and performed thorough evaluations with large data set collected from real-world scenarios. The results show that our Timing Verification can effectively detect the time gap between legitimate authentications and malicious cases. Our Face Verification can also differentiate 2D plane from 3D objects accurately. The overall accuracy of our liveness detection system is 98.8%, and its robustness was evaluated in different scenarios. In the worst case, our system's accuracy decreased to a still-high 97.3%",
            "keywords": [
                "Liveness Detection",
                "Face Authentication",
                "Light Reflections",
                "Timing Verification",
                "3D Shape Differentiation"
            ]
        },
        "url": "URL#3127794"
    },
    {
        "@score": "1",
        "@id": "3127795",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "164/5259",
                        "text": "Giorgos Tsirantonakis"
                    },
                    {
                        "@pid": "129/9582",
                        "text": "Panagiotis Ilia"
                    },
                    {
                        "@pid": "33/2939",
                        "text": "Sotiris Ioannidis"
                    },
                    {
                        "@pid": "51/4565",
                        "text": "Elias Athanasopoulos"
                    },
                    {
                        "@pid": "09/1231",
                        "text": "Michalis Polychronakis"
                    }
                ]
            },
            "title": "A Large-scale Analysis of Content Modification by Open HTTP Proxies.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/TsirantonakisII18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_04A-1_Tsirantonakis_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/TsirantonakisII18",
            "abstract": "Open HTTP proxies offer a quick and convenient solution for routing web traffic towards a destination. In contrast to more elaborate relaying systems, such as anonymity networks or VPN services, users can freely connect to an open HTTP proxy without the need to install any special software. Therefore, open HTTP proxies are an attractive option for bypassing IPbased filters and geo-location restrictions, circumventing content blocking and censorship, and in general, hiding the client\u2019s IP address when accessing a web server. Nevertheless, the consequences of routing traffic through an untrusted third party can be severe, while the operating incentives of the thousands of publicly available HTTP proxies are questionable. In this paper, we present the results of a large-scale analysis of open HTTP proxies, focusing on determining the extent to which user traffic is manipulated while being relayed. We have designed a methodology for detecting proxies that, instead of passively relaying traffic, actively modify the relayed content. Beyond simple detection, our framework is capable of macroscopically attributing certain traffic modifications at the network level to well-defined malicious actions, such as ad injection, user fingerprinting, and redirection to malware landing pages. We have applied our methodology on a large set of publicly available HTTP proxies, which we monitored for a period of two months, and identified that 38% of them perform some form of content modification. The majority of these proxies can be considered benign, as they do not perform any harmful content modification. However, 5.15% of the tested proxies were found to perform modification or injection that can be considered as malicious or unwanted. Specifically, 47% of the malicious proxies injected ads, 39% injected code for collecting user information that can be used for tracking and fingerprinting, and 12% attempted to redirect the user to pages that contain malware. Our study reveals the true incentives of many of the publicly available web proxies. Our findings raise several concerns, as we uncover multiple cases where users can be severely affected by connecting to an open proxy. As a step towards protecting users against unwanted content modification, we built a service that leverages our methodology to automatically collect and probe public proxies, and generates a list of safe proxies that do not perform any content modification, on a daily basis.",
            "keywords": [
                "Open HTTP Proxies",
                "Content Modification",
                "Traffic Manipulation",
                "Malicious Proxies",
                "User Privacy Risks"
            ]
        },
        "url": "URL#3127795",
        "sema_paperId": "7a032e7e263484d74cbd9e5a24752acba0821bdf"
    },
    {
        "@score": "1",
        "@id": "3127796",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "118/3426",
                        "text": "G\u00fcliz Seray Tuncay"
                    },
                    {
                        "@pid": "136/8374",
                        "text": "Soteris Demetriou"
                    },
                    {
                        "@pid": "224/2489",
                        "text": "Karan Ganju"
                    },
                    {
                        "@pid": "g/CarlAGunter",
                        "text": "Carl A. Gunter"
                    }
                ]
            },
            "title": "Resolving the Predicament of Android Custom Permissions.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/TuncayDGG18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_08-4_Tuncay_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/TuncayDGG18",
            "abstract": "Android leverages a set of system permissions to protect platform resources. At the same time, it allows untrusted third-party applications to declare their own custom permissions to regulate access to app components. However, Android treats custom permissions the same way as system permissions even though they are declared by entities of different trust levels. In this work, we describe two new classes of vulnerabilities that arise from the \u2018predicament\u2019 created by mixing system and custom permissions in Android. These have been acknowledged as serious security flaws by Google and we demonstrate how they can be exploited in practice to gain unauthorized access to platform resources and to compromise popular Android apps. To address the shortcomings of the system, we propose a new modular design called Cusper for the Android permission model. Cusper separates the management of system and custom permissions and introduces a backward-compatible naming convention for custom permissions to prevent custom permission spoofing. We validate the correctness of Cusper by 1) introducing the first formal model of Android runtime permissions, 2) extending it to describe Cusper, and 3) formally showing that key security properties that can be violated in the current permission model are always satisfied in Cusper. To demonstrate Cusper\u2019s practicality, we implemented it in the Android platform and showed that it is both effective and efficient.",
            "keywords": [
                "Android Permissions",
                "Custom Permissions",
                "Security Vulnerabilities",
                "Permission Management",
                "Custom Design"
            ]
        },
        "url": "URL#3127796",
        "sema_paperId": "bd40e0e308260ae045ea5feeb8d0f514aa9afe8b"
    },
    {
        "@score": "1",
        "@id": "3127797",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "62/11127",
                        "text": "Erkam Uzun"
                    },
                    {
                        "@pid": "38/860",
                        "text": "Simon Pak Ho Chung"
                    },
                    {
                        "@pid": "e/IrfanAEssa",
                        "text": "Irfan Essa"
                    },
                    {
                        "@pid": "29/5976",
                        "text": "Wenke Lee"
                    }
                ]
            },
            "title": "rtCaptcha: A Real-Time CAPTCHA Based Liveness Detection System.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/UzunCEL18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01B-4_Uzun_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/UzunCEL18",
            "abstract": "Facial/voice-based authentication is becoming increasingly popular (e.g., already adopted by MasterCard and AliPay), because it is easy to use. In particular, users can now authenticate themselves to online services by using their mobile phone to show themselves performing simple tasks like blinking or smiling in front of its built-in camera. Our study shows that many of the publicly available facial/voice recognition services (e.g. Microsoft Cognitive Services or Amazon Rekognition) are vulnerable to even the most primitive attacks. Furthermore, recent work on modeling a person's face/voice (e.g. Face2Face [1]) allows an adversary to create very authentic video/audio of any target victim to impersonate that target. All it takes to launch such attacks are a few pictures and voice samples of a victim, which can all be obtained by either abusing the camera and microphone of the victim's phone, or through the victim's social media account. In this work, we propose the Real Time Captcha (rtCaptcha) system, which stops/slows down such an attack by turning the adversary's task from creating authentic video/audio of the target victim performing known authentication tasks (e.g., smile, blink) to figuring out what is the authentication task, which is encoded as a Captcha. Specifically, when a user tries to authenticate using rtCaptcha, they will be presented a Captcha and will be asked to take a \u201cselfie\u201d video while announcing the answer to the Captcha. As such, the security guarantee of our system comes from the strength of Captcha, and not how well we can distinguish real faces/voices from synthesized ones. To demonstrate the usability and security of rtCaptcha, we conducted a user study to measure human response times to the most popular Captcha schemes. Our experiments show that, thanks to the humans' speed of solving Captchas, adversaries will have to solve Captchas in less than 2 seconds in order to appear live/human and defeat rtCaptcha, which is not possible for the best settings on the attack side.",
            "keywords": [
                "Liveness Detection",
                "CAPTCHA",
                "Facial Authentication",
                "Impersonation Attacks",
                "Real-Time Captcha (rtCaptcha)"
            ]
        },
        "url": "URL#3127797",
        "sema_paperId": "59c8197e0b0413aca8005283598b4eb81b9016d0"
    },
    {
        "@score": "1",
        "@id": "3127798",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "70/2619",
                        "text": "Nikos Vasilakis"
                    },
                    {
                        "@pid": "131/5095",
                        "text": "Ben Karel"
                    },
                    {
                        "@pid": "207/7208",
                        "text": "Nick Roessler"
                    },
                    {
                        "@pid": "20/10299",
                        "text": "Nathan Dautenhahn"
                    },
                    {
                        "@pid": "d/ADeHon",
                        "text": "Andr\u00e9 DeHon"
                    },
                    {
                        "@pid": "s/JonathanMSmith",
                        "text": "Jonathan M. Smith"
                    }
                ]
            },
            "title": "BreakApp: Automated, Flexible Application Compartmentalization.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/VasilakisKRDDS18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_08-3_Vasilakis_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/VasilakisKRDDS18",
            "abstract": "Developers of large-scale software systems may use third-party modules to reduce costs and accelerate release cycles, at some risk to safety and security. BREAKAPP exploits module boundaries to automate compartmentalization of systems and enforce security policies, enhancing reliability and security. BREAKAPP transparently spawns modules in protected compartments while preserving their original behavior. Optional high-level policies decouple security assumptions made during development from requirements imposed for module composition and use. These policies allow fine-tuning trade-offs such as security and performance based on changing threat models or load patterns. Evaluation of BREAKAPP with a prototype implementation for JavaScript demonstrates feasibility by enabling simplified security hardening of existing systems with low performance overhead.",
            "keywords": [
                "Application Compartmentalization",
                "Security Policies",
                "Module Boundaries",
                "Reliability Enhancement",
                "JavaScript Security Hardening"
            ]
        },
        "url": "URL#3127798",
        "sema_paperId": "36ecc45fb80a2c01e5e61f403a30ec8806346505"
    },
    {
        "@score": "1",
        "@id": "3127799",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "48/7556",
                        "text": "Huandong Wang"
                    },
                    {
                        "@pid": "76/5013-1",
                        "text": "Chen Gao 0001"
                    },
                    {
                        "@pid": "93/2334-8",
                        "text": "Yong Li 0008"
                    },
                    {
                        "@pid": "71/4292-11",
                        "text": "Gang Wang 0011"
                    },
                    {
                        "@pid": "46/5770",
                        "text": "Depeng Jin"
                    },
                    {
                        "@pid": "08/4860",
                        "text": "Jingbo Sun"
                    }
                ]
            },
            "title": "De-anonymization of Mobility Trajectories: Dissecting the Gaps between Theory and Practice.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/WangG0WJS18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_06B-3_Wang_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/WangG0WJS18",
            "abstract": "Human mobility trajectories are increasingly collected by ISPs to assist academic research and commercial applications. Meanwhile, there is a growing concern that individual trajectories can be de-anonymized when the data is shared, using information from external sources (e.g. online social networks). To understand this risk, prior works either estimate the theoretical privacy bound or simulate de-anonymization attacks on synthetically created (small) datasets. However, it is not clear how well the theoretical estimations are preserved in practice. In this paper, we collected a large-scale ground-truth trajectory dataset from 2,161,500 users of a cellular network, and two matched external trajectory datasets from a large social network (56,683 users) and a check-in/review service (45,790 users) on the same user population. The two sets of large ground-truth data provide a rare opportunity to extensively evaluate a variety of de-anonymization algorithms (7 in total). We find that their performance in the real-world dataset is far from the theoretical bound. Further analysis shows that most algorithms have underestimated the impact of spatio-temporal mismatches between the data from different sources, and the high sparsity of user generated data also contributes to the underperformance. Based on these insights, we propose 4 new algorithms that are specially designed to tolerate spatial or temporal mismatches (or both) and model user behavior. Extensive evaluations show that our algorithms achieve more than 17% performance gain over the best existing algorithms, confirming our insights.",
            "keywords": [
                "Mobility Trajectories",
                "De-anonymization",
                "Privacy Risks",
                "Spatio-temporal Mismatches",
                "User Behavior Modeling"
            ]
        },
        "url": "URL#3127799",
        "sema_paperId": "031f680f3387272ad4fd7636a958b8d794979c1b"
    },
    {
        "@score": "1",
        "@id": "3127800",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "19/1924-17",
                        "text": "Qi Wang 0017"
                    },
                    {
                        "@pid": "184/8086",
                        "text": "Wajih Ul Hassan"
                    },
                    {
                        "@pid": "126/3601",
                        "text": "Adam Bates 0001"
                    },
                    {
                        "@pid": "g/CarlAGunter",
                        "text": "Carl A. Gunter"
                    }
                ]
            },
            "title": "Fear and Logging in the Internet of Things.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/WangHBG18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_01A-2_Wang_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/WangHBG18",
            "abstract": "\u2014As the Internet of Things (IoT) continues to proliferate, diagnosing incorrect behavior within increasingly-automated homes becomes considerably more dif\ufb01cult. Devices and apps may be chained together in long sequences of trigger-action rules to the point that from an observable symptom (e.g., an unlocked door) it may be impossible to identify the distantly removed root cause (e.g., a malicious app). This is because, at present, IoT audit logs are siloed on individual devices, and hence cannot be used to reconstruct the causal relationships of complex work\ufb02ows. In this work, we present ProvThings, a platform-centric approach to centralized auditing in the Internet of Things. ProvThings performs ef\ufb01cient automated instrumentation of IoT apps and device APIs in order to generate data provenance that provides a holistic explanation of system activities, including malicious behaviors. We prototype ProvThings for the Samsung SmartThings platform, and benchmark the ef\ufb01cacy of our approach against a corpus of 26 IoT attacks. Through the introduction of a selective code instrumentation optimization, we demonstrate in evaluation that ProvThings imposes just 5% overhead on physical IoT devices while enabling real time querying of system behaviors, and further consider how ProvThings can be leveraged to meet the needs of a variety of stakeholders in the IoT ecosystem.",
            "keywords": [
                "Internet of Things",
                "Centralized Auditing",
                "Data Provenance",
                "Malicious Behaviors",
                "IoT Attack Detection"
            ]
        },
        "url": "URL#3127800",
        "sema_paperId": "5152f79d0c80186690947de4704a51b22852d3c9"
    },
    {
        "@score": "1",
        "@id": "3127801",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "95/4442-88",
                        "text": "Peng Wang 0088"
                    },
                    {
                        "@pid": "192/2270",
                        "text": "Xianghang Mi"
                    },
                    {
                        "@pid": "129/1113",
                        "text": "Xiaojing Liao"
                    },
                    {
                        "@pid": "06/6268",
                        "text": "XiaoFeng Wang 0001"
                    },
                    {
                        "@pid": "03/7768",
                        "text": "Kan Yuan"
                    },
                    {
                        "@pid": "54/476-1",
                        "text": "Feng Qian 0001"
                    },
                    {
                        "@pid": "12/6354",
                        "text": "Raheem A. Beyah"
                    }
                ]
            },
            "title": "Game of Missuggestions: Semantic Analysis of Search-Autocomplete Manipulations.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/WangML0YQB18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07A-1_Wang_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/WangML0YQB18",
            "abstract": "As a new type of blackhat Search Engine Optimization (SEO), autocomplete manipulations are increasingly utilized by miscreants and promotion companies alike to advertise desired suggestion terms when related trigger terms are entered by the user into a search engine. Like other illicit SEO, such activities game the search engine, mislead the querier, and in some cases, spread harmful content. However, little has been done to understand this new threat, in terms of its scope, impact and techniques, not to mention any serious effort to detect such manipulated terms on a large scale. Systematic analysis of autocomplete manipulation is challenging, due to the scale of the problem (tens or even hundreds of millions suggestion terms and their search results) and the heavy burdens it puts on the search engines. In this paper, we report the first technique that addresses these challenges, making a step toward better understanding and ultimately eliminating this new threat. Our technique, called Sacabuche, takes a semantics-based, two-step approach to minimize its performance impact: it utilizes Natural Language Processing (NLP) to analyze a large number of trigger and suggestion combinations, without querying search engines, to filter out the vast majority of legitimate suggestion terms; only a small set of suspicious suggestions are run against the search engines to get query results for identifying truly abused terms. This approach achieves a 96.23% precision and 95.63% recall, and its scalability enables us to perform a measurement study on 114 millions of suggestion terms, an unprecedented scale for this type of studies. The findings of the study bring to light the magnitude of the threat (0.48% Google suggestion terms we collected manipulated), and its significant security implications never reported before (e.g., exceedingly long lifetime of campaigns, sophisticated techniques and channels for spreading malware and phishing content).",
            "keywords": [
                "Autocomplete Manipulation",
                "Search Engine Optimization",
                "Semantic Analysis",
                "Malicious Suggestions",
                "Search Engine Abuse"
            ]
        },
        "url": "URL#3127801",
        "sema_paperId": "2e09139f2c2a3a7e694737e12ebe23a9ec4a1b1b"
    },
    {
        "@score": "1",
        "@id": "3127802",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "23/1556",
                        "text": "Frank Wang"
                    },
                    {
                        "@pid": "90/1843",
                        "text": "James Mickens"
                    },
                    {
                        "@pid": "99/5780",
                        "text": "Nickolai Zeldovich"
                    }
                ]
            },
            "title": "Veil: Private Browsing Semantics Without Browser-side Assistance.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/WangMZ18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_06B-4_Wang_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/WangMZ18",
            "abstract": "All popular web browsers offer a \u201cprivate browsing mode.\u201d After a private session terminates, the browser is supposed to remove client-side evidence that the session occurred. Unfortunately, browsers still leak information through the file system, the browser cache, the DNS cache, and on-disk reflections of RAM such as the swap file. Veil is a new deployment framework that allows web developers to prevent these information leaks, or at least reduce their likelihood. Veil leverages the fact that, even though developers do not control the client-side browser implementation, developers do control 1) the content that is sent to those browsers, and 2) the servers which deliver that content. Veil web sites collectively store their content on Veil\u2019s blinding servers instead of on individual, site-specific servers. To publish a new page, developers pass their HTML, CSS, and JavaScript files to Veil\u2019s compiler; the compiler transforms the URLs in the content so that, when the page loads on a user\u2019s browser, URLs are derived from a secret user key. The blinding service and the Veil page exchange encrypted data that is also protected by the user\u2019s key. The result is that Veil pages can safely store encrypted content in the browser cache; furthermore, the URLs exposed to system interfaces like the DNS cache are unintelligible to attackers who do not possess the user\u2019s key. To protect against post-session inspection of swap file artifacts, Veil uses heap walking (which minimizes the likelihood that secret data is paged out), content mutation (which garbles in-memory artifacts if they do get swapped out), and DOM hiding (which prevents the browser from learning site-specific HTML, CSS, and JavaScript content in the first place). Veil pages load on unmodified commodity browsers, allowing developers to provide stronger semantics for private browsing without forcing users to install or reconfigure their machines. Veil provides these guarantees even if the user does not visit a page using a browser\u2019s native privacy mode; indeed, Veil\u2019s protections are stronger than what the browser alone can provide.",
            "keywords": [
                "Private Browsing",
                "Information Leakage",
                "Web Development",
                "User Privacy",
                "Encrypted Content Storage"
            ]
        },
        "url": "URL#3127802",
        "sema_paperId": "8984c9e8556461b7093b5c549bcc20a051c71717"
    },
    {
        "@score": "1",
        "@id": "3127803",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "44/10356",
                        "text": "Weilin Xu"
                    },
                    {
                        "@pid": "e/DavidEvans",
                        "text": "David Evans 0001"
                    },
                    {
                        "@pid": "81/3848",
                        "text": "Yanjun Qi"
                    }
                ]
            },
            "title": "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/Xu0Q18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03A-4_Xu_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/Xu0Q18",
            "abstract": "Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by \\emph{adversarial examples} that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on refining the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, \\emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks.",
            "keywords": [
                "Adversarial Examples",
                "Feature Squeezing",
                "Deep Neural Network Defense",
                "Detection Methods",
                "Color Bit Depth and Spatial Smoothing"
            ]
        },
        "url": "URL#3127803",
        "sema_paperId": "9fec45e1ff97ffb0e0cf9f039e39b46043430301"
    },
    {
        "@score": "1",
        "@id": "3127804",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "153/5855",
                        "text": "Guangliang Yang 0001"
                    },
                    {
                        "@pid": "68/4706-1",
                        "text": "Jeff Huang 0001"
                    },
                    {
                        "@pid": "64/1147",
                        "text": "Guofei Gu"
                    }
                ]
            },
            "title": "Automated Generation of Event-Oriented Exploits in Android Hybrid Apps.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/Yang0G18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_04B-3_Yang_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/Yang0G18",
            "abstract": "Recently more and more Android apps integrate the embedded browser, known as \u201cWebView\u201d, to render web pages and run JavaScript code without leaving these apps. WebView provides a powerful feature that allows event handlers defined in the native context (i.e., Java in Android) to handle web events that occur in WebView. However, as shown in prior work, this feature suffers from remote attacks, which we generalize as EventOriented Exploit (EOE) in this paper, such that adversaries may remotely access local critical functionalities through event handlers in WebView without any permission or authentication. In this paper, we propose a novel approach, EOEDroid, which can automatically vet event handlers in a given hybrid app using selective symbolic execution and static analysis. If a vulnerability is found, EOEDroid also automatically generates exploit code to help developers and analysts verify the vulnerability. To support exploit code generation, we also systematically study web events, event handlers and their trigger constraints. We evaluated our approach on 3,652 most popular apps. The result showed that our approach found 97 total vulnerabilities in 58 apps, including 2 cross-frame DOM manipulation, 53 phishing, 30 sensitive information leakage, 1 local resources access, and 11 Intent abuse vulnerabilities. We also found a potential backdoor in a high profile app that could be used to steal users\u2019 sensitive information, such as IMEI. Even though developers attempted to close it, EOEDroid found that adversaries were still able to exploit it by triggering two events together and feeding event handlers with well designed input.",
            "keywords": [
                "Android Hybrid Apps",
                "WebView Security",
                "Event-Oriented Exploits",
                "Vulnerability Detection",
                "Exploit Code Generation"
            ]
        },
        "url": "URL#3127804",
        "sema_paperId": "df331b8f4e51dc562b873a26dfb5a6d006f081a4"
    },
    {
        "@score": "1",
        "@id": "3127805",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "158/4724",
                        "text": "Xiaokuan Zhang"
                    },
                    {
                        "@pid": "60/8494",
                        "text": "Xueqiang Wang"
                    },
                    {
                        "@pid": "20/10240",
                        "text": "Xiaolong Bai"
                    },
                    {
                        "@pid": "77/3503",
                        "text": "Yinqian Zhang"
                    },
                    {
                        "@pid": "06/6268",
                        "text": "XiaoFeng Wang 0001"
                    }
                ]
            },
            "title": "OS-level Side Channels without Procfs: Exploring Cross-App Information Leakage on iOS.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/ZhangWBZW18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_05B-4_Zhang_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/ZhangWBZW18",
            "abstract": "It has been demonstrated in numerous previous studies that Android and its underlying Linux operating systems do not properly isolate mobile apps to prevent cross-app sidechannel attacks. Cross-app information leakage enables malicious Android apps to infer sensitive user data (e.g., passwords), or private user information (e.g., identity or location) without requiring specific permissions. Nevertheless, no prior work has ever studied these side-channel attacks on iOS-based mobile devices. One reason is that iOS does not implement procfs\u2014 the most popular side-channel attack vector; hence the previously known attacks are not feasible. In this paper, we present the first study of OS-level sidechannel attacks on iOS. Specifically, we identified several new side-channel attack vectors (i.e., iOS APIs that enable cross-app information leakage); developed machine learning frameworks (i.e., classification and pattern matching) that combine multiple attack vectors to improve the accuracy of the inference attacks; demonstrated three categories of attacks that exploit these vectors and frameworks to exfiltrate sensitive user information. We have reported our findings to Apple and proposed mitigations to the attacks. Apple has incorporated some of our suggested countermeasures into iOS 11 and MacOS High Sierra 10.13 and later versions.",
            "keywords": [
                "iOS Security",
                "Cross-App Information Leakage",
                "Side-Channel Attacks",
                "OS-level Vulnerabilities",
                "iOS APIs"
            ]
        },
        "url": "URL#3127805",
        "sema_paperId": "b7e6b39a3a9b1c8d55d06d0426191f111d9df56a"
    },
    {
        "@score": "1",
        "@id": "3127806",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "158/4817",
                        "text": "Haizhong Zheng"
                    },
                    {
                        "@pid": "166/1912",
                        "text": "Minhui Xue 0001"
                    },
                    {
                        "@pid": "72/5422",
                        "text": "Hao Lu"
                    },
                    {
                        "@pid": "07/6713-1",
                        "text": "Shuang Hao 0001"
                    },
                    {
                        "@pid": "22/5702",
                        "text": "Haojin Zhu"
                    },
                    {
                        "@pid": "92/3528",
                        "text": "Xiaohui Liang"
                    },
                    {
                        "@pid": "r/KWRoss",
                        "text": "Keith W. Ross"
                    }
                ]
            },
            "title": "Smoke Screener or Straight Shooter: Detecting Elite Sybil Attacks in User-Review Social Networks.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/ZhengXLHZLR18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_10-3_Zheng_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/ZhengXLHZLR18",
            "abstract": "Popular User-Review Social Networks (URSNs)\u2014such as Dianping, Yelp, and Amazon\u2014are often the targets of reputation attacks in which fake reviews are posted in order to boost or diminish the ratings of listed products and services. These attacks often emanate from a collection of accounts, called Sybils, which are collectively managed by a group of real users. A new advanced scheme, which we term elite Sybil attacks, recruits organically highly-rated accounts to generate seemingly-trustworthy and realistic-looking reviews. These elite Sybil accounts taken together form a large-scale sparsely-knit Sybil network for which existing Sybil fake-review defense systems are unlikely to succeed.In this paper, we conduct the first study to define, characterize, and detect elite Sybil attacks. We show that contemporary elite Sybil attacks have a hybrid architecture, with the first tier recruiting elite Sybil workers and distributing tasks by Sybil organizers, and with the second tier posting fake reviews for profit by elite Sybil workers. We design ELSIEDET, a three-stage Sybil detection scheme, which first separates out suspicious groups of users, then identifies the campaign windows, and finally identifies elite Sybil users participating in the campaigns. We perform a large-scale empirical study on ten million reviews from Dianping, by far the most popular URSN service in China. Our results show that reviews from elite Sybil users are more spread out temporally, craft more convincing reviews, and have higher filter bypass rates. We also measure the impact of Sybil campaigns on various industries (such as cinemas, hotels, restaurants) as well as chain stores, and demonstrate that monitoring elite Sybil users over time can provide valuable early alerts against Sybil campaigns.",
            "keywords": [
                "User-Review Social Networks",
                "Sybil Attacks",
                "Fake Reviews",
                "Elite Sybil Users",
                "Sybil Detection"
            ]
        },
        "url": "URL#3127806"
    },
    {
        "@score": "1",
        "@id": "3127807",
        "info": {
            "authors": {
                "author": [
                    {
                        "@pid": "155/8420",
                        "text": "Shitong Zhu"
                    },
                    {
                        "@pid": "148/1333",
                        "text": "Xunchao Hu"
                    },
                    {
                        "@pid": "31/8302",
                        "text": "Zhiyun Qian"
                    },
                    {
                        "@pid": "83/9528",
                        "text": "Zubair Shafiq"
                    },
                    {
                        "@pid": "77/6178-1",
                        "text": "Heng Yin 0001"
                    }
                ]
            },
            "title": "Measuring and Disrupting Anti-Adblockers Using Differential Execution Analysis.",
            "venue": "NDSS",
            "year": "2018",
            "type": "Conference and Workshop Papers",
            "access": "open",
            "key": "conf/ndss/ZhuHQSY18",
            "ee": "https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_04A-2_Zhu_paper.pdf",
            "url": "https://dblp.org/rec/conf/ndss/ZhuHQSY18",
            "abstract": "Millions of people use adblockers to remove intrusive and malicious ads as well as protect themselves against tracking and pervasive surveillance. Online publishers consider adblockers a major threat to the ad-powered \u201cfree\u201d Web. They have started to retaliate against adblockers by employing antiadblockers which can detect and stop adblock users. To counter this retaliation, adblockers in turn try to detect and filter anti-adblocking scripts. This back and forth has prompted an escalating arms race between adblockers and anti-adblockers. We want to develop a comprehensive understanding of antiadblockers, with the ultimate aim of enabling adblockers to bypass state-of-the-art anti-adblockers. In this paper, we present a differential execution analysis to automatically detect and analyze anti-adblockers. At a high level, we collect execution traces by visiting a website with and without adblockers. Through differential execution analysis, we are able to pinpoint the conditions that lead to the differences caused by anti-adblocking code. Using our system, we detect anti-adblockers on 30.5% of the Alexa top10K websites which is 5-52 times more than reported in prior literature. Unlike prior work which is limited to detecting visible reactions (e.g., warning messages) by anti-adblockers, our system can discover attempts to detect adblockers even when there is no visible reaction. From manually checking one third of the detected websites, we find that the websites that have no visible reactions constitute over 90% of the cases, completely dominating the ones that have visible warning messages. Finally, based on our findings, we further develop JavaScript rewriting and API hooking based solutions (the latter implemented as a Chrome extension) to help adblockers bypass state-of-the-art anti-adblockers.",
            "keywords": [
                "Adblockers",
                "Anti-Adblockers",
                "Differential Execution Analysis",
                "Web Tracking",
                "JavaScript Rewriting"
            ]
        },
        "url": "URL#3127807",
        "sema_paperId": "ae56d3d5187f00b793b703086a9d045324ebbb9c"
    },
    {
        "@score": "1",
        "@id": "3170852",
        "info": {
            "title": "25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018",
            "venue": "NDSS",
            "publisher": "The Internet Society",
            "year": "2018",
            "type": "Editorship",
            "access": "open",
            "key": "conf/ndss/2018",
            "ee": "https://www.ndss-symposium.org/ndss2018",
            "url": "https://dblp.org/rec/conf/ndss/2018",
            "abstract": null
        },
        "url": "URL#3170852",
        "sema_paperId": "28cce830a94b52dc162d9ace10fbbc720d97777d"
    }
]